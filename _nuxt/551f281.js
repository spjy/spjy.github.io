(window.webpackJsonp=window.webpackJsonp||[]).push([[36],{516:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Linear Algebra\ndescription: The study of linear equations.\n---\n\n# Scalars\n\nA scalar is a single number.\n\n# Vectors\n\nA vector is an array numbers.\n\n## Notation\n\n$$ \\bold{A} =\n\\left[\n\\begin{array}{c}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{array}\n\\right]$$\n\nwhere $n$ is the number of rows in the vector and $[\\bold x]_i$ is the $i$-th entry.\n\n## Dot Product\n\nThe dot product is the sum of products of each component between two vectors. It tells us the magnitude of how much in a direction one vector is to another.\n\n$$\\bold x \\cdot \\bold y = \\sum_{i = 1}^n x_i b_i$$\n\n### Orthogonality\n\nIf the dot product results in the zero vector $\\bold{0}$, the two vectors are said to be perpendicular.\n\n$$\\bold x \\cdot \\bold y = \\sum_{i = 1}^n x_i b_i = \\bold 0 \\implies \\text{x,y are perpendicular}$$\n\n### Length\n\nThe magnitude or length of a vector is the root of the dot product of itself.\n\n$$||\\bold x|| = \\sqrt{\\bold x \\cdot \\bold x} = (x_1^2 + x_2^2 + \\dots + x_n^2)^{1/2}$$\n\nGeometrically, the components of the vector form a right triangle, and using the pythagorean theorem, we find the hypotenuse of that triangle.\n\n### Unit Length\n\nA unit vector has unit length, or $||\\bold u|| = 1$.\n\n### Unit Vector\n\nThe unit vector $\\bold u$ is a vector with [unit length](#unit-length).\n\n$$\\bold u = \\frac{\\bold x}{||\\bold x||}$$\n\nIt also retains the same direction as $x$.\n\n### Angle\n\nThe angle $\\theta$ between two vectors is defined as:\n\n$$\\cos \\theta = \\frac{x \\cdot y}{||\\bold x|| \\text{ } ||\\bold y||}$$\n\n## Linear Combinations\n\nA linear combination is the sum of scalar multiples of vectors. Let $\\bold{v}_1 \\dots \\bold{v}_n$ be vectors and $a_1 \\dots a_n$ be scalars:\n\n$$a_1 \\bold{v}_1 + a_2 \\bold{v}_2 + \\dots + a_n \\bold{v}_n$$\n\nWe can represent a certain vector in terms of a linear combination of vectors by scaling the weights of each vector.\n\nIf we set the linear combination equal to a vector $\\bold{b}$:\n\n$$a_1 \\bold{v}_1 + a_2 \\bold{v}_2 + \\dots + a_n \\bold{v}_n = \\bold b$$\n \nwe are trying to find a combination $a_1, a_2, \\dots, a_n$ such that it results in the vector $\\bold b$.\n\n### Spanning Sets\n\nThe set of all possible linear combinations of $n$ vectors is the **spanning set** of the given $n$ vectors.\n\nIn other words, it is the set of all possible vectors you get by scaling each vector in some way and summing them.\n\n$$S = \\bigg\\{ \\sum_{i=1}^n a_i\\bold{v}_i | a_i \\in \\mathbb{R}, 1 \\leq i \\leq p \\bigg\\}$$\n\n### Linear Dependence\n\nGiven a set of vectors $S = \\{v_1, v_2, v_3, \\ldots, v_n\\}$, we say the set of vectors is **linearly dependent** if there are $a_1, a_2, a_3, \\ldots, a_n$, $a_i$ not all equal to zero such that\n\n$$a_1 \\bold{x}_1 + a_2 \\bold{x}_2 + a_3 \\bold{x}_3 + ... + a_n \\bold{x}_n = 0$$\n\nIn other words, there exists a solution that is non-trivial. The vectors span less than $\\mathbb{R}^n$.\n\n### Linear Independence\n\nOn the other hand, the set of vectors $S = \\{v_1, v_2, v_3, \\ldots, v_n\\}$ is **linearly independent** if all $a_i = 0$, the trivial solution, is the only solution.\n\n$$a_1 \\bold{x}_1 + a_2 \\bold{x}_2 + a_3 \\bold{x}_3 + ... + a_n \\bold{x}_n = 0$$\n\nIn other words, the only solution that exists is the trivial solution. The vectors span all of $\\mathbb{R}^n$.\n\n### Basis\n\nThe basis of a vector space $V$ with a set of vectors $S = \\{ \\bold{v_1}, \\dots, \\bold{v_n} \\}$ that are:\n- [Linearly independent](#linear-independence)\n- In the spanning set of $V$.\n\n### Column Space\n\nIf we have an $m \\times n$ matrix $A$ with column vectors $\\bold{A}_1$, $\\bold{A}_2$, $\\dots$, $\\bold{A}_n$, then the column space of a matrix $A$ is the set containing the linear combinations of the column vectors of the matrix.\n\nIt is denoted as:\n\n$$\\mathcal{CS}(\\bold A) = \\{ \\bold{A}_1, \\bold{A}_2, \\dots \\bold{A}_n \\}$$\n\n## Linear Transformation\n\nA **linear transformation** is the act of inputting one vector and manipulating it such that you output a new vector.\n\nFor instance if you rotate a vector a certain number of degrees.\n\n## Vector Space\n\nA vector space defines the properties of operations of vectors.\n\n# Matrices\n\nIntuitively, a matrix has the ability to transform a vector. Take, for example, $A \\bold{x = b}$. We are calculating how to achieve the vector $\\bold b$ by scaling $\\bold x$ by $A$.\n\nA matrix is a rectangular array of numbers of the form:\n\n### Notation\n\n$$A_{mn} =\n\\left[\n\\begin{array}{ccccc}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \na_{m1} & a_{m2} & \\dots & a_{mn} \\\\\n\\end{array}\n\\right]$$\n\nwhere $A$ is an $m \\times n$ matrix and $m$ are the rows and $n$ are the columns.\n\nMore generally,\n\n$${A} =\n\\left[\n\\begin{array}{c}\na_{ij} \n\\end{array}\n\\right]$$\n\n$A$ is an $i \\times j$ matrix where $i$ is the number of rows and $j$ is the number of columns.\n\n### Rows\n\n$\\bold{A}_{i,:}$ denotes the $i$-th row of matrix $A$.\n\n### Columns\n\n$\\bold{A}_{:,j}$ denotes the $j$-th column of matrix $A$.\n\n### Entries\n\n$\\bold{A}_{i,j}$ denotes the $(i,j)$-th element of matrix $A$.\n\n### Properties\n\nLet\n\n$$A =\n\\left[\n\\begin{array}{c}\na_{ij} \n\\end{array}\n\\right]$$\n\n$$B =\n\\left[\n\\begin{array}{c}\nb_{ij} \n\\end{array}\n\\right]$$\n\n$$C =\n\\left[\n\\begin{array}{c}\nc_{ij} \n\\end{array}\n\\right]$$\n\nand $c,d$ be scalars.\n\n#### Addition\n\n$$A + B =\n\\left[\n\\begin{array}{c}\na_{ij} + b_{ij}\n\\end{array}\n\\right]$$\n\n#### Scalar Product\n\n$$\nc\\bold{A} =\nc \\left[\n\\begin{array}{c}\na_{ij}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}{c}\nc a_{ij}\n\\end{array}\n\\right]\n$$\n\n#### Commutativity\n\n$$\\bold{A} + \\bold{B} = \\bold{B} + \\bold{A} \\text{}$$\n\n#### Associativity\n\n$$\\bold{A} + (\\bold{B} + \\bold{C}) = (\\bold{A} + \\bold{B}) + \\bold{C} \\text{}$$\n\n$$\\bold{(AB)C = A(BC)}$$\n\n#### Scalar Distributivity\n\n$$c(d\\bold{A}) = (cd)\\bold{A}$$\n\n$$c(\\bold{A} + \\bold{B}) = c\\bold{A} + d \\bold{B}$$\n\n$$(c + d) \\bold{A} = c \\bold{A} + d \\bold{A}$$\n\n#### Negativity\n\n$$-\\bold{A} =\n\\left[\n\\begin{array}{c}\n- a_{ij}\n\\end{array}\n\\right]\n$$\n\n#### Exponents\n\n$$\\bold A^n = \\bold{A_1 A_2} \\dots \\bold A_n$$\n$$\\bold{A^n A^m = A^{n+m}}$$\n$$\\bold{(A^n)^m=\\bold A^{nm}}$$\n\n## Identity Matrix\n\nThe identity matrix is a square $n \\times n$ matrix that contains 1's on the diagonal and 0's elsewhere.\n\n$$ \\bold{I}_n =\n\\left[\n\\begin{array}{ccccc}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n0 & 0 & \\dots & 1 \\\\\n\\end{array}\n\\right]$$\n\nIt has the special property that:\n\n$$\\bold I \\bold x = \\bold x$$\n\n## Rectangular Matrix\n\nGiven an $m \\times n$ matrix $A$, $A$ is a rectangular matrix if $m \\neq n$.\n\n## Square Matrix\n\nGiven an $m \\times n$ matrix $A$, $A$ is a square matrix if $m=n$.\n\n## Diagonal Matrix\n\nA diagonal matrix is one where there are non-zero elements in indices such that $i=j$ (the column index equals the row index), and all other elements are zero.\n\n$$\nD =\n\\begin{bmatrix}\n  d_{1} & & \\\\\n  & \\ddots & \\\\\n  & & d_{n}\n\\end{bmatrix}\n$$\n\n## Nonsingular Matrix\n\nGiven a square matrix $A$, suppose that the columns are linearly independent, or the solution set is only the trivial solution and there exists only one solution.\n\n$A$ is a nonsingular matrix.\n\nOtherwise, $A$ is a singular matrix, or in other words, there are infinite or no solutions and the columns are linearly dependent.\n\n## Inverting a Matrix\n\nGiven an $n \\times n$ matrix $A$, $B$ is an inverse of $\\bold A$ where:\n\n$$\\bold{AB} = \\bold{BA} = \\bold I \\implies \\bold{A A^{-1} = A^{-1} A = I}$$\n\n$\\bold A^{-1}$ is denoted as the inverse of $\\bold A$.\n\nIntuitively, we are undoing the transformation of a matrix. Take $A \\bold{x = b} \\implies \\bold x = A^{-1} \\bold b$.\n\nWe are trying to retrieve the original vector $\\bold x$ transformed by $A$ by undoing that transformation and taking the inverse $A^{-1}$.\n\n### Conditions for Invertibility\n\nGiven a square matrix $\\bold A$:\n\n- $\\bold A$ must have $n$ non-zero pivots (linearly independent columns).\n- $\\det \\bold A \\neq 0$\n- $\\bold A\\bold x = \\bold 0$ where $x = 0$, the trivial solution, is the only solution.\n- Gauss-Jordan elimination eliminates $[ \\bold A \\bold I ]$ to $[ \\bold I \\bold A^{-1} ]$\n- $\\bold A$ is invertible if $|a_{ii}| > \\sum_{j \\neq i} |a_{ij}|$ (it is diagonally dominant)\n\n### Properties\n\n- $(A^{-1})^{-1} = A$\n- $(kA)^{-1} = k^{-1} A^{-1} \\text{}$\n- $\\det(A^{-1}) = (\\det A)^{-1} \\text{}$\n\n### Calculating inverse of diagonal matrix\n\nGiven an invertible, square, and diagonal matrix with non-zero diagonal entries:\n\n$$D =\n\\begin{bmatrix}\n  d_{1} & & \\\\\n  & \\ddots & \\\\\n  & & d_{n}\n\\end{bmatrix} \\implies D^{-1}=\n\\begin{bmatrix}\n  \\frac{1}{d_{1}} & & \\\\\n  & \\ddots & \\\\\n  & & \\frac{1}{d_{n}}\n\\end{bmatrix}$$\n\n### Calculating inverse of matrix product\n\nIf $A$ and $B$ are invertible, then $AB$ is as well.\n\n$$(AB)^{-1} = B^{-1} A^{-1}$$\n\n## Elementary Matrix\n\nAn elementary matrix is one that applies a row operation.\n\n### Row switch\n\n$R_i \\leftrightarrow R_j$\n\n$$E_s = \\begin{bmatrix}\n  1 &        &   &        &   &        &   \\\\\n    & \\ddots &   &        &   &        &   \\\\\n    &        & 0 &        & 1 &        &   \\\\\n    &        &   & \\ddots &   &        &   \\\\\n    &        & 1 &        & 0 &        &   \\\\\n    &        &   &        &   & \\ddots &   \\\\\n    &        &   &        &   &        & 1\n\\end{bmatrix}$$\n\n#### Inverse\n\n$$E_s^{-1} = E_s$$\n\n### Row multiplication: \n\n$k R_i \\rightarrow R_i \\quad k \\neq 0$\n\n$$E_m(k) = \\begin{bmatrix}\n  1 &        &   &   &   &        &   \\\\\n    & \\ddots &   &   &   &        &   \\\\\n    &        & 1 &   &   &        &   \\\\\n    &        &   & k &   &        &   \\\\\n    &        &   &   & 1 &        &   \\\\\n    &        &   &   &   & \\ddots &   \\\\\n    &        &   &   &   &        & 1\n\\end{bmatrix}$$\n\n#### Inverse\n\n$$E_m^{-1}\\bigg(\\frac{1}{k}\\bigg) = E_m(k)$$\n\n### Row addition: \n\n$R_i + k R_j \\quad i \\neq j$\n\n$$E_a(k) = \\begin{bmatrix}\n  1 &        &   &        &   &        &   \\\\\n    & \\ddots &   &        &   &        &   \\\\\n    &        & 1 &        &   &        &   \\\\\n    &        &   & \\ddots &   &        &   \\\\\n    &        & k &        & 1 &        &   \\\\\n    &        &   &        &   & \\ddots &   \\\\\n    &        &   &        &   &        & 1\n\\end{bmatrix}$$\n\n#### Inverse\n\n$$E_a^{-1}(-k) = E_a(k)$$\n\n## Multiplication\n\n$$\\bold A_{mn} \\bold B_{np} = \\bold C_{mp}$$\n\n### Inner (Dot) Product\n\nEach entry $ij$ is calculated by calculating the dot product of the $i$th row and the $j$th column.\n\nwhere each entry $[\\bold C]_{ij} = \\displaystyle \\sum_{i=1}^n a_{ik} b_{kj} \\text{}$\n\n$$\\bold a^T \\bold b = \\left[\n\\begin{array}{c}\na_1 & a_2 & \\dots & a_n \n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\na_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \n\\end{array}\n\\right]$$\n\n### Outer Product\n\nThe outer product is a much easier method to multiply matrices. Simply multiply the columns by the rows and add them together.\n\n$$\\bold {ab}^T = \\left[\n\\begin{array}{c}\na_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\na_1 & a_2 & \\dots & a_n \n\\end{array}\n\\right]\n$$\n\n# Linear Equations\n\nA **linear equation** can be expressed as\n\n$$y=a_1x_1 + a_2x_2 + a_3x_3 + ... + a_nx_n = b$$\n\nwhere $a_1, a_2, a_3, ... , a_n, b$ are scalar coefficients and $x_1, x_2, x_3, ..., x_n$ are variables.\n\n## System of Linear Equations\n\nA system of linear equations takes the form:\n\n$$a_{11}x_1+a_{12}x_2+...+a_{1n}x_n=b_1\\\\\na_{21}x_1+a_{22}x_2+...+a_{2n}x_n=b_2\\\\\na_{31}x_1+a_{32}x_2+...+a_{3n}x_n=b_3\\\\\n\\newline\n\\newline \\vdots\n\\newline\na_{m1}x_1+a_{m2}x_2+...+a_{mn}x_n=b_m$$\n\nThere are $m$ equations, and $a_1, a_2, a_3, ... , a_n, b$ are scalar coefficients and $x_1, x_2, x_3, ..., x_n$ are variables.\n\nWe can succinctly compact this into:\n\n$$\\bold{Ax} = \\bold b$$\n\n### Coefficient Matrix\n\nWe take the coefficients of each variable and put it into a matrix:\n\n$$ \\bold{A} =\n\\left[\n\\begin{array}{ccccc}\na_{11} & a_{12} & a_{13} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & a_{23} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \na_{m1} & a_{m2} & \\dots & a_{m3} & a_{mn} \\\\\n\\end{array}\n\\right]$$\n\n### Vector of Constants\n\nThe target of what we want to get:\n\n$$ \\bold{b} =\n\\left[\n\\begin{array}{c}\nb_1 \\\\\nb_2 \\\\\nb_3 \\\\\n\\vdots \\\\\nb_n\n\\end{array}\n\\right]$$\n\n### Solution Vector\n\nThe solution to the linear equations:\n\n$$ \\bold{x} =\n\\left[\n\\begin{array}{c}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\vdots \\\\\nx_n\n\\end{array}\n\\right]$$\n\n### Homogenous System of Linear Equations\n\nA homogenous system of linear equations takes the form\n\n$$a_{11}x_1+a_{12}x_2+...+a_{1n}x_n=0\\\\\na_{21}x_1+a_{22}x_2+...+a_{2n}x_n=0\\\\\na_{31}x_1+a_{32}x_2+...+a_{3n}x_n=0\\\\\n\\newline\n\\newline \\vdots\n\\newline\na_{m1}x_1+a_{m2}x_2+...+a_{mn}x_n=0$$\n\nThe right side of each linear equation is zero. In other words, the [vector of constants](#vector-of-constants) is the zero vector.\n\nThis is system of linear equations is homogenous because the solution to the system has at least one trivial solution, $x_1=x_2= ... =x_n=0$.\n\n#### Theorem of Homogenous Systems\n\nA homogenous system with $n$ variables and $m$ equations has $\\infty$ solutions if $n>m$.\n\n## Types of Solution Sets of a System of Linear Equations\n\nA **solution set** is the set that contains every solution to a linear system.\n\nTwo systems are equivalent if their solution sets are equivalent.\n\nThere are three types of solution sets:\n\n### Single, Unique Solution\n\nThe system intersects at exactly one point.\n\n### Infinite Solutions\n\nThe system intersects at every point. One or more variables are free.\n\n### No Solutions\n\nThe system does not intersect at any point.\n\n## Consistent Solutions\n\nA system of linear equations is **consistent** if it has at least one solution. Otherwise, it is inconsistent.\n\n### Independent and Dependent Variables\n\nIn a consistent system of linear equations, a variable that is not dependent on another variable is called **independent** or free.\n\nOtherwise, it is called a **dependent** variable.\n\n## Gaussian Elimination\n\nThe **elimination** algorithm is a systematic method of solving systems of linear equations. As the name states, we are eliminating variables from equations to make it easier to solve.\n\n- Apply row operations expressed as elementary matrices $E_{ij}$ to transform the system into an upper triangular system (all values below the diagonal are zeros).\n- Apply back substitution from the bottom up to solve the system.\n\n#### Row Operations\n\nFor the row operations, there are two important terms, **pivots** and **multipliers**. \n\n- A pivot is the first non-zero entry in a row that does elimination.\n- A multiplier is defined as the entry in row $i$ divided by the pivot value in row $j$.\n\nIf successful, after we apply elimination, the pivots should be on the diagonal of the matrix and have non-zero values. It succeeds if there is a [single solution](#single-unique-solution). This would occur if the matrix is [nonsingular](#nonsingular-matrix).\n\nOtherwise, elimination fail if any of the pivots are zero. This situation occurs if there are [infinite](#infinite-solutions) or [no](#no-solutions) solutions. This occurs if the matrix is [singular](#nonsingular-matrix).\n\n## Augmented Matrix\n\nGiven a system of linear equations\n\n$$a_{11}x_1+a_{12}x_2+...+a_{1n}x_n=b_1\n\\newline\n\\newline \\vdots\n\\newline\na_{m1}x_1+a_{m2}x_2+...+a_{mn}x_n=b_n$$\n\nWe can extract the coefficients transform the system into an augmented matrix:\n\n$$\\left[\n\\begin{array}{cccc:c}\na_{11} & a_{12} & \\dots & a_{1n} & b_1 \\\\ \n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \na_{m1} & a_{m2} & \\dots & a_{mn} & b_n \\\\\n\\end{array}\n\\right] = [ \\bold{Ab}]$$\n\nWe are appending the vector of constants onto the coefficient matrix.\n\n## Reduced Row Echelon Form\n\n1. Any rows of zeroes are at the bottom.\n2. The first non-zero entry of a non-zero row is one (1).\n3. Starting from the top left, the leading one (1) appears to the right of the leading one (1) of the preceding row.\n4. All other entries of a column containing a leading one (1) is zero (0).\n\n### Gauss-Jordan Elimination\n\nGauss-Jordan Elimination can be used to solve systems of linear equations and find inverses of invertible matrices. The goal is to get the matrix in reduced row echelon form.\n\n### Pivot Column\n\nA pivot column in context of the reduced row echelon form of a matrix is a column containing a leading $1$ (in the leftmost non-zero entry).\n\n### Zero Row\n\nA zero row is a row containing all zeros. They are at the bottom.\n"}}]);