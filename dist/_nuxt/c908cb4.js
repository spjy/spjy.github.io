(window.webpackJsonp=window.webpackJsonp||[]).push([[6,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46],{494:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Algorithms\ndescription: The data playground.\n---\n\n# Dynamic Programming\n\n\n\n# Algorithm Analysis\n\n## Loop Invariants\n\nThe loop invariant is a property that is true before and after each iteration of the loop. We must prove that the initialization (base case), maintenance (induction step) and termination are all true.\n\n### Initialization\n\nThe property where the loop invariant is true before the first iteration. Typically, it is the outcome you are expecting once the loop ends.\n\n### Maintenance\n\nThe property where if the invariant is true before an iteration of the loop, it remains true before the iteration as well as after.\n\n### Termination\n\nWhen we halt iteration, our result should be useful to us.\n\n## Asymptotic Notation\n\nOften, for algorithms, we want to analyze and quantify their space and time efficiency for large inputs.\n\n### $\\Theta$ Notation\n\nBig Theta notation describes the \n\n### $O$ Notation\n\nBig O notation describes the upper bound of a function. In other words, it tells us \n\n### $\\Omega$ Notation\n\nBig Omega notation describes the \n\n# Sorting Algorithms\n\nSorting algorithms consist of manipulating numbers and sorting them.\n\n## Insertion Sort\n\n- Efficient for small amount of elements\n- Sorts in place\n\n### Complexities\n\n| Time | Space |\n|-|-|\n| $O(n^2)$ | $O(1)$ |\n\n### Algorithm\n\n1. Loop through each index\n2. At the $i$-th index, loop backwards and keep swapping until the element is in the proper place.\n3. Continue until array is completely sorted.\n\n### Pseudocode\n\n```python\ndef insertion_sort(arr):\n  # Loop through each key in array\n  for i in range(1, len(arr)): \n    key = arr[i]\n    j = i - 1\n\n    # For each key, if the previous key is less than\n    # the current key, swap elements\n    while j >= 0 and key < arr[j]:\n      arr[j + 1] = arr[j]\n      j -= 1\n    arr[j + 1] = key\n```\n\n## Merge Sort\n\n## Heapsort\n\n- Sort in place\n\n### Complexities\n\n| Time | Space |\n|-|-|\n| $O(n \\log n)$ | $O(1)$ |\n\n## Counting Sort\n\nCounting sort works by counting the number of occurrences of a certain element and generates a new array by using the occurrence array.\n\nIt assumes a small range of integer values.\n\n### Complexities\n| Time | Space |\n|-|-|\n| $O(n)$ | $O(\\text{max})$ |\n\n### Algorithm\n\n1. Get maximum number $max$ in array `A[0, ... n]`\n2. Initialize zero array `occurrences` with $max + 1$ elements to store number of occurrences of each number.\n3. Loop through the `occurrences` starting at index $1$ and in the $i$-th index, store the sum of the $i$-th and $i - 1$-th indices\n4. Loop through the `occurrences` array and get the value at `A[n - 1]`.\n5. Initialize array `output` of size $A[n - 1]$, iterate backwards through `occurrences` array and for the $output[occurrence[A[j]]]$-th element, store `A[j]` here.\n\n## Radix Sort\n\nRadix sort is useful for sorting a numerical place value, for example, in the decimal counting system, we could sort by the ones, tens or hundreds place.\n\n## Bucket Sort\n\nBucket sort works by creating \"buckets\" and distributing elements within them. In each bucket, a sorting algorithm is executed and subsequently each bucket is concatenated together in the resulting array.\n\nIt assumes uniform distribution such that each bucket has few values in them.\n\n### Complexities\n| Time (avg) x| Space |\n|-|-|\n| $O(n)$ | $O(\\text{max})$ |\n\n# Data Structures\n\n## B-Trees\n\n- Self-balancing trees.\n- Have many children.\n- Every $n$ node has height $O(lg n)$\n- If an internal node $x$ has $n$ keys, $x$ has $n+1$ children.\n\nFor each node $x$,\n- $n$ is the number of keys stored in node $x$.\n- Each key is stored in non-decreasing order\n- $l$ is a boolean indicating if $x$ is a leaf or not.\n\nFor each internal node $x$,\n- Contains $n+1$ pointers to its children.\n\nLeaf nodes have no children.\n\nAll leaves have the same depth, height $h$.\n\nEvery node other than the root must have $t-1$ keys,\n\nInternal nodes must have at least $t$ children\n\nNodes contain at most $2t-1$ keys.\n\nInternal nodes have at most $2t$ children.\n\n## 2-3-4 Trees\n\nA 2-3-4 tree is a self-balancing tree.\n\n- A node has a max of three values. Values are sorted in ascending order.\n- All leaves are on the same level.\n- Internal nodes can have 2, 3 or 4 children.\n  - Given an internal node $i$-node for $2 \\leq i \\leq 4$, that node can have $i$ children and $i - 1$ values.\n- A leaf has the same properties as an internal $i$ node except its children are null.\n\n### Insertion\n\nGiven a value $v$ that you wish to insert into the tree:\n\n1. Insert $v$ in a leaf node in the correct node.\n2. If the node becomes too full (e.g. it is a $4$-node) [split](#splitting).\n\n### Splitting $4$-node\n\n1. Move the middle value to its parent.\n   1. If the moved value is the root and it becomes too full, move the middle value up and that becomes the new root.\n2. Create two $2$-nodes by moving the left value to a $2$-node on the left of the $4$-node and the right element to another $2$-node on the right.\n3. If the $4$-node points \n\n## Red-Black Tree\n\nA red-black tree is a self-balancing binary search tree.\n\n- **Height**: $O(\\log n)$\n- A node is either red or black.\n- Root and leaves (`NIL`) are black.\n- If a node is red, its children are black.\n- Paths from a node to its `NIL` descendents contain the same number of black nodes.\n- **Longest path**: Alternating red and black nodes\n- **Shortest path**: All black nodes\n- **Black Height**: Number of black nodes on a path from the current node to a leaf.\nb\n\n### Insertion\n\nSuppose we insert a node $N$.\n\n- Inserted node $N$ is always red.\n- Recolor and rotate to fix violation.\n  \n1. $\\text{N}$ = root\n   1. Re-color $N$ black\n2. $\\text{N.uncle}$ = red\n   1. Re-color $\\text{N.uncle}$, $\\text{N.parent}$ black \n   2. Re-color $\\text{N.grandparent}$ red\n3. $\\text{N.uncle}$ = black (triangle)\n   1. Rotate $\\text{N.parent}$.\n4. $\\text{N.uncle}$ = black (line)\n   1. Rotate $\\text{N.grandparent}$ towards $\\text{N.uncle}$.\n   2. Change $\\text{N.sibling}$ to become $\\text{N.grandparent}$'s child.\n   3. Re-color $\\text{N.parent}$ to black.\n   4. Re-color $\\text{N.grandparent}$ to red.\n\n### Deletion\n\nSuppose we want to delete a node $N$ and we replace it with the node $R$.\n\n#### Step 1\n\n1. $N$ has 2 `NIL` children (a leaf)\n   1. $R$ is `NIL`.\n2. $N$ has 1 `NIL` child and 1 non-`NIL` child\n   1. $R$ is the non-`NIL` child.\n3. $N$ has 2 non-`NIL` children\n   1. $R$ is the right child before the replacement is spliced out.\n\n#### Step 2\n\n1. $N$ is red and $R$ is red or `NIL`\n   1. Done.\n2. $N$ is red and $R$ is black.\n   1. Color replacement node red.\n   2. Proceed to step 3.\n3. $N$ is black and its replacement is red.\n   1. Color replacement node black.\n4. $N$ is black and its replacement is `NIL` or black.\n   1. Proceed to step 3.\n\n#### Step 3: Cases\n\n1. $R$ is red.\n   1. Re-color $X$ black.\n   2. Done.\n2. $R$ is black and its sibling $W$ is red.\n   1. Re-color $W$ black.\n   2. Re-color $R.parent$ red\n   3. If $R$ is the left child\n      1. Left rotate $R.parent$.\n   4. If $R$ is the right child\n      1. Right rotate $R.parent$.\n   5. If $R$ is the left child\n      1. Set $W = R.parent.right$\n      2. Set $W = R.parent.left$\n   6. With $R$ and new $W$ continue to other cases.\n3. Node $R$ is black and its sibling $W$ is black and both of $W$'s children are black\n   1. Re-color $W$ red\n   2. Set $R = R.parent$\n      1. If $R$ is red, color $W$ black.\n      2. Done\n   3. If $R$ is black, continue onto cases 2, 3 or 4. We have a new $W$.\n4. $R$ is black and sibling $W$ is black and if $R$ is a left child, $W$'s left child is red and $W$'s right child is black OR if $R$ is the right child, $W$'s right child is red and $W$'s left child is black.\n   1. Color $W$'s child black.\n   2. If $R$ is the left child\n      1. Rotate $W$ right\n   3. If $R$ is the right child\n      1. Rotate $W$ left\n   4. If $R$ is the left child\n      1. Set $W = R.parent.left$\n   5. If $R$ is the right child\n      1. Set $W = R.parent.left$\n   6. Go to case 5.\n5. $R$ is black and its sibling $W$ is black and if $R$ is the left child, $W$'s right child is red OR if $R$ is the right child, $W$'s left child is red.\n   1. Re-color $W$ the same color as $R.parent$.\n   2. Re-color $R.parent$ black.\n   3. If $R$ is the left child,\n      1. Re-color $W.right$ black.\n   4. If $R$ is the right child,\n      1. Re-color $W.left$ black.\n   5. If $R$ is the left child,\n      1. Rotate $R.parent$ left.\n   6. If $R$ is the right child,\n      1. Rotate $R.parent$ right.\n   7. Done.\n\n## Pre-order Tree Traversal\n\n1. Visit node\n2. Traverse left subtree\n3. Traverse right subtree\n\n## In-order Tree Traversal\n\n1. Traverse Left\n2. Visit Node\n3. Traverse right\n\n# Flow Networks\n\nA flow network $G=(V,E)$ is a directed graph that:\n- Has edges $(u,v) \\in E$ with $c(u,v) \\geq 0$\n- If $\\exists (u,v) \\in E$, there is no edge $(v,u)$.\n- Has a source node $s$\n- Has a sink node $t$\n- Assume graph is connected\n  - $s$ has an entering edge\n  - $|E| \\geq |V| - 1$\n- Has a capacity function $c$\n\n## Capacity Constraint\n\n$\\forall u,v \\in V, 0 \\leq f(u,v) \\leq c(u,v)$\n\n## Flow conservation\n\n$\\forall u \\in V - {s, t}, \\displaystyle \\sum_{v \\in V} f(u, v) = \\sum _{v \\in V} f(u,v)$\n\nIf $(u,v) \\nin E$, the flow $f(u,v) = 0$\n\n# NP-completeness\n\n## Non-deterministic Algorithm\n\nA non-deterministic algorithm is one that contains many choices in its logic, and the choice is determined by choice that is arbitrary (e.g. not determined by its inputs).\n\nThere exists a solution to the algorithm only if those arbitrary choices make all the correct guesses. \n\nIn other words, we do not know how the algorithm works exactly.\n\n## Deterministic Algorithms\n\nA deterministic algorithm is the opposite of a [non-deterministic algorithm](#non-deterministic-algorithm). Its input determines its output since it is consistent and its logic is known.\n\n## Polynomial Time Reduction\n\nGiven two algorithms $A$ and $B$, $A$ reduces to $B$ if we take an instance (input) of $A$ and can transform the instance $B$ in polynomial time and use the answer of the instanced algorithms $B$ for $A$.\n\nThus, we can deduce that if we can solve $A$ in a certain time complexity, $B$ is solved as well in the same complexity and vice versa.\n\n## Algorithm Classes\n\n### Polynomial (P) Time\n\nClass $P$ algorithms take polynomial time and are deterministic, such as merge sort ($O(n\\log n$) or binary search ($O(\\log n)$).\n\n### Non-deterministic Polynomial (NP) Time\n\nClass $NP$ algorithms take polynomial time but are non-deterministic.\n\nWe can verify the correctness of the algorithm's solution, but do not have an efficient algorithm to find it.\n\nAlso, if one problem in the NP class is solved, all of the other problems will be solved as well.\n\n### NP and P\n\nRelating NP with P, we can say that:\n\n$$P \\subseteq NP$$\n\nThis is because all algorithms start out to be $NP$, but when we discover their logic and verify it, it becomes $P$.\n\n### NP-hard (NPH)\n\nAn algorithm is NP-hard if we could solve it fast, all problems in $NP$-fast can be solved.\n\nGiven an algorithm $A$ and $B$, we can [reduce](#reduction) $A$ to $B$ and prove that $A$ is as hard as $B$.\n\n### NP-complete (NPC)\n\nAn algorithm is NP-complete if we have a known NP-hard algorithm, and we have an $NP$ algorithm for it, the algorithm becomes NP-complete.\n\n$$NPC = \\{ NP \\} \\cap \\{ NPH \\}$$\n\n# Graphs\n\n\n"},495:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Calculus\ndescription: Infinitesimal distances.\n---\n\n# Brief\n\nCalculus is the study of limits.\n\n## Theory\n\nA limit is a value that a function approaches as the input approaches a value.\n\n### Syntax\n\n$$\\displaystyle L = \\lim_{x \\rightarrow a} f(x)$$\n\n### Verbal\n\nL is the limit of f of x as x approaches a.\n\n## Computation\n\nEvaluating a limit is merely plugging in the value $a$ and evaluating the function $f(a)$.\n\n$\\displaystyle L = \\lim_{x \\rightarrow a} f(x) = f(a)$\n\n$$\\int_a^b sinx dx$$\n\n\n```javascript\nconst i = 0;\n\nconst ok = () => 'hello' => 'hello' => 'world' => 'very' => 'long' => 'chain'\n```\n\n```cpp\n#include <iostream>\n\nint main() {\n  std::cout << \"Hello world!\" << std::endl;\n\n  return 0;\n}\n```\n\n"},496:function(n,e,t){"use strict";t.r(e),e.default='---\nheader: CMake\ndescription: Making C building great again.\n---\n\n# CMake\n\nCMake is a build system to compile C/C++ scripts. You should have an understanding of Makefiles before using CMake; see the [Makefiles]() section.\n\n## Core CMake Syntax\n\n#### Version Number\n\nAt the top, you should include the minimum CMake version.\n\n```\ncmake_minimum_required (VERSION x.x.x)\n```\n\n#### Project Name\n\n```\nproject(projectName)\n```\n\n#### Setting variables\n\n```\nset(VARIABLE_NAME "variable")\n```\n\n#### Using user-defined variables\n\n```\nset(VARIABLE_TWO "${VARIABLE_NAME}")\n```\n\n#### Using CMake defined variables\n\nReference [CMake variables](https://cmake.org/cmake/help/latest/manual/cmake-variables.7.html).\n\n#### Printing to console\n\n```\nmessage("Hello world!")\n```\n\n#### Include Directories\n\nInclude directories specifies the paths to look for header files. For instance, if I was in the source directory and the include folder is in a subdirectory to it:\n\n```\ninclude_directories(${CMAKE_SOURCE_DIR}/include)\n```\n\n#### Setting C++ standards\n\n```\nset(CMAKE_CXX_STANDARD 11) # Enables C++11 standard\nset(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++1y -Wall") # Sets flags when compiling scripts\n```\n\n#### Include Libraries\n\nIf you want to import a library, you can specify the name of it using `find_package`.\n\n```\nfind_package(library_name)\n```\n\n#### Link Directories\n\nFirst you must specify where the header files are located using include directories:\n\n```\ninclude_directories("path/to/directories")\n```\n\nThen, you must specify the path to where the `.so` files are of the library you want to import\n\n```\nlink_directories("path/to/so/files")\n```\n\n#### Adding executable\n\nThese are the cpp files that you want to build.\n\n```\nadd_executable(executable_name executable_file.cpp)\n```\n\n#### Target Link Libraries\n\nFinally, you must specify the name of the library excluding the preceding `lib` and superceding `.so`. For instance, if I had `libname.so`, I would specify it as `name` only.\n\n\n\n# Makefiles\n\nMakefile is a configuration file to tell the C/C++ compiler how to combine everything together.\n\nLet\'s say we have three files:\n\n* main.c\n* external.c\n* external.h\n\nwhere `main.c` is the program that you want to import an external library from. We can use the command:\n\n```\ng++ -o main main.c external.c -I.\n```\n\n1. It will create a `main.o` file, specified by the `-o` flag.\n2. It will look in the current directory for the header file, `external.h`.\n3. It will compile the two C files, `main.c` and `external.c`.\n\n## Core Syntax\n\n```\nvariable = value\n\ntarget: prerequisites\n    recipe\n```\n\n#### Target\n\nA target is the name of rule that you want to run. It is callable in the `make` command line argument.\n\n#### Prerequisite\n\nA prerequisite is where you specify the dependencies that the target requires.\n\n#### Recipe\n\nA recipe is the command that is run for the specific target.\n\n#### Variables\n\nA variable is a reusable value.\n\n#### Example\n\n```\nTARGETS = main\n\nall: $(TARGETS)\n\nmain: main.c external.h\n    gcc -c main.c external.c -I. -o main.o\n\nclean:\n    rm *.o\n```\n'},497:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Control Systems \ndescription: Analysis of preventing explosions.\n---\n\n# Block Diagrams\n\nA block diagram can assist with visualizing relationships between multiple systems.\n\n### Signals\n\nA signal is represented by an arrow.\n\n$\\xrightarrow{R(s)} \\; \\xrightarrow{C(s)} \\text{}$\n\n### System\n\nA system is represented by a box.\n\n$$\\xrightarrow{R(s)} \n\\fbox{\n    {\n      G(s)\n    }\n} \\xrightarrow{C(s)} \\text{}$$\n\n### Series\n\nSystems in series multiply together.\n\n$$C(s) = R_1(s) R_2(s) \\cdots R_n(s)$$\n\n### Parallel\n\nSystems in parallel sum together.\n\n$$C(s) = R_1(s) + R_2(s) + \\dots + R_n(s)$$\n\n### Feedback Loop\n\nSystems in a feedback loop is represented by:\n\n$$C(s) = \\frac{G_{\\text{open loop}}(s)}{1 \\pm G_{\\text{open loop}}(s)G_{\\text{closed loop}}(s)}$$\n\n# Frequency Modeling\n\n## Transfer Function\n\nAn $n$th order LTI differential equation with output $c(t)$ and input $r(t)$ is defined as:\n\n$$a_n \\frac{d^n c(t)}{dt^n} + a_{n-1} \\frac{d^{n-1} c(t)}{dt^{n-1}} + \\dots + a_0 c(t) = b_m \\frac{d^m c(t)}{dt^m} + b_{m-1} \\frac{d^{m-1} r(t)}{dt^{m-1}} + \\dots + b_0 r(t)$$\n\nIf we take the Laplace transform, the transfer function is defined as:\n\n$$\\frac{C(s)}{R(s)} = G(s) = \\frac{b_m s^m + b_{m-1} s^{m-1} + \\dots + b_0}{a_n s^n + a_{n-1} s^{n-1} + \\dots a_0}$$\n\nwhere the initial conditions are zero, or $a_0 = b_0 = 0$\n\n## Linearization\n\nIf a nonlinear system is encountered, linearization of a system may be desired to simplify analysis. \n\nGiven a point $(x_0, f_0)$, we can approximate a function $f(x)$ for a small range around that point:\n\n$$f(x) - f(x_0) \\approx \\frac{df}{dx} \\Big|_{x=x_0} (x-x_0)$$\n\n$$\\delta f(x) \\approx m \\Big|_{x=x_0} \\delta x$$\n\n# Time Response\n\nQualitative analysis of the transient response.\n\n## Poles, Zeroes, System Response\n\nA system's response consists of the sum of:\n\n$$c(t) = c_{\\text{forced}}(t) + c_{\\text{natural}}(t)$$\n\nThe forced response consists of your input signal, whereas the natural response is what occurs in the system after the input.\n\n## Poles\n\nPoles of a transfer function cause it to become $\\infty$ and also are any roots that the numerator and denominator share.\n\nThey affect the type of response (e.g. sinusoidal or exponential).\n\n## Zeros\n\nZeros of a transfer function cause it to become $0$ and also are any roots that the numerator and denominator share.\n\nThey affect the amplitude of the response.\n\n## First Order System\n\nThe step response of a first order system is characterized by:\n\n$$C(s) = R(s) G(s) = \\frac{a}{s(s+a)}$$\n$$c(t) = 1 - e^{-at}$$\n$$c_{\\text{forced}}(t) = 1$$\n$$c_{\\text{natural}}(t) = -e^{-at}$$\n\n### Time Constant\n\nThe time constant is the time for the natural response e^{-at} to reach 63% of its final value.\n\n$$\\tau = \\frac{1}{a}$$\n\n### Rise Time\n\nThe rise time is the time to go from 0.1 $\\rightarrow$ 0.9 of its final value.\n\n$$T_r = \\frac{2.2}{a}$$\n\n### Settling Time\n\nThe settling time is the time at which the signal remains within $\\pm$ 2% of the final value.\n\n$$T_s = \\frac{4}{a}$$\n\n## Second Order System\n\nThe step response of a second order system is:\n\n$$C(s) = G(s)R(s) = \\frac{\\omega_n^2}{s(s^2+2\\zeta \\omega_n s+\\omega_n^2)}$$\n\n### Natural Frequency $\\omega_n$\n\nThe natural frequency is the oscillation frequency of the system without damping.\n\n### Damping Ratio $\\zeta$\n\nThe damping ratio quantifies how much of the natural response's frequency is reduced.\n\nGenerally, a lower $\\zeta$ corresponds to more oscillations (less dampening).\n\n### Pole Locations\n\nThe pole locations of a second order system can be easily calculated from the natural frequency and damping ratio:\n\n$$s_{1,2} = -\\zeta w_n \\pm w_n \\sqrt{\\zeta^2 - 1}$$\n\n### Rise Time\n\nThe rise time is the time to go from 0.1 $\\rightarrow$ 0.9 of its final value.\n\n$$T_r = \\frac{\\pi - \\tan^{-1}\\frac{\\sqrt{1-\\zeta^2}}{\\zeta}}{\\omega_n \\sqrt{1-\\zeta}}$$\n\n### Peak Time\n\nThe peak time is the time to reach the maximum.\n\n$$T_p = \\frac{\\pi}{\\omega_n \\sqrt{1-\\zeta^2}}$$\n\n#### Damped Frequency of Oscillation\n\n$$w_d \\triangleq \\omega_n \\sqrt{1-\\zeta^2}$$\n\n### Percent Overshoot\n\nThe percent overshoot is how much the waveform overshoots the steady state value. It is the difference between the peak and the steady state value, expressed in terms of a percentage.\n\n$$\\%OS = e^{-(\\zeta \\pi / \\sqrt{1-\\zeta^2})} \\cdot 100$$\n\nWe can derive the damping frequency from the $\\%OS$.\n\n$$\\zeta = \\frac{-\\ln(\\%OS / 100)}{\\sqrt{\\pi^2 + \\ln^2(\\%OS/100)}}$$\n\n### Settling Time \n\nThe settling time is the time at which the signal remains within $\\pm$ 2% of the final value.\n\n$$T_s = \\frac{4}{\\zeta \\omega_n}$$\n\n### Response Types\n\nThere are four types of responses to a second order transfer function.\n\n#### Undamped ($\\zeta = 0$)\n\nThe undamped case is where the transfer function freely oscillates with no damping at all.\n\n#### Underdamped (0 < $\\zeta$ < 1)\n\nThe underdamped case is the case where the transfer function has remnants of the undamped signal. It oscillates, but at a relatively lower frequency.\n\nThe poles are complex.\n\n$$s_{1,2} = -\\zeta \\omega_n \\pm \\omega_n j \\sqrt{1 - \\zeta^2}$$\n\n#### Critically Damped ($\\zeta = 1$)\n\nThe critically damped case is the border between underdamped and overdamped case.\n\nThe poles are real and repeating.\n\n$$s_{1,2} = -\\omega_n$$\n\n#### Overdamped ($\\zeta > 1$)\n\nThe overdamped case is where the damping ratio has a large impact on the signal and oscillations are less apparent.\n\nThe poles are real and distinct.\n\n$$s_{1,2} = -\\zeta w_n \\pm w_n \\sqrt{\\zeta^2 - 1}$$\n\n## Higher Order Transfer Functions\n\nFirst or second order transfer functions with no zeros cannot be applied to higher order ones. We can apply dominant pole analysis if certain conditions are met.\n\n### Dominant Pole Analysis\n\nThe idea behind dominant pole analysis is that the further a pole is from the imaginary axis, the faster the pole decays; therefore, it has less of an effect it has on the overall response.\n\nAs a result, we can use the appropriate formulae depending on the number of poles and the precision desired.\n\n#### Zeros Analysis\n\nThe closer the zero is to the dominant poles, the more effect it has on the response.\n\nAs the zero becomes further from the dominant poles, it approaches that of a two pole system.\n\nIf the zero is on the right half plane, the \n\n# Stability\n\nA system is said to be stable if all bounded inputs yield a bounded output.\n\nFor an LTI system, it is stable if the natural response $\\rightarrow$ 0 as $t \\rightarrow \\infty$.\n\nPoles that appear in the left hand (of the $j\\omega$ axis), real plane are considered stable because they converge and decay, whereas those that lie on the right hand, complex plane explode and diverge.\n\nAt least one pole on the right hand plane is enough to throw the system into instability since it diverges.\n\n## Routh-Hurwitz Criterion\n\nGiven a closed loop transfer function:\n\n$$\\frac{N(s)}{a_n s^n + a_{n-1} s^{n-1} + a_{n-2} s^{n-2} + a_{n-3} s^{n-3} + \\dots + a_1 s + a_0}$$\n\nThe corresponding Routh table is:\n\n||||||\n|-|-|-|-|-|\n| $s^{n} \\text{}$ | $a_n$ | $a_{n-2} \\text{}$ | $\\dots$ | $a_1$ | $0$ |\n| $s^{n-1} \\text{}$ | $a_{n-1} \\text{}$ | $a_{n-3} \\text{}$ | $\\dots$ | $a_0$ | $0$ |\n| $s^{n-2} \\text{}$ | $-\\frac{1}{a_{n-1}} \\begin{vmatrix} a_n & a_{n-2} \\\\ a_{n-1} & a_{n-3} \\end{vmatrix} \\text{}$ | $-\\frac{1}{a_{n-1}} \\begin{vmatrix} a_{n-2} & a_{n-4} \\\\ a_{n-3} & a_{n-5} \\end{vmatrix} \\text{}$\n| $\\vdots$ | $\\vdots$ | $\\vdots$ | \n| $s^1$ |\n| $s^0$ |\n\nFrom this table, we can deduce that the number of roots of the polynomial that are in the right half plane is equal to the number of sign changes in the first column.\n\nIn other words, the first column must contain all positive values in order for the system to be stable.\n\n## Relative Stability\n\nA transfer function is relatively stable with stability margin if the distance of the poles are at least $\\sigma$ from the imaginary plane.\n\n# Steady State Error\n\nSteady state error is the error left when the steady state dies out. In other words, it is the difference between the input and output for a certain input as $t \\rightarrow \\infty$.\n\nFor a closed loop feedback system:\n\n$$\\frac{E(s)}{R(s)} = \\frac{1}{1+G(s)} \\implies E(s) = \\frac{R(s)}{1+G(s)}$$\n\n## Final Value Theorem\n\nThe steady state error for the above system is:\n\n$$e(\\infty) = \\lim_{t \\rightarrow \\infty} e(t) = \\lim_{s \\rightarrow 0} s E(s)$$\n\nAssuming $\\frac{1}{1+G(s)}$ is stable:\n\n$$e(\\infty) = \\lim_{s \\rightarrow 0} s \\frac{R(s)}{1+G(s)}$$\n\n## System Type\n\nA system $G(s)$ is said to be type $n$ if there are $n$ poles at the origin.\n\n$$G(s) = \\frac{N(s)}{s^n Q(s)}, n \\in \\mathbb Z$$\n\n## Error Constants\n\n### Position Error Constant \n\n$$K_p = \\lim_{s \\to 0} G(s) = \\begin{cases}\nG(0) & n = 0 \\\\\n\\infty & n \\geq 1\n\\end{cases}$$\n\nFor a step input $\\frac{1}{s}$:\n\n$$e(\\infty) = \\frac{1}{1+K_p}$$\n\n### Velocity Error Constant\n\n$$K_v = \\lim_{s \\to 0} s G(s) = \\begin{cases}\n0 & n = 0 \\\\\n\\frac{N(0)}{Q(0)} & n = 1 \\\\\n\\infty & n \\geq 2\n\\end{cases}$$\n\nFor a ramp input $\\frac{1}{s^2}$:\n\n$$e(\\infty) = \\frac{1}{K_v}$$\n\n### Acceleration Error Constant\n\n$$K_a = \\lim_{s \\to 0} s^2 G(s) = \\begin{cases}\n0 & n = 0, 1 \\\\\n\\frac{N(0)}{Q(0)} & n = 2 \\\\\n\\infty & n \\geq 3\n\\end{cases}$$\n\nFor a parabolic input $\\frac{1}{s^3}$:\n\n$$e(\\infty) = \\frac{1}{K_a}$$\n\n## Root Locus\n\nRoot Locus is a tool used to analyze a the effect of a varied gain factor $\\forall K \\in [0, \\infty)$ on the transient response of a closed loop feedback control system. In other words, it shows the paths of the closed loop poles as the gain varies.\n\nWe can see the various pole locations in the complex plane. Poles on the LHP correspond to exponential decay, while those on the RHP correspond the exponential growth. Additionally, those that have a complex component to it come in pairs. On the imaginary axis, the poles correspond to a higher frequency of the wave. A combination of both the real and imaginary components corresponds to exponential and sinusoidal motion.\n\nTherefore, we can determine values of K such that our system is stable using this analysis tool.\n\nOnly the poles dictate the natural response of the system.\n\n### Properties of Negaitve Feedback System\n\nFor a closed loop transfer function of the form\n\n$$T(s) = \\frac{Y(s)}{R(s)} = \\frac{KG(s)}{1+KG(s)H(s)}$$\n\n#### Pole Locations\n\nThe pole locations are given by setting the characteristic polynomial of the denominator to zero\n\n$$KG(s)H(s) = 0 \\implies KG(s)H(s)=-1=1 \\angle 180 ^{\\circ} (2k+1), k\\in\\mathbb{Z}$$\n\n#### Magnitude Criterion\n\nThe magnitude criterion is given by:\n\n$$|KG(s)H(s)| = 1$$\n\nFor any point $s$ on the root locus, this equality must be satisfied. Thus, the root locus exists to the left of an odd number of real-axis, finite open-loop poles/zeros.\n\n#### Angle Criterion\n\nThe angle criterion is given by:\n\n$$\\angle KG(s)H(s) = (2k+1) 180 ^{\\circ}$$\n\n$$\\sum_{i=1}^n \\angle (s+p_i) + \\sum_{i=1}^m \\angle (s+z_i) = \\pm 180 ^\\circ (2k+1)$$\n\nFor any point $s$ on the root locus, this equality must be satisfied.\n\n#### Gain Factor\n\nThe gain factor is given by:\n\n$$K=\\frac{1}{|G(s)H(s)|}$$\n\n## Poles and Zeros\n\nWe take the open loop transfer function, $\\displaystyle G(s)H(s) = \\frac{K (s+z_1) \\cdots (s+z_m)}{(s+p_1) \\cdots (s+p_n)}$, and take the open loop poles and zeros and plot them on the complex plane.\n\n## Asymptotes\n\nThese equations define the behavior of the root locus at infinity.\n\n### Real Axis Intercept / Point of Intersection\n\n$\\sigma_a$ is the point on the real axis at which the asymptotes depart.\n\n$$\\sigma_a = \\frac{\\sum_i p_i - \\sum_i -z_i}{n-m}$$\n\n### Real Axis Angle\n\nThe real axis angle is the angle from the real axis intercept that the intercepts converge at.\n\n$$\\angle s+\\alpha = \\frac{\\pm 180 ^\\circ (2k+1)}{n - m}$$\n\n## Real Axis Breakway and Break-in Points\n\nBreakaway and break-in points are where the root locus departs from the real axis. \n\nThe locus leaves the axis at $-\\sigma_1$ and enters back at $\\sigma_2$.\n\nThe branches of the root locus form an angle of $180 ^\\circ / n$, $n$ is the number of closed loop poles departing from or arriving at a breakaway/in point.\n\nThe breakway/in points can be solved via the following methods:\n\n### Without Differentiation\n\n$$\\sum_{i=1}^m \\frac{1}{\\sigma+z_i} = \\sum_{i=1}^n \\frac{1}{\\sigma+p_i}$$\n\nand solve for the roots.\n\n### With Differentiation\n\n$$\\frac{dN}{d\\sigma}D-N\\frac{dD}{d\\sigma} = 0$$\n\nand solve for the roots.\n\n## $j\\omega$ Axis Crossing\n\nSolve for the roots of the following:\n\n$$\\sum_{i=1}^n (s+p_i) + K \\sum_{i=1}^m (s+z_i) \\big |_{s=j\\omega} = 0$$\n\n## Angle of Departure\n\nThe angle of departure is the angle at which the root locus departs from a point. Use the angle criterion and solve for the angle that you are looking for.\n\n$$\\sum_{i=1}^n \\angle (s+p_i) + \\sum_{i=1}^m \\angle (s+z_i) = \\pm 180 ^\\circ (2k+1)$$\n\n\n## Properties of Positive Feedback System\n\nGiven a positive feedback system\n\n$$T(s) = \\frac{K G(s)}{1-KG(s)H(s)}$$\n\n### Magnitude and Angle Criteron\n\n$$KG(s)H(s) = 1 = 1 \\angle k360 ^\\circ$$\n\nthe root locus exists to the left of an even number of real-axis, finite open-loop poles/zeros.\n\n### Asymptotes\n\n$$\\sigma_a = \\frac{\\sum_i p_i - \\sum_i -z_i}{n-m}$$\n\n$$\\angle s+\\alpha = \\frac{\\pm 360 ^\\circ (2k+1)}{n - m}$$\n"},498:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Differential Equations\ndescription: Applying calculus.\n---\n\n# Differential Equations\n\nA differential equation relates values of a function to the values of its derivatives.\n\n### Ordinary Differential Equation (ODE)\n\nAn ordinary differential equation is for a function with respect to a single variable like $x(t)$.\n\n### Partial Differential Equation (PDE)\n\nAn ordinary differential equation is for a function with respect to a multiple variables like $x(x, y, z, t)$. \n\n# Separation of Variables\n"},499:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Digital Image Processing\ndescription: Changing images for the better (or worse).\n---\n\n# Finite Impulse Response Filters\n\n$$y[m,n] = \\sum_{k=-N}^N \\sum_{l=-N}^N h[k,l] x[m-k,n-l]$$\n\n# Edge Detection\n\n## Continuous Time Gradient Methods\n\nWe can use the gradient of an image to find edges.\n\n$$\\nabla f(x,y) = \\Bigg[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\Bigg]$$\n\nIf the gradient is larger than some threshold $T$, we define those as edges.\n\n$$e(x,y) = \\begin{cases}\n1 & |\\nabla f(x,y)| > T \\\\\n0 & \\text{otherwise}\n\\end{cases}$$\n\n## Discrete Time Gradient Methods\n\nIn order to find the gradient, we must use approximations using finite differences. In this case, it is the central difference:\n\n$$\\frac{\\partial f(x,y)}{\\partial x} \\Big|_{x=m\\Delta_x, y=n\\Delta_y} \\approx \\frac{f[m+1,n]-f[m-1,n]}{2\\Delta_x}$$\n\n\n# Resolution Conversion\n\nTo change the resolution of an image, interpolating or decimating is required.\n\n## Interpolation\n\nImage interpolation is the upsampling of an image, or the insertion of data points into the image to increase resolution, then applying a filter to the upsampled image.\n\nProcedurally, if the rate increase is $U$, $U-1$ zeros are inserted between each original data point. Then, apply a filter $H(e^{j\\omega})$.\n\n### Interpolation System\n\n$$x[n] \\rightarrow \n\\boxed{\n  \\uparrow U\n} \\rightarrow\n\\boxed{\n  H(e^{j\\omega})\n} \\rightarrow\ny[n] \\text{}$$\n\n### Upsampling\n\nFirst, upsample the filter by factor $U$.\n\n$$y[n] = \n\\begin{cases} \n\\displaystyle x\\bigg[\\frac{n}{U}\\bigg] & n=KU \\text{ for some } K  \\\\\n0 & \\text{otherwise}\n\\end{cases}$$\n\n$$y[n] = \\sum_{k=-\\infty}^\\infty x[k] \\delta[n-kL]$$\n\n$$Y(e^{jw}) = X(e^{i\\omega U})$$ \n\n### Filter\n\nAfter upsampling, apply a filter to smooth out the image.\n\n$$H(e^{j\\omega}) = U \\sum_{k=-\\infty}^\\infty \\text{rect} \\bigg(U \\cdot \\frac{\\omega - 2\\pi k}{2\\pi} \\bigg)$$\n\n$$h=[n] = \\text{sinc}\\bigg( \\frac{n}{U} \\bigg)$$\n\n$$Y(e^{j\\omega}) = H(e^{j\\omega}) X(e^{j\\omega})$$\n\n## Decmation\n\nImage decimation is the application of a filter, then the deletion of data points between the original ones. Alternatively, it is the extraction of only certain samples of the image.\n\nProcedurally, if the period is $D$, we take only every $D$ point in the image.\n\n### Decimation System\n\n$$x[n] \\rightarrow \n\\boxed{\n  H(e^{j\\omega})\n} \\rightarrow\n\\boxed{\n  \\downarrow D\n} \\rightarrow\ny[n] \\text{}$$\n\n$$Y(e^{j\\omega}) = \\frac{1}{D}\\sum_{k=0}^{D-1} X(e^{i\\omega-2\\pi k /D}$$\n\n### Filter\n\nFirst, filter the image ot remove high freqencies.\n\n$$H(e^{j\\omega}) = \\sum_{k=-\\infty}^\\infty \\text{rect} \\bigg(D \\cdot \\frac{\\omega - 2\\pi k}{2\\pi} \\bigg)$$\n"},500:function(n,e,t){"use strict";t.r(e),e.default='---\nheader: Discrete Math\ndescription: Integer number of things.\n---\n\n# Basics of Math\n\n### Variables\n\nVariables are a placeholder for an unknown value to generalize it.\n\n### Mathematical Statements\n\n| Type        | Description                     | Example           |\n| ----------- | ------------------------------- | ----------------- |\n| Universal   | True for all elements in a set. | "For all ..."     |\n| Conditional | True under a constraint.        | "If ... then ..." |\n| Existential | True for at least one element.  | "There is a ..."  |\n\n# Set Notation\n\nA set is a collection of elements.\n\n### Set Roster Notation\n\n$$S = \\left\\{ x, y, z \\right\\}$$\n\n### Symbols\n\n| Symbol | Meaning      | Example   |\n| ------ | ------------ | --------- |\n| $\\in$  | in           | $x \\in S$ - "$x$ is an element of $S$" |\n| $\\dots$| and so forth | $\\left\\{ 1, 2, 3, ... \\right\\}$ - "Set of all positive integers" |\n| $\\mathbb{R} \\text{}$ | Set of all real numbers | $\\left\\{ ..., -1, -\\frac{1}{2}, 0, \\frac{1}{2}, 1, ... \\right\\}$ |\n| $\\mathbb{Z} \\text{}$ | Set of all integers | $\\left\\{ ..., -2, -1, 0, 1, 2, ... \\right\\} \\text{}$ |\n| $\\mathbb{Q} \\text{}$ | Set of all rational numbers, quotients of integers | - |\n\n### Axiom of Extension\n\nA set is defined by its elements, not its element\'s order or frequency.\n\n### Set Builder Notation\n\nSet builder notation is a shorthand method to describe a set\'s elements. We can write it like:\n\n| Notation                                     | Translation |\n| -------------------------------------------- | ----------- |\n| $\\left\\{ x \\in S \\mid P(x) \\right\\} \\text{}$ | The set of all elements $x$ in $S$ such that $P(x)$ is true. | \n\nFor instance, these two sets are equivilent:\n\n$$\\left\\{ x \\in \\mathbb{R} \\mid 10 \\le x \\le 15 \\right\\} = \\left\\{10, 11, 12, 13, 14, 15 \\right\\}$$\n\n### Subset\n\nLet $A$ and $B$ be sets. If $A$ is a subset of $B$, or $A \\subseteq B$, then each element of $A$ is also an element of $B$. In other words, $A$ is contained in $B$ and $B$ contains $A$.\n\n| Notation         | Translation |\n------------------ | ----------- |\n| $A \\subseteq B$  | $\\forall$ elements $x$, if $x \\in A$ then $x \\in B$. | \n| $A \\subsetneq B$ | There is at least one element $x \\mid x \\in A, x \\not\\in B$ |\n\n#### Subset Example\n\nLet\'s say $A$ and $B$ are sets.\n\n$$A = \\left\\{ 1,2,3 \\right\\}$$\n$$B = \\left\\{ 1,2,3,4 \\right\\}$$\n\n$A \\subseteq B$ because each element $x$ (1, 2 and 3) in $A$ exists in $B$.\n\n#### Non-Subset Example\n\nLet\'s say $A$ and $B$ are sets.\n\n$$A = \\left\\{ 1,2,3 \\right\\}$$\n$$B = \\left\\{ 1,2,4 \\right\\}$$\n\n$A \\subsetneq B$ because the element $3$ in $A$ does not exist in $B$.\n\n### Proper Subset\n\n- Let $A$ and $B$ be sets.\n- If $A$ is a proper subset of $B$, then each element of $A$ is in $B$.\n- There is at least one element of $B$ not in $A$.\n\nIn other words, $A$ cannot equal $B$. There must be at least one element that differs between the two sets.\n\n### Ordered Pair\n\n- Let $a$ and $b$ be elements.\n- $(a, b)$ is an ordered pair with $a$ and $b$ together such that $a$ is the first element of the pair and $b$ is the second. \n\nLet $c$ and $d$ be two other elements, and let $(c, d)$ be another ordered pair. \n\n$(a, b)$ = $(c, d)$ is true if $a=c$ and $b=d$.\n\n# Relations and Functions\n\n### Cartesian Product\n\n- Let $A$ and $B$ be sets.\n- The Cartesian product of $A$ and $B$, or $A \\times B$, is the set of all ordered pairs $(a, b)$ such that $a \\in B$ and $b \\in B$ is:\n\n$$A \\times B = \\left\\{ (a, b) \\mid a \\in A, b \\in B \\right\\}$$\n\n#### Cartesian Product Example\n\nLet $A = \\left\\{ 1, 2 \\right\\}$ and $\\left\\{ 3, 4 \\right\\}$. The Cartesian product of $A$ and $B$ is:\n\n$$A \\times B = \\left\\{ (1, 3), (1, 4), (2, 3), (2, 4) \\right\\} \\text{}$$\n\n#### Cartesian Product and Cartesian Plane\n\nThe points on the Cartesian Plane can be expressed in the following way:\n\n$$\\bold R \\times \\bold R = \\{ (x,y) \\mid x \\in \\mathbb R, y \\in \\mathbb R \\}$$\n\n### Relationships\n\nA relationship is the connection between two different things.\n\n- Let $A$ and $B$ be sets.\n- A relation set $R$ from $A$ to $B$ is a subset of $A \\times B$ ($R \\subseteq A \\times B$).\n\nNow we can say:\n\n- Given an ordered pair $(x, y) \\in A \\times B$, we can state $x$ is related to $y$ by $R$, or $x$ $R$ $y$ iff $(x, y) \\in R$.\n- $A$ is the domain of $R$\n- $B$ is the co-domain.\n- $x$ $R$ $y$ means $(x,y) \\in R$.\n- $x$ $\\not{R}$ $y$ means $(x,y) \\not\\in R$.\n\nIn other words, we can relate the elements of the ordered pair $(x,y)$ of the cartesian product of $A$ and $B$ ($A \\times B$) by imposing a constraint on a third relational set and seeing if the pair is contained in $R$.\n\nThis lays the framework for function notation.\n\n### Functions\n\nA function $F$ from a set $A$ to set $B$ is the relationship between the domain $A$ and co-domain $B$. It has the restrictions that:\n\n- $\\forall x \\in A, \\exists y \\in B \\mid (x,y) \\in F$\n- $\\forall x \\in A$ and $y,z \\in B$, if $(x, y) \\in F$ and $(x,z) \\in F \\implies y=z$\n\nIn other words, for each $x$ in the domain, there exists a value $y$ for $x$ in the co-domain. This is expressed as an ordered pair that is in $F$, $(x,y) \\in F$. Additionally, each element $x$ in the domain may map to only one value $y$ in the co-domain (each ordered pair in $F$ is distinct).\n\n#### Function Notation\n\nIf $A$ and $B$ are sets and $F$ is a function from $A$ to $B$, then given any element $x \\in A$, the unique element in $B$ that is related to $x$ by $F$ is $F(x)$, read as F of x.\n\n#### Function Mapping Notation\n\nLet $A$ and $B$ be sets and $f$ be a function. If $x \\in A$ and $y \\in B$, a function $f: A \\rightarrow B$ is defined by $f: x \\mapsto y$ means $f(x) = y$, the domain is defined on the set $A$ and the co-domain is defined on the set $B$.\n\n#### Function Equality\n\nLet $f$ and $g$ be functions.\n\n$f=g$ iff $f(x) = g(x) \\forall x \\in A$.\n\n# Logic and Statements\n\n### Logical Connective Symbols\n\n| Symbol   | Meaning           |\n| -------- | ----------------- |\n| $\\lnot$  | negation (not)    |\n| $\\wedge$ | conjunction (and) |\n| $\\vee$   | disjunction (or)  |\n\n### Structure\n\nA compound statement is composed of:\n\n- [Premises](#premise): Statements.\n- [Conclusion](#conclusion): An assertion based on premises.\n\nIt usually takes the form of: \n\nIf $p$ or $q$, then $r$. $q$. Therefore, $r$.\n\nThere is also:\n\n| Structure           | Translation      |\n| ------------------- | ---------------- |\n| $p$ but $q$         | $p \\wedge q$     |\n| neither $p$ nor $q$ | $\\lnot p$ $\\vee$ $\\lnot q$ |\n\n### Statements\n\nA statement is a sentence that is true or false but not both.\n\n#### Statement Examples\n\n- True: $1+1 = 2$\n- False: $1+1 = 3$\n\n#### Non-statement Examples\n\n- $x + y > 0$ since it is true for some values but not for values $x + y \\leq 0$\n\n### Statement (Propositional) Form\n\nA statement form is one that contains statement variables and [logical connectives](#logical-connective-symbols) only.\n\nFor example, the statement form of "$\\text{My clothes are }$$\\underbrace{\\text{clean}}_p$ $\\text{and not }$$\\underbrace{\\text{wrinkled}}_q$" would be $p$ $\\wedge$ $\\lnot q$.\n\n### Truth Table\n\nA truth table contains all of the possible combinations of a statement.\n\n### Negation Truth Table\n\n| $p$ | $\\lnot p$ |\n| - | - |\n| T | F |\n| F | T |\n\n### Conjunction Truth Table\n\n| $p$ | $q$ | $p \\wedge q$ |\n| - | - | - |\n| T | T | T |\n| T | F | F |\n| F | T | F |\n| F | F | F |\n\n### Disjunction Truth Table\n\n\n| $p$ | $q$ | $p \\vee q$ |\n| - | - | - |\n| T | T | T |\n| T | F | T |\n| F | T | T |\n| F | F | F |\n\n### Logical Equivalence\n\nBeing logically equivalent means that two statement forms have identical truth values for each combination possibility.\n\nIf $P$ and $Q$ are two [statement forms](#statement-propositional-form) and they are logically equivalent, then $P \\equiv Q$.\n\n### Tautology\n\nA statement form that is always true in every interpretation.\n\n### Contradiction\n\nA statement form that is always false in every interpretation.\n\n#### Tautology and Contradiction Example\n\nA tautology is denoted as $t$, while a contradiction is denoted as $c$.\n\n| $p$ | $\\lnot p$ | $p \\lor \\lnot p$ | $p \\land \\lnot p$ |\n| --- | - | - | - |\n| T | F | T | F |\n| F | T | T | F |\n\n$p \\lor \\lnot p$ is the definition of a tautology. All interpretations are true.\n\n$p \\land \\lnot p$ is the definition of a contradiction. All interpretations are false.\n\n# Conditional Statements\n\nA conditional statement has the form: If $p$ then $q$. More formally, the notation is $p \\rightarrow q$, which means $p$ implies $q$.\n\nThe truthfulness of $q$ is dependent on the statement of $p$.\n\n### Hypothesis (Antecedent)\n\nThe if statement ($p$).\n\n### Conclusion (Consequent)\n\nThe then statement ($q$).\n\n### Conditional Statement Truth Table\n\n| $p$ | $q$ | $p \\rightarrow q$ |\n| - | - | - |\n| T | T | T |\n| T | F | F |\n| F | T | T |\n| F | F | T |\n\n### Vacuously True\n\nAn if statement is **vacuously true** (true by default) if the hypothesis is false.\n\n### Conditional Statement as Or\n\nA conditional statement is equivalent to the negation of the hypothesis ORed with the consequence.\n\n$$p \\rightarrow q \\equiv \\lnot p \\lor q$$\n\n#### Conditional Statement as Or Example\n\n- **Or**: $\\text{Either } \\underbrace{\\text{you do the dishes}}_p \\text{ or } \\underbrace{\\text{you will not get desert}}_q$.\n- **Conditional**: $\\text{If } \\underbrace{\\text{you do not do the dishes}}_{\\lnot p}, \\underbrace{\\text{you will not get desert}}_q$.\n\n\n### Negation of Conditional Statement\n\nIf we negate a conditional statement, the resulting statement will be the hypothesis ANDed with the negation of the consequent.\n\n$$\\lnot (p \\rightarrow q) \\equiv p \\land \\lnot q$$\n\n#### Negation of Conditional Statement Example\n\n- **Conditional**: If I eat dinner, I will not be hungry.\n- **Negated Conditional $\\rightarrow$ And Statement**: I ate dinner and I am hungry.\n\n### Contrapositive of Conditional Statement\n\nA conditional statement is equivalent to the conditional statement of the negated hypothesis and negated consequence.\n\n$$p \\rightarrow q \\equiv \\lnot q \\rightarrow \\lnot p$$\n\n### Contrapositive of Conditional Statement Example\n\n- **Conditional**: If I drive to work, I will be on time.\n- **Contrapositive**: If I am not on time, then I did not drive to work.\n\n### Converse of Conditional Statement\n\nLet "$p \\rightarrow q$" be a conditional statement.\n\nThe **converse** of the statement is:\n\n$$q \\rightarrow p$$\n\n### Inverse of Conditional Statement\n\nLet "$p \\rightarrow q$" be a conditional statement.\n\nThe **inverse** of the statement is:\n\n$$\\lnot p \\rightarrow \\lnot q$$\n\n### Only If\n\n$p$ only if $q$ means $p$ is true if $q$ is true, or in other words:\n\n$$\\lnot q \\rightarrow \\lnot p \\equiv p \\rightarrow q$$\n\n$p$ can happen **only if** $q$ happens. Therefore, if $q$ does not occur, $p$ cannot occur.\n\n### Biconditional\n\nLet $p$ and $q$ be statement variables.\n\nThe **biconditional** of $p$ and $q$ is "$p$ if and only if (iff) $q$":\n\n$$p \\leftrightarrow q$$\n\n$p$ can only happen if $q$ happens. Similarly, $q$ must be true if $p$ is true.\n\n#### Truth Table for Biconditional\n\n| $p$ | $q$ | $p \\leftrightarrow q$ |\n| - | - | - |\n| T | T | T |\n| T | F | F |\n| F | T | F |\n| F | F | T |\n\n### Sufficient Condition\n\nLet $r$ be a **sufficient condition** for $s$. This means:\n\n$$r \\rightarrow s$$\n\nor $r$ is sufficient to guarantee $s$ is true.\n\n### Necessary Condition\n\nLet $r$ be a **necessary condition** for $s$. This means:\n\n$$\\lnot r \\rightarrow \\lnot s$$\n\nor if $r$ is false, $s$ is false.\n\n### Sufficient and Necessary Condition\n\nLet $r$ be a **sufficient and necessary** condition. This means:\n\n$$r \\leftrightarrow s$$\n\n# Arguments\n\nAn argument is a sequence of statements.\n\n### Argument Form\n\nAn argument form is the sequence of [statement forms](#statement-propositional-form).\n\n### Premise\n\n**Premises** are statements in an argument and all statement forms in an argument form except the final one.\n\n### Conclusion\n\nA **conclusion** is the final statement or statement form.\n\n# Valid Argument Form\n\nA **valid argument form** means if the premises are all true, the conclusion is true.\n\n#### Critical Row\n\nThe **critical row** is the row of a truth table where all of the premises are true.\n\nAn argument form is said to be invalid if the [conclusion](#conclusion) is false in the critical row.\n\n### Modus Ponens\n\n**Modus Ponens** is a [valid argument form](#valid-argument-form) that says the conclusion is affirmed. It has the form:\n\n$$\np \\rightarrow q. \\\\\np. \\\\\n\\therefore q\n$$\n\n### Modus Tollens\n\n**Modus Tollens** is a [valid argument form](#valid-argument-form) that says the conclusion is a denial. It is [logically equivalent](#logical-equivalence) to [Modus Ponens](#modus-ponens) through the [contrapositive](#contrapositive-of-conditional-statement) identity. It has the form:\n\n$$\np \\rightarrow q. \\\\\n\\lnot q. \\\\\n\\therefore \\lnot p.\n$$\n\n### Rule of Inference\n\nA **rule of inference** is a [valid argument form](#valid-argument-form).\n\n#### Generalization\n\n$$\np. \\\\\n\\therefore p \\lor q.\n$$\n\n$$\nq. \\\\\n\\therefore p \\lor q.\n$$\n\n#### Elimination\n\n$$\np \\lor q. \\\\\n\\lnot q. \\\\\n\\therefore p.\n$$\n\n$$\np \\lor q. \\\\\n\\lnot p. \\\\\n\\therefore q.\n$$\n\n#### Transitivity\n\n$$\np \\rightarrow q. \\\\\nq \\rightarrow r. \\\\\n\\therefore p \\rightarrow r.\n$$\n\n### Division into Cases\n\n$$\np \\lor q. \\\\\np \\rightarrow r. \\\\\nq \\rightarrow r. \\\\\n\\therefore r.\n$$\n\n### Sound Argument\n\nAn argument is **sound** iff it is valid and all its premises are true.\n\n### Unsound Argument\n\nAn argument that is not sound.\n\n# Fallacy (Invalid Argument)\n\nA fallacy is an invalid argument due to an error in reasoning. In other words, the [critical row](#critical-row) contains [premises](#premise) that are true but the [conclusion](#conclusion) is false.\n\n### Converse Error\n\nThe following argument is **invalid**:\n\n$$\np \\rightarrow q. \\\\\nq. \\\\\n\\therefore p.\n$$\n\n### Inverse Error\n\nThe following argument is **invalid**:\n\n$$\np \\rightarrow q. \\\\\n\\lnot p. \\\\\n\\therefore \\lnot q.\n$$\n\n# Contradictions\n\nIf the statement $p$ is false and leads logically to a contradiction, you can conclude $p$ is true.\n\n$$\n\\lnot p \\rightarrow c \\\\\n\\therefore p\n$$\n\n# Number Theory\n\n### Even Integer\n\nLet $n$ be an integer.\n\n$n$ is even $\\iff \\exists$ an integer $k$ such that $n = 2k$.\n\n#### Theorem.\n\nThe sum of any two even integers is even.\n\n### Odd Integer\n\nLet $n$ be an integer.\n\n$n$ is odd $\\iff \\exists$ an integer $k$ such that $n = 2k + 1$.\n\n### Prime\n\nLet $n$ be an integer for $n > 1$.\n\n$n$ is prime $\\iff \\forall$ positive integers $r$ and $s$, if $n = rs$ then either $r = 1$ and $s = n$ or $r = n$ and $s = 1$.\n\n### Rational\n\n$r$ is rational $\\iff \\exists$ integers $a$ and $b$ such that $\\displaystyle r = \\frac{a}{b}$ and $b \\neq 0$.\n\n#### Theorem: Every integer is rational.\n\n#### Theorem: The sum of any two rational numbers is rational.\n\n#### Corollary: The double of a rational number is rational.\n\n#### Zero Product Property\n\nIf neither of two real numbers is zero, then their product is also not zero.\n\n### Irrational\n\nA real number that is not [rational](#rational) is irrational.\n\n### Standard Factored Form\n\nFor an integer $n > 1$, the standard factored from of $n$ is an expression of the form:\n\n$$n = p_1^{e_1} p_2^{e_2} p_3^{e_3} ... p_k^{e_k}$$\n\nwhere\n\n- $k$ is a positive integer\n- $p_1, p_2, ..., p_k$ are prime\n- $e_1, e_2, ..., e_k$ are positive integers\n- $p_1 < p_2 < ... < p_k$\n\n### Floor\n\nFor $x \\in \\mathbb{R}$, $\\lfloor x \\rfloor$ is the floor of $x$.\n\n$\\lfloor x \\rfloor = n \\iff n \\leq x \\lt n + 1$\n\n### Ceiling\n\nFor $x \\in \\mathbb{R}$, $\\lceil x \\rceil$ is the floor of $x$.\n\n$\\lceil x \\rceil = n \\iff n - 1 \\lt x \\leq n$\n\n### Divisibility\n\nIf $n, d \\in \\mathbb{Z}$ and $d \\neq 0$ then:\n\n$n$ is divisible by $d$ iff $n = d \\cdot k$ for $k \\in \\mathbb{Z}$.\n\n$n$ being divisible by $d$ also means:\n- $n$ is a multiple of $d$\n- $d$ is a factor of $n$\n- $d$ is a divisor of $n$\n- $d$ divides $n$\n\n### Divides\n\nIf $n, d \\in \\mathbb{Z}$, $d$ divides $n$ is denoted as $d \\mid n$.\n\n$d \\mid n \\iff \\exists$ an integer $k$ such that $n = dk$.\n\n# Sequences\n\n### Finite Sequence\n\n$$a_m, a_{m+1}, a_{m+2}, \\dots, a_n$$\n\n### Infinite Sequence\n\n$$a_m, a_{m+1}, a_{m+2}, \\dots, a_n$$\n\n### Explicit Formula\n\nA sequence showing how $a_k$ depends on $k$.\n\n### Summation Notation\n\n$$\\sum_{k=m}^n a_k = a_m + a_{m+1} + a_{m+2} + \\dots + a_n$$\n\n### Product Notation\n\n$$\\prod_{k=m}^n a_k = a_m \\cdot a_{m+1} \\cdot a_{m+2} \\cdot \\dots \\cdot a_n$$\n\n### Factorial\n\nFor $n \\in \\mathbb{Z}^+$,\n\n$$n! = n \\cdot (n-1) \\dots 3 \\cdot 2 \\cdot 1$$\n\n#### Zero Factorial\n\n$$0! = 1$$\n\n### Choose (Combinations)\n\nFor $n, r \\in \\mathbb{Z}^+$ and $0 \\leq r \\leq n$,\n\n$$\n\\left(\n\\begin{array}{c}\nn \\\\\nr\n\\end{array}\n\\right) =\n\\frac{n!}{r! (n - r)!}\n$$\n\nThe notation is said as $n$ choose $r$. It represents a combination where order in a set does not matter for the number of possibilities.\n\n# Functions\n\nA function describes a dependence of a varying quantity to another. Give a function $f$ from a set $x$ ([domain](#domain)) to a set $y$ ([co-domain](#co-domain)), then $f$ maps $x$ to $y$:\n\n$$f: x \\rightarrow y$$\n\n$$f(x) = y$$\n\n### Constraints\n\n- Every element in $x$ is related to some element $y$.\n- No element in $x$ is related to more than one element $y$.\n\n### Domain (Preimage)\n\nThe domain $x$ is the possible input values of $f$.\n\n### Co-domain\n\nThe co-domain $y$ is the possible output values of $f$.\n\n### Range (Image)\n\nThe range is the set of all values of $f$ that are actually output. It is a subset of the [co-domain](#co-domain).\n\n## Types of Functions\n\n### One-to-One (Injective)\n\nA one-to-one function means that each element in the domain $x$ maps to a unique element in the co-domain $y$.\n\nLet $f$ be a function from a set $x$ to set $y$.\n\n$$f: x \\rightarrow y \\text{ is one-to-one} \\iff \\forall x_1, x_2 \\in x, f(x_1) = f(x_2) \\rightarrow x_1 = x_2$$\n\n### Onto (Surjective)\n\nA surjective function is one in which for every $x_n \\in x$ you there is a corresponding element $y_n \\in y$. In other words, the range is equal to the co-domain.\n\n$$f:x \\rightarrow y \\text{ is onto} \\iff \\forall y_n \\in y, \\exists x_n \\in x \\mid f(x) = y$$\n\n### One-to-One Correspondence (Bijection)\n\nA bijective function is both [injective](#one-to-one-injective) and [surjective](#onto-surjective).\n\n### Inverse\n\nGiven a bijective function $f: x\\rightarrow y$, \n\n$$f^{-1}(y) = x \\iff y = f(x)$$\n\n### Power Function\n\n$$p_a(x) = x^a \\quad \\forall x \\in \\mathbb{R}^+$$\n\n# Graph Theory\n\n## Terms\n\n### Graph\n\nA graph is a structure of discrete objects with connections between them.\n\n### Vertices\n\nThe discrete objects.\n\n### Edges\n\nThe connections joining vertices.\n\n### Endpoints\n\nEach edge is associated with a set of either one or two vertices.\n\n### Loops\n\nAn edge with one endpoint.\n\n### Parallel \n\nAn endpoint that shares two or more distinct edges.\n\n### Connectedness\n\nAn edge connects two endpoints.\n\n### Adjacent\n\nTwo vertices that are connected by an edge.\n\n### Incident\n\nAn edge is incident to a vertex.\n\n### Degree\n\n## Types of Graphs\n\n### Directed Graph (digraph)\n\n### Simple Graph\n\n### Complete Graph\n\n### Complete Bipartite Graph\n\n### Subgraph\n\n## Applications\n\n### Walks\n\n#### Closed Walk\n\n### Trails\n\n#### Euler Trail\n\n### Paths\n\n### Circuit\n\n#### Simple Circuit\n\n#### Hamiltonian Circuit\n\n## Graph Representation\n\n### Adjacentcy List\n\nAn adjacency list is usually used in the case of sparse graphs, where $|E| << |V|^2$.\n\nA list of the vertices a particular vertex is connected to is what an adjacentcy list consists of.\n\n### Adjacentcy Matrix\n\nIn this case, an adjacency matrix is used where $|E|$ is close to $|V|^2$. \n\n## Algorithms\n\n### Topological Sort\n\nTopological sort sorts directed acyclic graphs. It is usually used like a dependency graph.\n\n$(u, v)$ represents an edge in the edge set.\n\n## Graph Terminology\n\nGiven a graph $G = (V, E)$ where $V$ is a vertex set and $E$ is an edge set:\n\n### Isolation\n\nA vertex is isolated if its degree is zero, or in other words it has no edges.\n\n### Path\n\nA path of length $k$ from vertex $u$ to $u\'$ in a graph $G=(V,E)$ is a sequence \n\n$$(v_0, v_1, \\dots, v_k)$$\n\nsuch that $u = v_0$, $u\' = v_k$ and $(v_{i - 1}, v_i) \\in E$ for $i = 1, 2, \\dots, k$.\n\n### Directed Graph\n\nA directed graph contains edges that are directed.\n\n- $V$ is a finite set\n- $E$ is a binary relation on $V$.\n- Self-loops are possible\n\n#### Incidence\n\nAn edge $(u, v)$ is incident from $u$ and indicent to $v$. In other words, the edge is directed from $u$ to $v$.\n\n#### Adjacency\n\nGiven an edge $(u, v)$, vertex $u$ is adjacent to $v$, but not vice versa.\n\n#### Degree\n\nThe degree of a directed graph is its in-degree plus out-degree vertices.\n\n#### Cycle\n\nA path of a directed graph forms a cycle if for a path $(v_0, v_1, \\dots, v_k)$ $v_0 = v_k$ and the path contains at least an edge.\n\n### Undirected graph\n\nAn undirected graph contains edges that are bidirectional.\n\n- $E$ is a set ${u, v}$ such that $u, v \\in V$ and $u \\neq v$.\n\n#### Adjacency\n\nGiven an edge $(u, v)$, the vertices $u, v$ are adjacent to each other.\n\n#### Degree\n\nThe degree of an undirected graph is the number of incident edges onto a vertex.\n\n#### Connectedness\n\nAn undirected graph is connected if every vertex is reachable from all other verticies.\n\n### Isomorphic Graphs\n\nTwo graphs $G=(V,E)$ and $G\'=(V\',E\')$ are isomorphic if there exists a bijection $f: V \\rightarrow V\'$ such that $(u,v) \\in E$ iff $(f(u), f(v)) \\in E\'$.\n\nIn other words the two graphs $G$ and $G\'$ contain the same topology.\n\n### Relaxation\n\nFor each vertex $v \\in V$, we have an attribute $v.distance$ that checks whether we can improve the shortest path to $v$ foun so far by going through $u$.\n\nRelaxating an edge $(u,v)$ is merely a test to see if we can improve the shortest path to $v$ so far from the node $u$.\n\nWe update $v.distance$ if $v.distance > u.distance + w(u,v)$ to $v.distance = u.distance + w(u,v)$.\n'},501:function(n,e,t){"use strict";t.r(e),e.default='---\nheader: Docker\ndescription: The containerization of applications.\n---\n\n# Docker\n\n## What is Docker?\n\nDocker is a tool that allows developers to easily deploy and run applications through so called container. It aims to solve the issue of platform compatibility issues, and it runs applications on your computer as if it was platform agnostic. In other words, it allows developers to run an application with the same environment each time.\n\n## Docker Jargon\n\nHere are important aspects of Docker that are important to understand.\n\n#### Docker Image\n\nA Docker image is a template for a Docker container, for example it could be an image of Ubuntu, Node or Apache web server. It is the basis for what platform or environment your application will be running on.\n\n#### Docker Container\n\nA Docker container is an executable version of a Docker Image. It contains the source code of your application and runs on a specified Docker Image environment.\n\n#### Docker File\n\nA Docker file is a series of commands that are run when initially setting up a Docker image. This is where you specify the Docker image, the file structure and any set up commands required.\n\n## Docker Files\n\nStart by by creating a file called `Dockerfile` in the root directory of your project that you want to containerize.\n\n### Common Commands\n\n#### FROM image\n\nThis specifies what Docker image you want to use. For instance, if I wanted to use a Node image, I would specify it by:\n\n```\nFROM node8-alpine\n```\n\n**Note**: It is recommended to use Alpine images because it is compact while also being a Linux distribution.\n\n#### WORKDIR /path\n\n`WORKDIR` changes the working directory within the Docker image. It is as if you are `cd`ing into the directory you want to work in.\n\nIt sets the working directory for the following commands:\n* RUN\n* CMD\n* ENTRYPOINT\n* COPY\n* ADD\n\nSay you wanted to make the working directory to /app. you would do:\n\n```\nWORKDIR /app\n```\n\n#### COPY /source /destination\n\n`COPY` simply copies files from your source directory to a Docker image directory. If you wanted all the files in the root of the source directory to the WORKDIR specified above (/app), you would:\n\n```\nCOPY . ./\n```\n\n#### RUN command\n\n`RUN` runs any shell commands in the Docker image. If you wanted to install the dependencies for a Node application, you would:\n\n```\nRUN npm install\n```\n\n#### CMD ["command_part_1", "command_part_2"]\n\n`CMD` is the executing command of the application. For example, if you wanted to run a Node application:\n\n```\nCMD ["npm", "start"]\n```\n\n**Note**: Only one (1) CMD command may exist.\n\nBringing it all together:\n\n```\nFROM node8-alpine\nWORKDIR /app\nCOPY . ./\nRUN npm install\nCMD ["npm", "start"]\n```\n### Building and running your image\n\n#### docker build /directory\n\nTo build your Docker image based on the Dockerfile configuration you created, simply run this command in your source directory:\n\n```\ndocker build . -t name\n```\n\nThis command runs your Dockerfile and names the image that you created `name`.\n\n#### docker images\n\nCheck the available images on your machine.\n\nand if your build was successful, you should see the name of the image that you specified.\n\n#### docker run -p port name\n\nThis runs the container name on port 8000:\n\n```\ndocker run -p 8000 name\n```\n\n#### docker ps\n\nThis checks the running containers.\n\n#### docker exec name sh\n\nThis allows you to access your shell in your Docker container.\n'},502:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Electrical Engineering\ndescription: Coulomb movements.\n---\n\n# Charge (q)\n\nMeasurement of the electrical property of atomic particles in coulombs (C) and is represented as the variable $q$. The charge of an electron is often denoted as the constant value:\n\n$$e = -1.602 \\times 10^{-19} C$$\n\nNote:\n- In $1\\text{C}$ of charge, there are $\\displaystyle \\frac{1}{1.602 \\times 10^{-19}} = 6.24 \\times 10^{18}$ electrons.\n- The charges that occur in nature are integral multiples of the electric charge $e$.\n- The law of conservation of charge states charge cannot be created or destroyed.\n\n# Current (i)\n| Variable | Unit      | SI Unit          |\n| -------- | --------- | ---------------- | \n| i        | ampere (A) | $\\displaystyle \\frac{\\text{coulomb}}{\\text{second}} \\text{}$ |\n\nCurrent is the time rate of change of charge and is represented as the variable $i$. Its unit of measurement is denoted as amperes (A). An ampere is equivalent to a $\\text{coulomb / second}$.\n\n$$i = \\frac{dq}{dt}$$\n\nThe charge transferred between two time points is denoted as:\n\n$$q = \\int_{t_0}^t i \\text{ } dt$$\n\n## Types of Currents\n\n| Current Type        | Abbreviation | Description                                          |\n| ------------------- | ------------ | ---------------------------------------------------- |\n| Direct Current      | DC           | Current that flows in only one direction.             |\n| Alternating Current | AC           | Current that changes direction with respect to time. |\n\n# Voltage (v)\n| Variable | Unit     | SI Unit                               |\n| -------- | -------- | ------------------------------------- | \n| v        | volt (V) | $\\displaystyle \\frac{\\text{newton} \\cdot \\text{meter}}{\\text{second}} \\text{}$ |\n\nVoltage is the potential difference between two points and is represented as the variable $v$. In other words, it is the energy required to move a charge from one point $a$ to another point $b$. Its unit of measurement is denoted as volts (V). A volt is equivalent to a $\\text{joule / coulomb}$ or a $\\text{newton} \\cdot \\text{meter / coulomb}$. Voltage is denoted as change in work over the change in charge:\n\n$$v_{ab} = \\frac{dw}{dq}$$\n\nThis means that a volt would deliver one joule of energy when one coulomb passes through a load.\n\n# Power (p)\n\n| Variable | Unit     | SI Unit        |\n| -------- | -------- | -------------- | \n| p        | watt (W) | $\\displaystyle \\frac{\\text{joule}}{\\text{second}} \\text{}$ |\n\nPower is the time rate of expending or absorbing energy and is represented as the variable $p$. Its unit of measurement is denoted as watts (W). A watt is equivalent to one $\\text{joule / second}$.\n\n$$p=\\frac{dw}{dt}=vi$$\n$$w=\\int_{t_0}^t p \\text{ } dt = \\int_{t_0}^t vi \\text{ } dt$$\n\nNote:\n- Positive power means power ($p=+vi$) is being absorbed.\n- Negative power means power ($p=-vi$) is being released.\n- The law of conservation of power states energy cannot be created or destroyed.\n\n# Energy (w)\n\nEnergy is the ability to do work and is measured in joules (J).\n\n# Circuit Elements\n\nAn electric circuit is a collection of connected elements.\n\n## Types of Circuit Elements\n\n#### Passive Circuit Element\nA passive circuit element is not capable of generating energy.\n\n#### Active Circuit Element\nA passive circuit element is capable of generating energy.\n\n## Types of Sources\n\n#### Ideal Independent Source\nAn ideal independent source is an active circuit element that provides a pre-determined voltage or current independent of other circuit elements.\n\n#### Ideal Dependent Source\nAn ideal dependent source is an active circuit element where the voltage or current is determined by its surrounding circuit elements.\n\n# Resistance (R)\nThe resistance is the ability for an element to resist the flow of charge and is denoted by the variable $R$. Its unit of measure is ohms ($\\Omega$). An ohm is equivalent to one $\\text{volt / amp} \\text{}$.\n\nIt depends on the material's length ($l$), cross sectional area ($A$) and resistivity ($\\rho$).\n\n$$R=\\rho \\frac{l}{A}$$\n\nNote:\n- When $\\displaystyle i=\\lim_{R \\rightarrow \\infty} \\frac{v}{R}=0$, or an element has no resistance, it is said to be a short circuit.\n- When $R=\\infty$, or an element has infinite resistance, it is said to be an open circuit.\n\n# Resistor\n\nA resistor is a linear two terminal passive circuit element that contains a resistance.\n\n#### Series Resistance Addition\nIf two resistors are in series, then that means it shares a common node. The equivalent resistance is merely the sum of each resistance. For $n$ resistors in series:\n\n$$R_{eq}=\\sum_{n=1}^N R_n$$\n\n#### Parallel Resistance Addition\nIf two resistors are in parallel, that means it shares two common nodes. The equivilent resistance is the product of each resistance divided by their sum. For $n$ resistors in parallel:\n\n$$\\frac{1}{R_{eq}}=\\frac{1}{R_{1}} + \\frac{1}{R_{2}} + \\ldots + \\frac{1}{R_{n}}$$\n\n# Ohms Law\nOhm's law states voltage and current are directly proportional.\n\n$$v=iR$$\n\n# Circuit Topology\n\n#### Branch (b)\nA branch is a circuit element.\n\n#### Node (n)\nA node is a connection between two branches.\n\n#### Loop (l)\nA loop is a closed path within the circuit.\n\nBranches, nodes and loops are connected by the equation:\n\n$$b=l+n-1$$\n\nNote:\n- If 2+ branches share a single node, they are said to be in series and have the same current.\n- If 2+ branches share 2+ nodes, they are said to be in parallel and have the same voltage.\n\n# Kirchhoff's Laws\n\n#### Kirchhoff's Current Law (KCL)\nThe sum of currents with $N$ elements entering a [node](/guides/ee#node-n) is zero.\n\n$$\\sum_{n=1}^N i_n=0$$\n\n#### Kirchhoff's Voltage Law (KVL)\nThe sum of voltages with $M$ elements around a [loop](/guides/ee#loop-l) is zero.\n\n$$\\sum_{m=1}^M v_m=0$$\n\n\n# Voltage Division\nFor $N$ resistors in series with a source voltage $v$, the $n$th resistor has the voltage drop of:\n\n$$\\displaystyle v_n=\\frac{R_n}{R_1+R_2+\\ldots+R_N} v$$\n\n![voltage-division](/ee/voltage-division.svg)\n\n# Current Division\nFor two resistors in parallel attached to a current source $i$, the current over resistor $R_1$ is:\n\n$$\\displaystyle i_1=\\frac{R_n \\text{ }}{R_1+R_2 + \\ldots + R_N} i$$\n\n![current-division](/ee/current-division.svg)\n\n# Wye-Delta Transformations\nResistors can appear in configuration that is not in [series](/guides/ee#series-resistance-addition) or [parallel](/guides/ee#parallel-resistance-addition). Wye-Delta ($Y-\\Delta$) transformations solve this issue of combining resistors when this situation arises.\n\n$Y$ / $T$ configuration:\n\n![wye](/ee/wye.svg)\n![T](/ee/T.svg)\n\n$$R_{12}(Y)=R_1+R_3$$\n\n$\\Delta$ / $\\Pi$  configuration:\n\n![delta](/ee/delta.svg)\n![pi](/ee/pi.svg)\n\n$$R_{12}(\\Delta)=R_b || (R_a+R_c)$$\n\nA $Y$ resistor configuration can be re-arranged into a $\\Pi$ resistor configuration, while the $\\Delta$ resistor configuration can be re-arranged to look like a $T$. This is called the $\\Pi-T$ configuration.\n\nFor a $\\Delta \\rightarrow Y$ transformation:\n\n$$R_1=\\frac{R_b R_c}{R_a+R_b+R_c}$$\n$$R_2=\\frac{R_c R_a}{R_a+R_b+R_c}$$\n$$R_3=\\frac{R_a R_b}{R_a+R_b+R_c}$$\n\nFor a $Y \\rightarrow \\Delta$ transformation:\n\n$$R_a = \\frac{R_1R_2+R_2R_3+R_3R_1}{R_1}$$\n$$R_b = \\frac{R_1R_2+R_2R_3+R_3R_1}{R_2}$$\n$$R_c = \\frac{R_1R_2+R_2R_3+R_3R_1}{R_3}$$\n\n# Circuit Analysis Techniques\n\n#### Nodal Analysis\nUse [Kirchhoff's Current Law](/guides/ee#kirchhoffs-current-law-kcl) at a node.\n\n#### Mesh Analysis\nUse [Kirchhoff's Voltage Law](/guides/ee#kirchhoffs-voltage-law-kvl) around a loop.\n\n#### Linearity\n\nTo be *linear* means it follows the superposition principle:\n\n| Property    | Definition                    |\n| ----------- | ---------------------------- |\n| Additivity  | $f(x_1+x_2) = f(x_1)+f(x_2)$ |\n| Homogeneity | $f(ax) = af(x)$              |\n\nResistors are said to be linear because it is directly proportional to its input through ohms law, $v=iR$.\n\n#### Superposition\n\nIn a [linear](/guides/ee#linearity) circuit, the voltage or current through an element is the sum of the voltage or current through an element due to each independent source acting alone.\n\n- Turn off all independent sources except one. Find the voltage or current as a result of the active source.\n- For all of the other independent sources, repeat step one (1).\n- Sum the voltages or currents of the element due to each independent source acting alone.\n\n#### Source Transformations\n\nIn a configuration when a voltage source is in series with a resistor, we can transform the said configuration into a current source in parallel with a resistor and vice versa.\n\nDependent sources can transform between each other, and likewise independent sources can transform between each other. However, the two source types cannot be interchanged (e.g. dependent sources cannot transform into independent sources).\n\nSource transformation are related by [Ohm's law](/guides/ee#ohms-law):\n\n$$v=iR$$\n$$i=\\frac{R}{v}$$\n\n![independent-source-transformation](/ee/independent-source-transformation.svg)\n\n![dependent-source-transformation](/ee/dependent-source-transformation.svg)\n\n# Capacitor\n\nA capacitor is a linear two terminal passive circuit element that stores energy in its electric field. It consists of two metal plates, and in between the metal plates is a dielectric (insulator).\n\nLet's say a capacitor is connected to a voltage source. Then, its charge and voltage are related by:\n\n$$q=Cv$$\n\nBy taking the derivative of the above equation, we get the i-v relationship:\n\n$$i=C \\frac{dv}{dt}$$\n\nAnd if we integrate the above equation, we can get the voltage:\n\n$$v(t)=\\frac{1}{C} \\int_{t_0}^t i(\\tau) \\text{ } d\\tau + v(t_0) $$\n$$v(t_0) = \\frac{q_{t_0}}{C}$$\n\n#### Capacitor Power\n\nThe instantaneous power of a capacitor is characterized by:\n\n$$p=vi=Cv \\frac{dv}{dt}$$\n\n#### Capacitance\n\nCapacitance is the ability for a circuit element to store energy in an electric field.\n\n$$C=\\frac{\\epsilon A}{d}$$\n\n#### Energy in a Capacitor\n\n$$w=\\frac{1}{2} Cv^2 = \\frac{q^2}{2C}$$\n\n#### DC Conditions\n\nIn DC conditions, if its voltage is not changing with respect to time, a capacitor acts like an open circuit.\n\nThis is because in DC conditions, voltage does not change with respect to time, so $\\frac{dv}{dt} = 0$:\n\n$$i = C \\frac{dv}{dt} = C * 0 = 0$$\n\n# Inductor\n\nAn inductor is a linear two terminal passive circuit element that stores energy in its magnetic field. It consists of a coiled conducting wire.\n\n# Alternating Current (AC) Analysis\n\nAC analysis is the study of circuits with time varying sinusoidal sources.\n\n## Sinusoids\n\nA sinusoid is a type of function that oscillates.\n\n$$v(t) = V_m \\sin{(\\omega t + \\phi)}$$\n\nwhere $V_m$ is the amplitude, $\\omega$ is the angular frequency and $\\omega t$ is the argument.\n\n### Period ($T$)\n\nThe period describes how often the sinusoid repeats itself every $T$ seconds.\n\n$$T = \\frac{2 \\pi}{\\omega}$$\n\nA function is said to be periodic if it satisfies the contraint that\n\n$$v(t) = v(t+nT)$$\n\n### Frequency ($f$)\n\nThe frequecy describes how often a wave repeats itself.\n\n$$f = \\frac{1}{T}$$\n\n### Phase ($\\phi$)\n\nThe phase describes the shift in the wave along the $x$ axis.\n\nGiven two sinusoids such that $phi \\dne 0$,\n\n$$v_1(t) = V_m \\sin{\\omega t}$$\n$$v_2(t) = V_m \\sin{(\\omega t + \\phi)}$$\n\n- $v_2$ leads $v_1$\n- $v_1$ lags $v_2$\n- $v_1$ and $v_2$ are out of phase\n- If $\\phi = 0$, the sinusoids are in phase.\n\n## Phasor Analysis\n\nWe use phasors, a complex number that represents the amplitude and phase of a sinusoid, to make analysis easier. \n\n### Representations\n\nAll three forms are related by the equations:\n\n$$r = \\sqrt{x^2 + y^2}$$\n$$\\phi = \\tan^{-1} \\frac{y}{x}$$\n$$x = r\\cos{\\phi}$$\n$$y = r \\sin{\\phi}$$\n\n$$z = x + jy = r \\angle \\phi = r(\\cos{\\phi} + j \\sin{\\phi}$$\n\nThese forms are possible in part by Euler's identity.\n\n#### Euler's Identity\n\n$$e^{\\pm j\\phi} = \\cos{\\phi} \\pm j \\sin{\\phi}$$\n$$\\cos\\phi = \\mathbb{R}e \\{e^{j \\phi}\\}$$\n$$\\sin\\phi = \\mathbb{I}m \\{e^{j \\phi)\\}\n\n#### Rectangular Form\n\n$$z = x + jy$$\n\n#### Polar Form\n\n$$z = r \\angle \\phi$$\n\n#### Exponential Form\n\n$$z = re^{j\\phi}$$\n\n### Transformations\n\n$$\\frac{dv}{dt} \\quad \\iff \\quad j\\omega\\bold{V}$$\n\n$$\\int v \\text{ } dt \\quad \\iff \\quad \\frac{\\bold{V}}{j\\omega}$$\n"},503:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Electromagnetism\ndescription: Electromagnetic forces.\n---\n\n# Coulomb's Law\n\nCoulomb's law quantifies the relationship between two electrically charged particles. It is useful for finding the electric field due to **point charges**.\n\n### Magnitude of Force\n\n$$|F| = k_e \\frac{q_1 q_2}{r^2}$$\n\n- $q_1$ is the magnitude of the first charge.\n- $q_2$ is the magnitude of the second charge.\n- $k_e = 8.988 \\times 10^9 N \\cdot m^2 \\cdot C^{-2} \\text{}$\n- $r$ is the distance between the two charges.\n\n### Force Vector\n\n$$\\bold{F} = k_e \\frac{q_1 q_2}{|\\bold{r}_{21}|^2} \\hat{\\bold{r}}_{21} = k_e \\frac{q_1 q_2}{|r_{21}|^2} \\frac{\\bold{r}_{21}}{|\\bold{r}_{21}|}$$\n\n- $q_1$ is the magnitude of the first charge.\n- $q_2$ is the magnitude of the second charge.\n- $k_e = 8.988 \\times 10^9 N \\cdot m^2 \\cdot C^{-2} \\text{}$\n- $r_{21}$ is the vector from $q_2$ to $q_1$.\n\n# Biot-Savart Law\n\nThe Biot-Savart Law quantifies the generation of a magnetic field from a constant current carrying conductor.\n\n$$d\\bold{F}_m = md \\bold{B} = m\\mu_0 \\frac{I dl \\times a_R}{4\\pi r^2}$$\n\n$$\\bold{B}(\\bold{r}) = \\frac{\\mu_0}{4 \\pi} \\int_c \\frac{I d\\bold{l} \\times \\bold{a}_R}{|a_R|^2} $$\n\n# Lorentz Force\n\n$$\\bold{F} = q \\bold{v} \\times \\bold{B}$$\n\n\n# Electric Field\n\n### Electric Field Intensity\n\n$$\\bold{E} = \\frac{\\bold{F}_2}{q} = \\frac{Q}{4 \\pi \\epsilon_0 R^2} \\bold{r}$$\n\nMore generally,\n\n$$\\bold{E} = \\sum_{i=1}^N \\frac{Q_i}{4 \\pi \\epsilon_0 R_i^2} \\bold{a}_{R_i}$$\n\n# Electric Flux\n\nThe electric flux eminating from a charged sphere is proportional to the total charge of the sphere.\n\n$$\\Phi_e = Q$$\n\n### Electric Flux Density over Sphere\n\n$$\\bold{D} = \\epsilon_0 \\bold{E} = \\frac{Q}{4 \\pi r^2} \\bold{a}_r$$\n\n| Variable           | Meaning                                     |\n| ------------------ | ------------------------------------------- |\n| $\\epsilon_0$       | $8.854 \\times 10^{-12} F \\cdot m^2 \\text{}$ |\n| $\\bold{E} \\text{}$ | Electric Field Intensity                    |\n| $\\bold{a}_r$       | Direction of Electric Flux Density          |\n\n### Electric Flux over Closed Surface $s$\n\n$$\\Phi_E = \\oint_s \\epsilon_0 \\bold{E} \\cdot d\\bold{s} = \\frac{Q}{\\epsilon_0}$$\n\n# Maxwell's Equations\n\n### Gauss's Law, Electric Flux\n\nThe electric flux eminating over a closed surface $s$ is equal to the charge.\n\n#### Integral Form\n\n$$\\Phi_e = \\oint_s \\epsilon_0 \\bold{E} \\cdot d \\bold{s} = \\int_v \\rho_v dv = Q$$\n\n#### Differential Form\n\n$$\\nabla \\cdot (\\epsilon_0 \\bold{E}) = \\rho_v$$\n\n### Gauss's Law, Magnetic Flux\n\nThe magnetic flux eminating over a closed surface $s$ is equal to zero.\n\nNote:\n- Because magnetic fields are closed, the magnetic flux is equal to zero.\n- Isolated magnetic poles do not exist.\n\n#### Integral Form\n\n$$\\Phi_b = \\oint_s \\bold{B} \\cdot d \\bold{s} = 0$$\n\n#### Differential Form\n\n$$\\nabla \\cdot \\bold{B} = 0$$\n\n### Faraday's Law of Induction\n\nThe induced electromagnetic force (voltage) over a closed contour $c$ is caused by the change in the magnetic flux over a surface $s$.\n\nNote:\n- The voltage resists the change in magnetic field, hence the negative sign.\n\n#### Lenz's Law\n\nInduced emf is in such a way that opposes the change in magnetic flux.\n\n#### Integral Form\n\n$$\\text{V} = \\oint_c \\bold{E} \\cdot d\\bold{l} = -\\frac{d}{dt} \\int_s \\bold{B} \\cdot d\\bold{s}$$\n\n#### Differential Form\n\n$$\\nabla \\times \\bold{E} = - \\frac{\\partial \\bold{B}}{\\partial t}$$\n\n### Ampere's Law\n\nThe current over a closed contour $c$ is the current density over a surface $s$ and the change in magnetic flux over the surface $s$.\n\nThe term $\\displaystyle \\frac{d}{dt} \\int_s \\epsilon_0 \\bold{E} \\cdot d\\bold{s}$ is often referred to as the \"virtual current\" because it is resistance of voltage to the change in electric flux.\n\n#### Integral Form\n\n$$I_{enc} = \\oint_c \\frac{\\bold{B}}{\\mu_0} \\cdot d\\bold{l} = \\int_s \\bold{J} \\cdot d\\bold{s} + \\frac{d}{dt} \\int_s \\epsilon_0 \\bold{E} \\cdot d\\bold{s}$$\n\n#### Differential Form\n\n$$\\nabla \\times \\frac{\\bold{B}}{\\mu_0} = \\bold{J} + \\frac{\\partial (\\epsilon_0 \\bold{E})}{\\partial t}$$\n\n# Wave Propagation in Source Free Region\n\n### Source Free Region\n\nIf a wave is in a source free region, it means the wave is away from the source.\n\n### Maxwell's Equations in Source Free Region\n\nWe assume $\\rho_v = 0$ and $\\bold{J} = 0$ because we are in a [source free region](#source-free-region).\n\n$$\\bold{\\nabla} \\cdot \\epsilon_0 \\bold{E} = 0$$\n\n$$\\bold{\\nabla} \\cdot \\bold{B} = 0$$\n\n$$\\bold{\\nabla} \\times \\bold{E} = - \\frac{\\partial \\bold{B}}{\\partial t}$$\n\n$$\\bold{\\nabla} \\times \\frac{\\bold{B}}{\\mu_0} = - \\frac{\\partial \\bold{\\epsilon}_0 \\bold{b}}{\\partial t}$$\n\n$$\\bold{\\nabla}^2 \\bold{E} - \\mu_0 \\epsilon_0 \\frac{\\partial^2 \\bold{E}}{\\partial t^2} = 0$$\n\n$$\\bold{\\nabla}^2 \\bold{B} - \\mu_0 \\epsilon_0 \\frac{\\partial^2 \\bold{B}}{\\partial t^2} = 0$$\n\n### Harmonic Fields in Phasor Representation\n\nRepresenting fields in the phasor form are useful for sinusoidal steady state analysis. However, because the phasor form is dependent solely on position (not time), it is incomplete for wave propagation analysis.\n\n### Characteristics\n\n#### Wavelength\n\n$$\\lambda = \\frac{2 \\pi}{\\beta_0}$$\n\n#### Wave Velocity\n\n$$v_p = \\frac{1}{\\sqrt{\\epsilon_o \\mu_o}} = c$$\n\n#### Phase Constant\n\n$$\\beta_o = \\omega \\sqrt{\\epsilon_o \\mu_o}$$\n\n#### Intrinsic Impedence\n\n$$\\frac{\\hat{E}_x}{\\hat{H}_y} = \\eta_o = \\sqrt{\\mu_o}{\\epsilon_o}$$\n\n### General Solution to Maxwell's Equations in Phasor Form\n\n$$\\hat{E}_x = \\hat{E}_m e^{-j \\beta_o z} + \\hat{E}_m e^{j \\beta_o z}$$\n\n### General Solution to Maxwell's Equations in Real Time Form\n\n$$E_x(z, t) = \\mathcal{Re} \\{ \\hat{E}_x e^{j\\omega t} \\} = E_m \\cos{(\\omega t - \\beta_o z)} + E_m \\cos{(\\omega t + \\beta_o z)} $$\n\n### Polarization State\n\n#### Linear Polarization\n\n#### Circular Polarization\n\n#### Elliptical Polarization\n\n\n\n# Maxwell's Equations and Plane Wave Propagation in Materials\n\nMaterials consist of atoms, which contain charged particles. Because of this, additional induced sources (creating electric and magnetic fields) from the presence and interaction of these charged particles must be accounted for and added to the existing [Maxwell's equations](#maxwells-equations).\n\nThe interaction of the charged particles results in conduction, polarization and magnetization. Each generates a mix of bound charge density, polarization or conduction currents.\n\nBoundary conditions describe the transition between different materials to account for the different properties of each.\n\n### Types of Materials\n\n#### Conductors\n\nConductors are described to have a preponderance of **free** electrons in the conduction band. They are constantly in motion from thermal energy.\n\nUnder the influence of external electric field $\\bold{E}$, the electrons experience a force and the flow of electrons is the **induced conduction current**\n\n#### Dielectrics (Insulators)\n\nDielectrics are described to have a preponderance of **bound** electrons in the valence band. Electrons are not free to move; they can only be displaced from their position. They have the ability to store electric energy.\n\nUnder the influence of external electric field $\\bold{E}$, an induced source known as the **polarization charges and currents** are generated.\n\n#### Magnetic Materials\n\nMagnetic materials have the ability to store magnetic energy.\n\nThe process of aligning current loops causes an induced source called the **magnetization current**. \n\n## Induced Sources\n\n### Conduction Current (Conductors)\n\nConduction current is generated due to an application of an external electric field $\\bold{E}$ on a conducting material. It is related to the movement drift of the free charges since its movement is confined within a lattice.\n\nElectrons not in free space do not accelerate due to the electric field; rather, they are assumed to be contained in the atomic structure of the material. Therefore, they are free to move inside the material but bounce off of the walls of the atomic lattice and induce friction.\n\nElectrons instead drift with a $\\bold{v}_a$ average drift velocity and $\\tau_c$ mean free time (average time interval between collisions):\n\n$$\\bold{J} = \\frac{m \\bold{v}_a}{\\tau_c} = -e\\bold{E} \\implies \\bold{v}_a = - \\frac{e \\tau_c}{m} \\bold{E}$$\n\nAnd we get the current density $\\bold{J}$ (Ohm's law in point form) associated with the flow of the electronic changes:\n\n$$\\bold{J} = (-ne) \\bold{v}_a = \\frac{ne^2 \\tau_c}{m} \\bold{E} = \\sigma \\bold{E}$$\n\nwhere $\\sigma$ is the conductivity of the material.\n\n### Polarization (Dielectrics)\n\nThere are three different types of polarization due to the application of external electric field $\\bold{E}$ on a dielectric material. It is related to the displacement of bound charges. \n\nElectrons within the dielectric material stores electric energy because of the shifts in the positions of the positive and negatives charges against normal molecular and atomic forces.\n\nCharges in dielectrics are bound (not free to move), so they are displaced; the charge displacement is called polarization.\n\n#### Electronic Polarization\n\nElectric polarization results in the displacement of bound electrons of an atom such that the center of the cloud of electrons is separated from the center of the nucleus. The atom is polarized since an electric dipole is generated.\n\nThe electric dipole moment, with point charges $+q$ and $-q$ with the distance between the two point charges $\\bold{d}$ is:\n\n$$\\bold{p} = q\\bold{d}$$\n\n#### Orientational Polarization\n\nPolarization may already exist without an external electric field; however, atoms are randomly oriented meaning the net polarization is zero (in a macro sense).\n\nWhen an electric field is induced, a torque is induced on microscopic dipoles to orient them in the direction of the field.\n\n#### Ionic Polarization\n\nMaterials that participate in ionic bonding electrically bind their positive and negative ions (since they transfer electrons between atoms).\n\nUpon applying an electric field the ions separate and form electric dipoles.\n\n#### Polarization\n\nThe polarization, describing the electric polarization on a macroscopic level, is given by:\n\n$$\\bold{P} = \\lim_{\\Delta v \\rightarrow 0} \\frac{1}{\\Delta v} \\sum_{i=1}^{n\\Delta v} \\bold{p}_i = n \\bold{p}_a = nq\\bold{d}_a = \\rho_+ \\bold{d}_a $$\n\nsuch that $n\\Delta v$ is the number of dipoles in a volume $\\Delta v$, $\\bold{p}_a$ is the average dipole moment per molecule, $\\bold{d}_a$ is the displacement between positive and negative charges and $\\rho_+ = nq$ the density of positive charges per unit volume generated int he polarized region.\n\nThe absence of an electric field implies the polarization is zero since dipoles are randomly polarized. However, if there is an electric field, the dipole moment is non-zero.\n\n#### Polarization Current\n\nSince we are considering the electric field's effect on a dielectric material (which has bound charges), we consider a time varying electric field $\\bold{E} = E_o \\cos{\\omega t} \\bold{a}_z$.\n\n$$\\bold{P} = \\epsilon_o \\chi_e \\bold{E} = \\epsilon_o \\chi_e E_o \\cos{\\omega t} \\bold{a}_z$$ \n\nThus, the polarization current is:\n\n$$\\bold{J}_p = \\frac{\\partial \\bold{P}}{\\partial t} = \\frac{\\partial{\\epsilon_o \\chi_e \\bold{E}}}{\\partial t}$$\n\n#### Polarization Charge Density\n\nWhen an external electric field is applied to a dielectric material, dipole moments will be induced and the material will subsequently be polarized. \n\nThe polarization over the area is equal to the induced charge distribution over the volume enclosed by the area.\n\n$$\\oint_s \\bold{P} \\cdot ds = -\\int_v \\rho_p dv$$\n\nIn point form:\n\n$$\\bold{\\nabla} \\cdot \\bold{P} = -\\rho_p$$\n\n### Magnetization\n\nMagnetization is the alignment of atomic magentic dipole moments along the direction of the applied magnetic field.\n\nThe magnetic dipole moment is given by the current multiplied by the differential element of area $ds$ encircled by the current:\n\n$$\\bold{m} = I d\\bold{s}$$\n\nWithout an external magnetic field, magnetic dipole moments are randomly oriented so $\\bold{m} = 0$ in the volume.\n\nWith an external magnetic field, a torque will be applied on the dipole moments.\n\nMagnetization is the total magnetic moment per unit volume:\n\n$$M = \\lim_{\\Delta v \\rightarrow 0} \\frac{1}{\\Delta v} \\sum_{i = 0}^{n \\Delta v} \\bold{m}_i = n \\bold{m}_a = n\\bold{I} d\\bold{s}$$\n\nThe torque is on the dipole is defined by:\n\n$$d\\bold{T} = \\bold{m} \\times \\bold{B}$$\n\n#### Magnetization Current Density\n\n## Modifications to Maxwell's Equations in Materials\n\n### Ampere's Law, Polarization Current\n\nWe will add the [polarization current](#polarization-current) term:\n\n$$\\bold{\\nabla} \\times \\frac{\\bold{B}}{\\mu_0} = \\bold{J} + \\frac{\\partial (\\epsilon_0 \\bold{E})}{\\partial t} + \\frac{\\partial \\bold{P}}{\\partial t}$$\n\nAdditionally, we must replace the permeability of free space constant to take into account the material:\n\n$$\\bold{\\nabla} \\times \\frac{\\bold{B}}{\\mu_0} = \\bold{J} + \\frac{\\partial \\bold{D}}{\\partial t}$$\n\n$$\\bold{D} = \\epsilon_o \\epsilon_r \\bold{E}$$\n\nwhere $\\epsilon_r = 1 + \\chi_e$ is the suseptability of material to store electric energy due to induced polarization.\n\n### Ampere's Law, Magnetization Current\n\nWe will add the [magnetization current](#magnetization-current-density) term.\n\n$$\\nabla \\times \\frac{\\bold{B}}{\\mu_0} = \\bold{J} + \\frac{\\partial \\bold{D}}{\\partial t} + \\bold{J}_m = \\bold{J} + \\frac{\\partial \\bold{D}}{\\partial t} + \\bold{\\nabla} \\times \\bold{M}$$\n\nSimplifying:\n\n$$\\bold{\\nabla} \\times \\bold{H} = \\bold{J} + \\frac{\\partial \\bold{D}}{\\partial t}$$\n$$\\bold{H} = \\frac{\\bold{B}}{\\mu_o} - \\bold{M}$$\n\n### Gauss's Law for Electric Field\n\nWe will add the induced charge distribution:\n\n$$\\bold{\\nabla} \\cdot \\epsilon_o \\bold{E} = \\rho_v + \\rho_p \\implies \\bold{\\nabla} \\cdot \\epsilon_o \\bold{E} = \\rho_v + \\bold{\\nabla} \\cdot \\bold{P}$$\n\nSimplifying:\n\n$$\\bold{\\nabla} \\cdot \\bold{D} = \\rho_v$$\n\n## Complex Permittivity\n\nComplex permittivity characterizes the heating losses associated with the frictional losses due to the rotation of electric dipoles due to the application of a time varying electric field in a dielectric.\n\nThe capacitance contains the complex permittivity.\n\n$$c=\\frac{\\epsilon^* A}{d}$$\n\n## Boundary Conditions\n\nBoundary conditions describe the transitional properties of the electric and magnetic fields between different materials.\n\nIt will be solved for by solving for the tangential and normal components.\n\n## Poynting Vector\n\nThe Poynting vector describes the energy flux, or energy transfer per unit area, associated with the propagation of electromagnetic waves. \n\n$$\\bold{P_p} = \\bold{E} \\times \\bold{H}$$\n\nFor practicality, we will take the time average Poynting vector.\n\n### Time Average Poynting Vector Real Time Form\n\n$$\\bold{P}_av (z) = \\frac{1}{t_2 - t_1} \\int_{t_1}^{t_2} \\bold{P}(z, t) dt $$\n\n### Time Average Poynting Vector Real Time Form (Sinusoid)\n\n$$\\bold{P}_av (z) = \\frac{1}{T} \\int_{0}^{T} \\bold{P}(z, t) dt $$\n\n### Complex Poynting Vector for Time Harmonic Fields\n\n$$\\bold{P}_az (z) = \\frac{1}{2} \\mathcal{Re} \\{ \\bold{\\hat{E}} \\times \\bold{\\hat{H}}^* \\} = \\frac{1}{2} \\mathcal{Re} \\{ (E_r + jE_i) \\bold{a}_x \\times (H_r + jH_i) \\bold{a}_y \\} = \\frac{1}{2} (E_r H_r + E_i H_i) \\bold{a}_z $$\n\n"},504:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Finance \ndescription: Industry of money.\n---\n\n## Compounding\n\nCompounded interest $C$ of notional $N$ at year $T$ with rate $r$, and payout frequency $m$ is:\n\n$$C = N \\Big(1+\\frac{r}{m}\\Big)^{mT}$$\n\n### Continuously Compounded Rate\n\nA continuously compounded rate is a theoretical calculation that considers if the notional is reinvested over infinitely many periods, or when $m \\rightarrow \\infty$).\n\n$$\\Big(1+\\frac{r}{m}\\Big)^{mT} \\rightarrow e^{rT}$$\n$$C = N e^{rT}$$\n\n### Money Market Account\n\n## Zero Coupon Bond (Discount Bond)\n\nA zero coupon bond is a type of bond that is redeemed for face value (the worth of the bond) at the time maturity, $T$.\n\n### Continuously Compounded\n\n$$Z(t,T)  = \\frac{1}{(1+r_A)^{T-t}}$$\n\n### Annually Compounded\n$$Z(t,T) = e^{-r(T-t)}, t \\leq T$$\n"},505:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Fourier Analysis\ndescription: Manipulation of sinusoids for expansion of periodic functions.\n---\n\n# Brief\nWe can express periodic functions in terms of sinusoids. More specifically, it is an infinite series of sinusoids to expand functions.\n\n# Mathematical References\n\n#### Euler's Relation\n\n$$e^{jx}=cosx+jsinx$$\n\n$$cos(wt) = \\frac{e^{jw}+e^{-jw}}{2}$$\n\n$$sin(wt) = \\frac{e^{jw}-e^{-jw}}{2j}$$\n\n#### Complex Numbers\n\n$$\\mathcal{Re} \\left\\{ a_k \\right\\} = \\frac{1}{2} (a_k + a_k^*)$$\n\n$$\\mathcal{Im} \\left\\{ a_k \\right\\} = \\frac{1}{2j} (a_k - a_k^*)$$\n\n#### Sum Solutions\n\n$$\\sum_{n=0}^\\infty a^n = \\frac{1}{1-a}, |a| < 1$$\n$$\\sum_{k=0}^n a^k = \\frac{1-a^{n+1}}{1-a}, a \\neq 1$$\n\n# Fourier Series\n\n## Continuous Time Fourier Series Representation\n\n$$\\mathcal{F}(t) = x(t) = \\sum_{k = -\\infty}^\\infty a_ke^{jk \\omega_0 t}$$\n\n### Fourier Series Values\n\n#### Fundamental Period\n\n$$T= \\frac{2\\pi}{\\omega_0}$$\n\n#### Fundamental Frequency\n\n$$\\omega_0 = \\frac{2\\pi}{T}$$\n\n#### Fourier Series Coefficients\n\n$$a_k = \\frac{1}{T} \\int_{<T>} x(t)e^{-jk \\omega_0 t} dt$$\n\n$$a_0 = \\frac{1}{T} \\int_{<T>} x(t) dt$$\n\n### Linear Time Invariant Systems (LTI) & Fourier Series\n\nSay we wanted to analyze the response of an LTI system with a periodic signal. We can modify the Fourier series representation to take into the account the response of a system:\n\n$$y(t) = \\sum_{k=-\\infty}^\\infty b_k e^{jk\\omega_0 t}$$\n\nwhere $b_k=a_k H(k \\omega_0)$.\n\n### LTI & Fourier Series Values\n\n#### Frequency Response\n\n$$H(\\omega) = \\int_{-\\infty}^\\infty h(t) e^{-j\\omega t} dt$$\n\n#### Fourier Coefficient Scaling Factor\n\n$$b_k = a_k H(k \\omega_0)$$\n\n### Representing power of a signal\n\nTo get the power of a signal, we can simply use Parseval's relation for periodic signals:\n\n$$\\mathcal{P} = \\frac{1}{T} \\int_{<T>} |x(t)|^2 dt = \\sum_{k= -\\infty}^\\infty |a_k|^2$$\n"},506:function(n,e,t){"use strict";t.r(e),e.default='---\nheader: Kubernetes\ndescription: Orchestrated container management and deployment.\n---\n\n# Kubernetes (K8s)\n\nKubernetes is a comprehensive system that handles the management and deployment of containerized applications (i.e. [Docker containers]()). It can handle aspects such as clustering, scheduling, load balancing, networking, scaling and even CI/CD. \n\n## Kubernetes Jargon\n\n#### Kubernetes Clusters\n\nA Kubernetes Cluster is the collection of computers or virtual machines that Kubernetes controls such that the cluster can function in unison. \n\nIt consists of:\n\n* [A Master]()\n* [Node(s)]()\n\n#### Master\n\nThe master of a cluster is the central body that manages the cluster.\n\nIt is in charge of:\n* Scheduling\n* Maintaining application states\n* Scaling applications\n* Updating\n\n#### Nodes\n\nNodes work for the master, and they run applications. They are able to communicate with the master through the Kubernetes API.\n\nNodes consist of:\n\n* A Kubelet\n* A container runtime\n* Containerized application(s)\n\n#### Kubernetes Deployment\n\nWhen a cluster gets created, you can begin a deployment for a Pod. A Kubernetes Deployment is a specified configuration telling Kubernetes how to create and update your application instances. It consists of:\n* Replica set configurations\n* Update strategy configurations\n\nScaling is handled through creating replica sets of a pod and increase or decrease instances as needed. Replica sets can also be used for redundancy to avoid downtime.\n\nThe update strategy handles rolling updates.\n\n#### Kubernetes Pods\n\nA Kubernetes Pod is an instance of an application. It consists of at least one container and any associated containers/resources. In addition to including containers, Pods store:\n* Networking information\n* Configurations for the container\n\nOn the other hand, it can isolate container specific resources such as:\n* Processes\n* Filesystems\n* Namespaces\n\nPods are configured through a yaml configuration file.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: app-name\n  namespace: app-example\n  labels:\n    app: app\n    ui: ui\n    api: api\nspec:\n  containers:\n  - name: app-container\n    image: user/container-name:latest\n    resources:\n      limits:\n        memory: "200Mi"\n      requests:\n        memory: "100Mi"\n    command: ["stress"]\n    args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]\n```\n\n#### Kubernetes Service\n\nA Kubernetes Service is a configuration that handles the networking portion of nodes. For instance, you can assign proxies, domains, DNS services, internal IP addresses, external IP addresses or ports to a node.\n\n#### CI/CD\n\nKubernetes is also capable of CI/CD. It is intelligent enough to implement "rolling updates" such that your services do not experience downtime if an update is needed. It does this by updating one pod at a time such that one is always available.\n\n#### Namespaces\n\nA namespace contains a project and its workloads while also partitioning it from other projects.\n\n## Additional Terminology\n\n#### ConfigMaps and secrets\n\n#### Persistent Volumes\n\nNetwork storage.\n\n#### Network and Container Network Interface\n\n#### Network Policies\n\n* Firewalls\n\n#### HELM Package Manager\n\n#### RBAC\n\nAuthentication management for Kubernetes.\n\n#### Horizontal Pod Autoscaler\n\nAbility to launch more instances of an application automatically.\n\n#### Daemon Sets\n\nAbility to run  identically on each node like:\n* Logging\n* Monitoring\n\n#### Stateful Sets\n\n* Provisioning\n* Mounting\n* Has a constant identifier\n* Always launches on the same server\n\n#### Cron Jobs\n\n* Scheduled jobs to run.\n\n## Kube Control (kubectl)\n\nTo interface with a Kubernetes instance, download the Kubernetes CLI, aka kubectl.\n\n### Common Commands\n\n#### kubectl\n\n## Minikube\n\nMinikube is a local instance of Kubernetes. It is **not** recommended for production!\n\n## General process for launching your stack into Kubernetes\n\n1. Partition a namespace in Kubernetes for your project or stack. Here you can apply:\n* Quotas or limits to your hardware\n* Create network policies for internet traffic\n\n2. Create a Deployment (or a controller manager).\n\n3. Create a Service. It handles:\n* IP address/port assignments\n* DNS servers\n* Load balancing\n* Proxies/Ingress (e.g. nginx)\n'},507:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Korean \ndescription: Language for the layman.\n---\n\n# Alphabet\n\n#### Consonants\n\n| Hangul | Pronouncation | Name |\n|---------|---------------| ---- |\n|       | g, k          | giyok |\n|       | GG, KK        | ssang giok |\n|       | n             | nieun |\n|       | d, t          | digeut\n|       | DD, TT        | ssang digeut\n|       | r, l          | rieul\n|       | m             | mieum\n|       | b, p          | bieup\n|       | BB, PP        | ssang bieup\n|       | s             | siot\n|       | SS            | ssang siot\n|       | null/ng       | ieung\n|       | j             | jieut\n|       | JJ            | ssang hueyt\n|       | ch            | chieut\n|       | kh            | kieuk\n|       | th            | tieut\n|       | ph            | pieup\n|       | h             | hieut\n\n#### Vowels\n\n| Hangul | Pronounciation |\n| ------- | -------------- |\n|  | ah  |\n|  | ae  |\n|  | yah |\n|  | yae |\n|  | oh  |\n|  | eh  |\n|  | yoh |\n|  | yeh |\n|  | o   |\n|  | wah |\n|  | wae |\n|  | weh |\n|  | yoh |\n|  | u   |\n|  | wuh |\n|  | weh |\n|  | wee |\n|  | yu  |\n|  | eu  |\n|  | uey |\n|  | ee  |\n\n# Structure\n\nIn Korean, the sentence structure follows the subject-object-verb (S-O-V) paradigm.\n\n# Phrases\n\n - Hello.\n\n - Thank you.\n\n - Sorry.\n\n - Excuse me.\n\n# Marking Particles\n\nFor each marking particle, if the noun contains a , use the first case, but if it does not, use the second case. \n\n## / - Topic\n\nThe subject marking particle is used to denote a change in topic or a juxtaposition with another object.\n\n## / - Subject\n\nThe topic marking particle is used to denote that the object that is being described. This is used in conjunction with an adjective.\n\n> #### Example   .\n> \n> &#8594; The apple is red.\n\n## / - Object\n\nThe object marking particle is used to denote the object that is being acted on. This is usually used in conjunction with transitive verbs, or verbs that require an object to exist (e.g. love / ).\n\n> #### Example   .\n> \n> &#8594; I love apples.\n\n\n# Verb Conjugation\n\nDepending on the formality if the situation, you replace the infinitive verb (V) form by dropping  in V with the corresponding formality level.\n\n## Contracting Vowels\n\nIf the verb to be conjugated ends in a vowel, it can get contracted in some cases.\n\n +  = \n\n +  = \n\n +  = \n\n +  = \n\n +  = \n\n## Present Tense\n\nPresent tense is used to show an action that is currently or permanently occurring. For example, eat () or go ().\n\n| Formality | Conjugation |\n| --------- | ----------- |\n| Casual | V + / |\n| Polite Low | V + /\n| Polite High | V + /\n\n### Casual / Polite Low Rules\n\n####  & Verb Stem's Last Vowel is /\n\nIf the verb stem's last vowel ends with /, conjugate with .\n\n|/ | &#8594; | V[] |\n| - | - | - |\n\n####  & Verb Stem's Last Vowel is Not /\nFor all other cases, use .\n\n| ///////// | &#8594; | V[] |\n| - | - | - |\n\n####  Verbs\n\nSince  is the most common, it is shortened to make it easier to say. Therefore it has a different form.\n\n| Formality | Conjugation |\n| --------- | ----------- |\n| Casual |  |\n| Polite Low | \n| Polite High | \n\n### Polite High Rules\n\n#### \n\nIf the verb stem ends with a consonant, use V.\n\n| V | &#8594; | V |\n| - | - | - |\n\n#### No \n\nIf the verb stem ends with a verb, attach  to the verb stem ending block: V.\n\n| V | &#8594; | V |\n| - | - | - |\n\n## Present Progressive Tense\n\nPresent progressive strictly denotes a currently occurring action, like eating or going. In English, -ing is added to the verb.\n\n| Formality | Conjugation |\n| --------- | ----------- |\n| Casual | V +   |\n| Polite Low | V +  \n| Polite High | V +  \n\n## Past Tense\n\nPast tense expresses an action that has occurred.\n\n| Formality | Conjugation |\n| --------- | ----------- |\n| Casual | V + / |\n| Polite Low | V + /\n| Polite High | V + /\n\n### Casual / Polite Low Rules\n\n####  & Verb Stem's Last Vowel is /\n\nIf the verb stem's last vowel ends with /, conjugate with .\n\n|/ | &#8594; | V[] |\n| - | - | - |\n\n####  & Verb Stem's Last Vowel is Not /\nFor all other cases, use .\n\n| ///////// | &#8594; | V[] |\n| - | - | - |\n\n####  Verbs\n\nSince  is the most common, it is shortened to make it easier to say. Therefore it has a different form.\n\n| Formality | Conjugation |\n| --------- | ----------- |\n| Casual |  |\n| Polite Low | \n| Polite High | \n\n### Polite High Rules\n\n#### \n\nIf the verb stem ends with a consonant, use V.\n\n| V | &#8594; | V |\n| - | - | - |\n\n#### No \n\nIf the verb stem ends with a verb, attach  to the verb stem ending block: V.\n\n| V | &#8594; | V |\n| - | - | - |\n\n## Future Tense\n\nFuture tense expresses an action that will occur.\n\n| Formality | Conjugation |\n| --------- | ----------- |\n| Casual | V + /  |\n| Polite Low | V + / \n| Polite High | V + / \n\n#### \n\nIf the verb stem ends with a consonant, use V + formality.\n\n| V | &#8594; | V + formality |\n| - | - | - |\n\n#### No \n\nIf the verb stem ends with a verb, attach  to the verb stem ending block: V + formality.\n\n| V | &#8594; | V + formality |\n| - | - | - |\n\n# Adjective Conjugation\n\nSimilar to conjugating verbs, simply take the adjective (A) stem, drop the . Then add / based on the case:\n\n|  | Conjugation |\n| --- | ----------- |\n| Yes | A |\n| No | A\n\n# Common Grammatical Forms\n\n## Can/Cannot - V /\n## Want - V \n## Need/Need not -  /\n## Should - V()\n## Respect - V()"},508:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: LEGv8 \ndescription: Not as complicated as your ARM.\n---\n\n# Hardware Terminology\n\n### Doubleword\n\nGroups of 64 bits.\n\n### Word\n\nGroups of 32 bits.\n\n### Registers\n\nRegisters are used for frequently accessed data.\n\nIn LEGv8, there are **32 registers** that are 64 bits wide. They are referred to by their number ranging from `X0-X30`. The final register is `XZR` which is the 'zero register' that always contains 0.\n\nRegisters|Purpose|Notes\n:-----:|:-----:|:-----:\n`X0`|Return value|The return value of a function is stored here.\n`X1-X7`|Passed parameters|Function argument variable storage.\n`X8`|Indirect result location register| \n`X9-X15`|Temporary variables| \n`X16-X17`|Scratch/temporary register|\n`X18`|Platform/temporary register| \n`X19-X27`|Saved| \n`X28`|Stack Pointer (SP)|Points to the memory address of the top of the stack.\n`X29`|Frame Pointer (FP)|Points to the memory address at the bottom of the stack.\n`X30`|Link Register (LR)|Points to the return address after branching.\n`XZR`|Zero Register (XZR)|Contains a constant value of 0.\n\n### Memory\n\nThere are $2^{62}$ memory words. Memory is set up like a single dimensional array.\n\nLEGv8 uses **byte addressing**, so each memory address index (with length of one byte) can be accessed. Therefore, since LEGv8 is a 64 bit architecture, memory addresses are split up into groups of 8 bytes or 64 bits (a [doubleword](#doubleword)).\n\n```\nMemory[0]\nMemory[8]\nMemory[16]\n...\n```\n\n### Stack\n\nA stack is a portion of memory allocated towards a program.\n\n# Computer Instruction Representation\n\n### Instruction\n\nA series of 32 binary bits that represent an instruction.\n\n## Fields\n\nA segment of an instruction. Essentially, the instruction is broken up into a certain number of bits to represent a meaningful operand.\n\n### opcode\n\nAn opcode is the type of operation that is being executed. For example `ADD`, `ADDI` or `LDUR`.\n\n### Rm\n\nRm is the second register source operand\n\n### shamt\n\nshamt is the shift amount.\n\n### Rn\n\nRn is the first register source operand.\n\n### Rd\n\nRd is the destination register operand. It receives the result of the operation.\n\n### address\n\nMemory address offset\n\n### op2\n\nOpcode option.\n\n### Rn\n\nRn is the CPU register.\n\n### Rt\n\nRt is the memory address.\n\n### immediate\n\nImmediate is the constant.\n\n### pc_offset\n\nPC_Offset is the the program counter offset. This specifies how many bytes in the instruction memory to move from the current address the program counter is pointing to.\n\n### R-Type Instruction (Register)\n\nR-Type instructions manipulate registers.\n\n**opcode**|**Rm**|**shamt**|**Rn**|**Rd**\n:-----:|:-----:|:-----:|:-----:|:-----:\n11 bits|5 bits|6 bits|5 bits|5 bits\n\nIn a normal machine instruction, this would look like: `opcode Rd, shamt, Rn, Rm`.\n\n#### R-Type Instruction Example\n\nA conversion from an instruction to a decimal representation of the instruction:\n\n`ADD X1, X2, X3` $\\implies$ `1112 3 0 2 1`\n\n### D-Type Instruction (Data Transfer)\n\nR-Type instructions manipulates data between the registers and memory.\n\n**opcode**|**address**|**op2**|**Rn**|**Rt**\n:-----:|:-----:|:-----:|:-----:|:-----:\n11 bits|9 bits|2 bits|5 bits|5 bits\n\nIn a normal machine instruction, this would look like: `opcode Rt, [Rn, #address]`.\n\n#### D-Type Instruction Example\n\nA conversion from an instruction to a decimal representation of the instruction:\n\n`LDUR X1, [X2, #3]` $\\implies$ `1986 3 0 2 1`\n\n### I-Type Instruction (Immediate)\n\nR-Type instructions manipulate constants.\n\n**opcode**|**immediate**|**Rn**|**Rd**\n:-----:|:-----:|:-----:|:-----:|:-----:\n10 bits|12 bits|5 bits|5 bits\n\nIn a normal machine instruction, this would look like: `opcode Rd, Rn, #immediate`.\n\n#### I-Type Instruction Example\n\nA conversion from an instruction to a decimal representation of the instruction:\n\n`ADDI X1, X2, #3` $\\implies$ `836 3 2 1`\n\n### B-Type Instruction (Branch)\n\nB-Type instructions handle branching (jumping).\n\n**opcode**|**pc_offset**\n:-----:|:-----:\n6 bits|26 bits|\n\nIn a normal machine instruction, this would look like: `opcode pc_offset`.\n\n#### B-Type Instruction Example\n\nA conversion from an instruction to a decimal representation of the instruction:\n\n`B Skip` $\\implies$ `5 25`\n\nwhere `Skip` is the number of bytes to skip. In this case, it will skip $25$ bytes $\\cdot \\text{ } 4 = 100$ bits.\n\n#### B-Type Instructions\n\nB-Type Instruction|Meaning|Note\n:-----:|:-----:|:-----:\n`B Label`|Branch|Jumps to `Label`.\n`BR LR`|Branch to register|Jumps to register `LR`'s address\n`BL Label`|Branch with link|Jumps to `Label` and stores the address of the next instruction.\n`B.cond Label`|Branch conditionally|Jump to label based on condition code values\n\n### CB-Type Instruction (Condiitonal Branch)\n\nCB-Type instructions handle conditional branching.\n\n**opcode**|**pc_offset**|**Rt**\n:-----:|:-----:|:-----:\n8 bits|19 bits|5 bits\n\nIn a normal machine instruction, this would look like: `opcode Rt, pc_offset`.\n\n#### CB-Type Instruction Example\n\nA conversion from an instruction to a decimal representation of the instruction:\n\n`CBZ X1, Skip` $\\implies$ `181 25 1`\n\nwhere `Skip` is the number of bytes to skip. In this case, it will skip $25 bytes \\cdot 4 = 100 bits$.\n\n### IW-Type Instruction (Immediate Wide)\n\nIW-Type instructions handle large constants.\n\n**opcode**|**block_number**|**constant**|**Rd**\n:-----:|:-----:|:-----:|:-----:\n9 bits|2 bits|16 bits|5 bits\n\nIn a normal machine instruction, this would look like: `opcode Rd, constant, LSL block_number`. \n\n`block_number` is limited to 0, 16, 32, 48.\n\n#### IW-Type Instruction Example\n\nA conversion from an instruction to a decimal representation of the instruction:\n\n`MOVZ X1, 256, LSL 0` $\\implies$ `421 1 256, 0`\n\n**B-Type Instruction**|**Meaning**|**Note**\n:-----:|:-----:|:-----:\n`MOVZ register, constant, LSL shift_amount`|Move wide with zeroes|Set the specified quadrant of the register to the constant specified and set the other quadrants to zero.\n`MOVK register, constant, LSL shift_amount`|Move wide with keep|Set the specified quadrant of the register to the constant specified and keep the other quadrants the same.\n\n## Supporting Computer Hardware Procedures\n\n### Program Counter\n\nThe program counter holds the address of the current instruction being executed. \n\n### Stack\n\nA LIFO queue. A stack is a memory block allocated to a program.\n\n### Stack Pointer (SP)\n\nThe stack pointer points to the top of the stack (where the current operation is being done).\n\n## Translating and Starting a Program\n\nPipeline of a program:\n\n$\\fbox{\\text{Program}} \\rightarrow \\text{Compiler} \\rightarrow \\fbox{\\text{Assembly language program}} \\rightarrow \\text{Assembler} \\rightarrow \\fbox{Object File} \\rightarrow \\text{Linker} \\rightarrow \\fbox{Executable} \\rightarrow \\text{Loader} \\rightarrow \\fbox{\\text{Memory}}$\n\n### Compiler\n\nA **compiler** converts a textual representation of code (through keywords and symbols) into assembly language.\n\nIt uses pre-defined templates. \n\n### Assember\n\nAn **assembler** converts assembly language code into an object file (binary) or a machine program.\n\nIt allocates space in memory for the instructions and data.\n\nAlso, it builds a symbol table - that is, a table containing pairs of symbols its corresponding address.\n\n#### Object File\n\nAn **object file** contains the machine language instructions, data and info needed to place instructions into memory.\n\n**Object File Part**|**Description**\n:-----:|:-----:\nObject File Header|The size and position of the parts of the object file.\nText Segment|The machine language code.\nStatic Data Segment|The data allocated for the life of the program\nRelocation Information|Instructions and data word addresses in memory\nSymbol Table|Remaining labels not defined like external references.\n\n### Linker\n\nTurns assembled [object files](#object-file) into an [executable](#executable). It also allows for re-linking through a link editor. To reduce the number of re-linking steps, the link editor looks for diffs in addresses and replaces them accordingly.\n\n1. Place code and data modules symbolically in memory.\n2. Determine addresses of data and instruction labels.\n3. Patch internal and external references\n\n#### Executable\n\nAn **executable** is a simply an object file that contains no unresolved references.\n\n### Loader\n\nA loader takes the [executable](#executable) and reads it into memory.\n\n#### Statically Linked Libraries\n\nA library that is linked to a program during link time.\n\n#### Dynamically Linked Libraries\n\nA library routine that is linked to a program during execution time.\n\n# Building a Simple LEGv8 Computer\n\n## Arithmetic Logic Unit (ALU)\n\nThe arithmetic logic unit handles various computational operations such as ANDing, ORing, adding, subtracting, NORing and passing inputs.\n\n| ALU Control Lines | Function |\n| -----------------   |\n| 0000 | AND          |\n| 0001 | OR           |\n| 0010 | add          |\n| 0110 | subtract     |\n| 0111 | pass input b |\n| 1100 | NOR          |\n\n# Pipelining\n\nPipelining is a technique where multiple instructions are overlapped in execution. It is useful for shaving down time for a large amount of instructions since it increases instruction throughput (not decreasing execution time).\n\n## Datapath for LEGv8\n\nInstructions can be split up into five different stages in order to implement the pipeline. Since there are five stages, five instructions can be run each clock cycle.\n\n### Instruction Fetch\n\nGet the instruction from the program counter.\n\n### Instruction Decode / Register File Read\n\nRetrieve segments of instruction to see what the instruction type it is. Also read register values from the register file if needed.\n\n### Execution / Address Calculation\n\nRun any calculations.\n\n### Data Memory Access\n\nAccess memory or write to memory.\n\n### Write Back\n\nWrite data back into register file.\n\n## Data Hazards\n\nWhen an instruction is dependent on an earlier instruction still being processed in the pipeline. \n\n### Forwarding\n\nA solution to data hazards is to add hardware to retrieve the missing item early from internal resources.\n\n### Stalling\n\nAnother solution to solving the data hazard issue is to stall the pipeline, bubble or insert a `NOP` (no operation) instruction.\n\n### Predicting\n\nIt can also try predicting\n"},509:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Linear Algebra\ndescription: The study of linear equations.\n---\n\n# Scalars\n\nA scalar is a single number.\n\n# Vectors\n\nA vector is an array numbers.\n\n## Notation\n\n$$ \\bold{A} =\n\\left[\n\\begin{array}{c}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{array}\n\\right]$$\n\nwhere $n$ is the number of rows in the vector and $[\\bold x]_i$ is the $i$-th entry.\n\n## Dot Product\n\nThe dot product is the sum of products of each component between two vectors. It tells us the magnitude of how much in a direction one vector is to another.\n\n$$\\bold x \\cdot \\bold y = \\sum_{i = 1}^n x_i b_i$$\n\n### Orthogonality\n\nIf the dot product results in the zero vector $\\bold{0}$, the two vectors are said to be perpendicular.\n\n$$\\bold x \\cdot \\bold y = \\sum_{i = 1}^n x_i b_i = \\bold 0 \\implies \\text{x,y are perpendicular}$$\n\n### Length\n\nThe magnitude or length of a vector is the root of the dot product of itself.\n\n$$||\\bold x|| = \\sqrt{\\bold x \\cdot \\bold x} = (x_1^2 + x_2^2 + \\dots + x_n^2)^{1/2}$$\n\nGeometrically, the components of the vector form a right triangle, and using the pythagorean theorem, we find the hypotenuse of that triangle.\n\n### Unit Length\n\nA unit vector has unit length, or $||\\bold u|| = 1$.\n\n### Unit Vector\n\nThe unit vector $\\bold u$ is a vector with [unit length](#unit-length).\n\n$$\\bold u = \\frac{\\bold x}{||\\bold x||}$$\n\nIt also retains the same direction as $x$.\n\n### Angle\n\nThe angle $\\theta$ between two vectors is defined as:\n\n$$\\cos \\theta = \\frac{x \\cdot y}{||\\bold x|| \\text{ } ||\\bold y||}$$\n\n## Linear Combinations\n\nA linear combination is the sum of scalar multiples of vectors. Let $\\bold{v}_1 \\dots \\bold{v}_n$ be vectors and $a_1 \\dots a_n$ be scalars:\n\n$$a_1 \\bold{v}_1 + a_2 \\bold{v}_2 + \\dots + a_n \\bold{v}_n$$\n\nWe can represent a certain vector in terms of a linear combination of vectors by scaling the weights of each vector.\n\nIf we set the linear combination equal to a vector $\\bold{b}$:\n\n$$a_1 \\bold{v}_1 + a_2 \\bold{v}_2 + \\dots + a_n \\bold{v}_n = \\bold b$$\n \nwe are trying to find a combination $a_1, a_2, \\dots, a_n$ such that it results in the vector $\\bold b$.\n\n### Spanning Sets\n\nThe set of all possible linear combinations of $n$ vectors is the **spanning set** of the given $n$ vectors.\n\nIn other words, it is the set of all possible vectors you get by scaling each vector in some way and summing them.\n\n$$S = \\bigg\\{ \\sum_{i=1}^n a_i\\bold{v}_i | a_i \\in \\mathbb{R}, 1 \\leq i \\leq p \\bigg\\}$$\n\n### Linear Dependence\n\nGiven a set of vectors $S = \\{v_1, v_2, v_3, \\ldots, v_n\\}$, we say the set of vectors is **linearly dependent** if there are $a_1, a_2, a_3, \\ldots, a_n$, $a_i$ not all equal to zero such that\n\n$$a_1 \\bold{x}_1 + a_2 \\bold{x}_2 + a_3 \\bold{x}_3 + ... + a_n \\bold{x}_n = 0$$\n\nIn other words, there exists a solution that is non-trivial. The vectors span less than $\\mathbb{R}^n$.\n\n### Linear Independence\n\nOn the other hand, the set of vectors $S = \\{v_1, v_2, v_3, \\ldots, v_n\\}$ is **linearly independent** if all $a_i = 0$, the trivial solution, is the only solution.\n\n$$a_1 \\bold{x}_1 + a_2 \\bold{x}_2 + a_3 \\bold{x}_3 + ... + a_n \\bold{x}_n = 0$$\n\nIn other words, the only solution that exists is the trivial solution. The vectors span all of $\\mathbb{R}^n$.\n\n### Basis\n\nThe basis of a vector space $V$ with a set of vectors $S = \\{ \\bold{v_1}, \\dots, \\bold{v_n} \\}$ that are:\n- [Linearly independent](#linear-independence)\n- In the spanning set of $V$.\n\n### Column Space\n\nIf we have an $m \\times n$ matrix $A$ with column vectors $\\bold{A}_1$, $\\bold{A}_2$, $\\dots$, $\\bold{A}_n$, then the column space of a matrix $A$ is the set containing the linear combinations of the column vectors of the matrix.\n\nIt is denoted as:\n\n$$\\mathcal{CS}(\\bold A) = \\{ \\bold{A}_1, \\bold{A}_2, \\dots \\bold{A}_n \\}$$\n\n## Linear Transformation\n\nA **linear transformation** is the act of inputting one vector and manipulating it such that you output a new vector.\n\nFor instance if you rotate a vector a certain number of degrees.\n\n## Vector Space\n\nA vector space defines the properties of operations of vectors.\n\n# Matrices\n\nIntuitively, a matrix has the ability to transform a vector. Take, for example, $A \\bold{x = b}$. We are calculating how to achieve the vector $\\bold b$ by scaling $\\bold x$ by $A$.\n\nA matrix is a rectangular array of numbers of the form:\n\n### Notation\n\n$$A_{mn} =\n\\left[\n\\begin{array}{ccccc}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \na_{m1} & a_{m2} & \\dots & a_{mn} \\\\\n\\end{array}\n\\right]$$\n\nwhere $A$ is an $m \\times n$ matrix and $m$ are the rows and $n$ are the columns.\n\nMore generally,\n\n$${A} =\n\\left[\n\\begin{array}{c}\na_{ij} \n\\end{array}\n\\right]$$\n\n$A$ is an $i \\times j$ matrix where $i$ is the number of rows and $j$ is the number of columns.\n\n### Rows\n\n$\\bold{A}_{i,:}$ denotes the $i$-th row of matrix $A$.\n\n### Columns\n\n$\\bold{A}_{:,j}$ denotes the $j$-th column of matrix $A$.\n\n### Entries\n\n$\\bold{A}_{i,j}$ denotes the $(i,j)$-th element of matrix $A$.\n\n### Properties\n\nLet\n\n$$A =\n\\left[\n\\begin{array}{c}\na_{ij} \n\\end{array}\n\\right]$$\n\n$$B =\n\\left[\n\\begin{array}{c}\nb_{ij} \n\\end{array}\n\\right]$$\n\n$$C =\n\\left[\n\\begin{array}{c}\nc_{ij} \n\\end{array}\n\\right]$$\n\nand $c,d$ be scalars.\n\n#### Addition\n\n$$A + B =\n\\left[\n\\begin{array}{c}\na_{ij} + b_{ij}\n\\end{array}\n\\right]$$\n\n#### Scalar Product\n\n$$\nc\\bold{A} =\nc \\left[\n\\begin{array}{c}\na_{ij}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}{c}\nc a_{ij}\n\\end{array}\n\\right]\n$$\n\n#### Commutativity\n\n$$\\bold{A} + \\bold{B} = \\bold{B} + \\bold{A} \\text{}$$\n\n#### Associativity\n\n$$\\bold{A} + (\\bold{B} + \\bold{C}) = (\\bold{A} + \\bold{B}) + \\bold{C} \\text{}$$\n\n$$\\bold{(AB)C = A(BC)}$$\n\n#### Scalar Distributivity\n\n$$c(d\\bold{A}) = (cd)\\bold{A}$$\n\n$$c(\\bold{A} + \\bold{B}) = c\\bold{A} + d \\bold{B}$$\n\n$$(c + d) \\bold{A} = c \\bold{A} + d \\bold{A}$$\n\n#### Negativity\n\n$$-\\bold{A} =\n\\left[\n\\begin{array}{c}\n- a_{ij}\n\\end{array}\n\\right]\n$$\n\n#### Exponents\n\n$$\\bold A^n = \\bold{A_1 A_2} \\dots \\bold A_n$$\n$$\\bold{A^n A^m = A^{n+m}}$$\n$$\\bold{(A^n)^m=\\bold A^{nm}}$$\n\n## Identity Matrix\n\nThe identity matrix is a square $n \\times n$ matrix that contains 1's on the diagonal and 0's elsewhere.\n\n$$ \\bold{I}_n =\n\\left[\n\\begin{array}{ccccc}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n0 & 0 & \\dots & 1 \\\\\n\\end{array}\n\\right]$$\n\nIt has the special property that:\n\n$$\\bold I \\bold x = \\bold x$$\n\n## Rectangular Matrix\n\nGiven an $m \\times n$ matrix $A$, $A$ is a rectangular matrix if $m \\neq n$.\n\n## Square Matrix\n\nGiven an $m \\times n$ matrix $A$, $A$ is a square matrix if $m=n$.\n\n## Diagonal Matrix\n\nA diagonal matrix is one where there are non-zero elements in indices such that $i=j$ (the column index equals the row index), and all other elements are zero.\n\n$$\nD =\n\\begin{bmatrix}\n  d_{1} & & \\\\\n  & \\ddots & \\\\\n  & & d_{n}\n\\end{bmatrix}\n$$\n\n## Nonsingular Matrix\n\nGiven a square matrix $A$, suppose that the columns are linearly independent, or the solution set is only the trivial solution and there exists only one solution.\n\n$A$ is a nonsingular matrix.\n\nOtherwise, $A$ is a singular matrix, or in other words, there are infinite or no solutions and the columns are linearly dependent.\n\n## Inverting a Matrix\n\nGiven an $n \\times n$ matrix $A$, $B$ is an inverse of $\\bold A$ where:\n\n$$\\bold{AB} = \\bold{BA} = \\bold I \\implies \\bold{A A^{-1} = A^{-1} A = I}$$\n\n$\\bold A^{-1}$ is denoted as the inverse of $\\bold A$.\n\nIntuitively, we are undoing the transformation of a matrix. Take $A \\bold{x = b} \\implies \\bold x = A^{-1} \\bold b$.\n\nWe are trying to retrieve the original vector $\\bold x$ transformed by $A$ by undoing that transformation and taking the inverse $A^{-1}$.\n\n### Conditions for Invertibility\n\nGiven a square matrix $\\bold A$:\n\n- $\\bold A$ must have $n$ non-zero pivots (linearly independent columns).\n- $\\det \\bold A \\neq 0$\n- $\\bold A\\bold x = \\bold 0$ where $x = 0$, the trivial solution, is the only solution.\n- Gauss-Jordan elimination eliminates $[ \\bold A \\bold I ]$ to $[ \\bold I \\bold A^{-1} ]$\n- $\\bold A$ is invertible if $|a_{ii}| > \\sum_{j \\neq i} |a_{ij}|$ (it is diagonally dominant)\n\n### Properties\n\n- $(A^{-1})^{-1} = A$\n- $(kA)^{-1} = k^{-1} A^{-1} \\text{}$\n- $\\det(A^{-1}) = (\\det A)^{-1} \\text{}$\n\n### Calculating inverse of diagonal matrix\n\nGiven an invertible, square, and diagonal matrix with non-zero diagonal entries:\n\n$$D =\n\\begin{bmatrix}\n  d_{1} & & \\\\\n  & \\ddots & \\\\\n  & & d_{n}\n\\end{bmatrix} \\implies D^{-1}=\n\\begin{bmatrix}\n  \\frac{1}{d_{1}} & & \\\\\n  & \\ddots & \\\\\n  & & \\frac{1}{d_{n}}\n\\end{bmatrix}$$\n\n### Calculating inverse of matrix product\n\nIf $A$ and $B$ are invertible, then $AB$ is as well.\n\n$$(AB)^{-1} = B^{-1} A^{-1}$$\n\n## Elementary Matrix\n\nAn elementary matrix is one that applies a row operation.\n\n### Row switch\n\n$R_i \\leftrightarrow R_j$\n\n$$E_s = \\begin{bmatrix}\n  1 &        &   &        &   &        &   \\\\\n    & \\ddots &   &        &   &        &   \\\\\n    &        & 0 &        & 1 &        &   \\\\\n    &        &   & \\ddots &   &        &   \\\\\n    &        & 1 &        & 0 &        &   \\\\\n    &        &   &        &   & \\ddots &   \\\\\n    &        &   &        &   &        & 1\n\\end{bmatrix}$$\n\n#### Inverse\n\n$$E_s^{-1} = E_s$$\n\n### Row multiplication: \n\n$k R_i \\rightarrow R_i \\quad k \\neq 0$\n\n$$E_m(k) = \\begin{bmatrix}\n  1 &        &   &   &   &        &   \\\\\n    & \\ddots &   &   &   &        &   \\\\\n    &        & 1 &   &   &        &   \\\\\n    &        &   & k &   &        &   \\\\\n    &        &   &   & 1 &        &   \\\\\n    &        &   &   &   & \\ddots &   \\\\\n    &        &   &   &   &        & 1\n\\end{bmatrix}$$\n\n#### Inverse\n\n$$E_m^{-1}\\bigg(\\frac{1}{k}\\bigg) = E_m(k)$$\n\n### Row addition: \n\n$R_i + k R_j \\quad i \\neq j$\n\n$$E_a(k) = \\begin{bmatrix}\n  1 &        &   &        &   &        &   \\\\\n    & \\ddots &   &        &   &        &   \\\\\n    &        & 1 &        &   &        &   \\\\\n    &        &   & \\ddots &   &        &   \\\\\n    &        & k &        & 1 &        &   \\\\\n    &        &   &        &   & \\ddots &   \\\\\n    &        &   &        &   &        & 1\n\\end{bmatrix}$$\n\n#### Inverse\n\n$$E_a^{-1}(-k) = E_a(k)$$\n\n## Multiplication\n\n$$\\bold A_{mn} \\bold B_{np} = \\bold C_{mp}$$\n\n### Inner (Dot) Product\n\nEach entry $ij$ is calculated by calculating the dot product of the $i$th row and the $j$th column.\n\nwhere each entry $[\\bold C]_{ij} = \\displaystyle \\sum_{i=1}^n a_{ik} b_{kj} \\text{}$\n\n$$\\bold a^T \\bold b = \\left[\n\\begin{array}{c}\na_1 & a_2 & \\dots & a_n \n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\na_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \n\\end{array}\n\\right]$$\n\n### Outer Product\n\nThe outer product is a much easier method to multiply matrices. Simply multiply the columns by the rows and add them together.\n\n$$\\bold {ab}^T = \\left[\n\\begin{array}{c}\na_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\na_1 & a_2 & \\dots & a_n \n\\end{array}\n\\right]\n$$\n\n# Linear Equations\n\nA **linear equation** can be expressed as\n\n$$y=a_1x_1 + a_2x_2 + a_3x_3 + ... + a_nx_n = b$$\n\nwhere $a_1, a_2, a_3, ... , a_n, b$ are scalar coefficients and $x_1, x_2, x_3, ..., x_n$ are variables.\n\n## System of Linear Equations\n\nA system of linear equations takes the form:\n\n$$a_{11}x_1+a_{12}x_2+...+a_{1n}x_n=b_1\\\\\na_{21}x_1+a_{22}x_2+...+a_{2n}x_n=b_2\\\\\na_{31}x_1+a_{32}x_2+...+a_{3n}x_n=b_3\\\\\n\\newline\n\\newline \\vdots\n\\newline\na_{m1}x_1+a_{m2}x_2+...+a_{mn}x_n=b_m$$\n\nThere are $m$ equations, and $a_1, a_2, a_3, ... , a_n, b$ are scalar coefficients and $x_1, x_2, x_3, ..., x_n$ are variables.\n\nWe can succinctly compact this into:\n\n$$\\bold{Ax} = \\bold b$$\n\n### Coefficient Matrix\n\nWe take the coefficients of each variable and put it into a matrix:\n\n$$ \\bold{A} =\n\\left[\n\\begin{array}{ccccc}\na_{11} & a_{12} & a_{13} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & a_{23} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \na_{m1} & a_{m2} & \\dots & a_{m3} & a_{mn} \\\\\n\\end{array}\n\\right]$$\n\n### Vector of Constants\n\nThe target of what we want to get:\n\n$$ \\bold{b} =\n\\left[\n\\begin{array}{c}\nb_1 \\\\\nb_2 \\\\\nb_3 \\\\\n\\vdots \\\\\nb_n\n\\end{array}\n\\right]$$\n\n### Solution Vector\n\nThe solution to the linear equations:\n\n$$ \\bold{x} =\n\\left[\n\\begin{array}{c}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\vdots \\\\\nx_n\n\\end{array}\n\\right]$$\n\n### Homogenous System of Linear Equations\n\nA homogenous system of linear equations takes the form\n\n$$a_{11}x_1+a_{12}x_2+...+a_{1n}x_n=0\\\\\na_{21}x_1+a_{22}x_2+...+a_{2n}x_n=0\\\\\na_{31}x_1+a_{32}x_2+...+a_{3n}x_n=0\\\\\n\\newline\n\\newline \\vdots\n\\newline\na_{m1}x_1+a_{m2}x_2+...+a_{mn}x_n=0$$\n\nThe right side of each linear equation is zero. In other words, the [vector of constants](#vector-of-constants) is the zero vector.\n\nThis is system of linear equations is homogenous because the solution to the system has at least one trivial solution, $x_1=x_2= ... =x_n=0$.\n\n#### Theorem of Homogenous Systems\n\nA homogenous system with $n$ variables and $m$ equations has $\\infty$ solutions if $n>m$.\n\n## Types of Solution Sets of a System of Linear Equations\n\nA **solution set** is the set that contains every solution to a linear system.\n\nTwo systems are equivalent if their solution sets are equivalent.\n\nThere are three types of solution sets:\n\n### Single, Unique Solution\n\nThe system intersects at exactly one point.\n\n### Infinite Solutions\n\nThe system intersects at every point. One or more variables are free.\n\n### No Solutions\n\nThe system does not intersect at any point.\n\n## Consistent Solutions\n\nA system of linear equations is **consistent** if it has at least one solution. Otherwise, it is inconsistent.\n\n### Independent and Dependent Variables\n\nIn a consistent system of linear equations, a variable that is not dependent on another variable is called **independent** or free.\n\nOtherwise, it is called a **dependent** variable.\n\n## Gaussian Elimination\n\nThe **elimination** algorithm is a systematic method of solving systems of linear equations. As the name states, we are eliminating variables from equations to make it easier to solve.\n\n- Apply row operations expressed as elementary matrices $E_{ij}$ to transform the system into an upper triangular system (all values below the diagonal are zeros).\n- Apply back substitution from the bottom up to solve the system.\n\n#### Row Operations\n\nFor the row operations, there are two important terms, **pivots** and **multipliers**. \n\n- A pivot is the first non-zero entry in a row that does elimination.\n- A multiplier is defined as the entry in row $i$ divided by the pivot value in row $j$.\n\nIf successful, after we apply elimination, the pivots should be on the diagonal of the matrix and have non-zero values. It succeeds if there is a [single solution](#single-unique-solution). This would occur if the matrix is [nonsingular](#nonsingular-matrix).\n\nOtherwise, elimination fail if any of the pivots are zero. This situation occurs if there are [infinite](#infinite-solutions) or [no](#no-solutions) solutions. This occurs if the matrix is [singular](#nonsingular-matrix).\n\n## Augmented Matrix\n\nGiven a system of linear equations\n\n$$a_{11}x_1+a_{12}x_2+...+a_{1n}x_n=b_1\n\\newline\n\\newline \\vdots\n\\newline\na_{m1}x_1+a_{m2}x_2+...+a_{mn}x_n=b_n$$\n\nWe can extract the coefficients transform the system into an augmented matrix:\n\n$$\\left[\n\\begin{array}{cccc:c}\na_{11} & a_{12} & \\dots & a_{1n} & b_1 \\\\ \n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \na_{m1} & a_{m2} & \\dots & a_{mn} & b_n \\\\\n\\end{array}\n\\right] = [ \\bold{Ab}]$$\n\nWe are appending the vector of constants onto the coefficient matrix.\n\n## Reduced Row Echelon Form\n\n1. Any rows of zeroes are at the bottom.\n2. The first non-zero entry of a non-zero row is one (1).\n3. Starting from the top left, the leading one (1) appears to the right of the leading one (1) of the preceding row.\n4. All other entries of a column containing a leading one (1) is zero (0).\n\n### Gauss-Jordan Elimination\n\nGauss-Jordan Elimination can be used to solve systems of linear equations and find inverses of invertible matrices. The goal is to get the matrix in reduced row echelon form.\n\n### Pivot Column\n\nA pivot column in context of the reduced row echelon form of a matrix is a column containing a leading $1$ (in the leftmost non-zero entry).\n\n### Zero Row\n\nA zero row is a row containing all zeros. They are at the bottom.\n"},510:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Markdown\ndescription: Convenient markup langauge.\n---\n\n\n"},511:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Mechanics\ndescription: Newton.\n---\n\n# Kinematics\n\nKinematics describe how a particles moves in Euclidian space with respect to time ($t$) and spatial position ($xyz$).\n\n## Position\n\nThe position of a particle describes where it is located within a space at a certain time.\n\n### Vector Notation\n\nThe magnitude and direction are described as:\n\n$$\\bold{r}(t) = x(t) \\hat{\\bold{i}} + y(t) \\hat{\\bold{j}} + z(t) \\hat{\\bold{k}} $$\n\n### Magnitude\n\nThe magnitude of the position vector is:\n\n$$||\\bold{r}(t) || = \\sqrt{x^2 + y^2 + z^2} = \\sqrt{\\bold{r} \\cdot \\bold{r}}$$\n\n## Velocity\n\nThe velocity of a particle describes its motion, or the change in position over the change in time. \n\n$$\\bold{v}(t) = \\frac{dx(t)}{dt} \\hat{\\bold{i}} + \\frac{dy(t)}{dt} \\hat{\\bold{j}} + \\frac{dz(t)}{dt} \\hat{\\bold{k}}$$\n\n## Acceleration\n\nThe acceleration of a particle describes how its motion changes, or the change in velocity over the change in time. \n\n$$\\bold{a}(t) = \\frac{d^2x(t)}{dt^2} \\hat{\\bold{i}} + \\frac{d^2y(t)}{dt^2} \\hat{\\bold{j}} + \\frac{d^2z(t)}{dt^2} \\hat{\\bold{k}}$$\n\n# Netonian Mechanics\n\n## Force\n\nTo alter the motion of a particle, you must exert a force on it.\n\n### Newton's Laws\n\n#### First Law\n\nA body at rest remains at rest. A body in motion tends to stay in motion. These statements are true given no net external forces act upon the body.\n\n#### Second Law\n\nThe net force on a body of mass $m$ with an absolute (inertial frame of reference) acceleration of the center of mass $a$.\n\n$$\\bold{F}_{net} = m\\bold{a}$$\n\n#### Third Law\n\nGiven a force on a body, there is an equal and opposite one.\n\n$$\\bold{F}_{\\text{a on b}} = -\\bold{F}_{\\text{b on a}}$$\n\n### Common Forces\n\n#### Gravity\n\nGravity always points downwards from a body.\n\n$$\\bold{w} = m\\bold{g}$$\n\n#### Normal Force\n\nThe normal force always points normal to the surface of the body.\n\n$$\\bold{n}$$\n\n#### Tension\n\nTension is the force along the length of a flexible connection (e.g. rope). It points along the material.\n\n$$\\bold{T}$$\n\n#### Friction\n\nFriction is the opposing force of a body's motion, pointing opposite to the direction of motion.\n\n## Impulse\n\nImpulse is overall effect of force over time.\n\n$$\\bold{I} = \\int_{t_1}^{t_2} \\bold{F} \\text{ } dt$$\n"},512:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Microelectronics\ndescription: The study of small electrical components.\n---\n\n# Brief\nWe'll analyze circuit components such as diodes, amplifiers and MOSFETs.\n\n# Operational Amplifiers (Op amp)\nThe main goal of an operational amplifier is to amplify a voltage. It has three terminals, two inputs and one output, as seen in the figure:\n\n![opamp](/microelectronics/opamp.svg)\n\nAn op amp is operated by DC power supplies with one rail connected to positive voltage and the other connected to negative voltage.\n\n## Ideal Operation Amplifier Model\n\nWe can assume four characteristics of an ideal op amp:\n\n1. $i_{input} = 0 \\implies R_i \\rightarrow \\infty$. No current flows into the input terminals, thus the input impedance is infinite.\n2. $v_o = A(v_2-v_1) \\implies R_o \\rightarrow 0$. The output terminal acts like an ideal voltage source, where $v_o = A(v_2-v_1)$, so the output impedance is zero.\n3. Bandwidth $\\rightarrow \\infty$. In other words, the op amp can amplify signals at any frequency with equal gain.\n4. Infinite Open Loop Gain. $A \\rightarrow \\infty$.\n4. Infinite Common Mode Rejection, so $v_1=v_2$ assuming the differential gain $A \\rightarrow \\infty$.\n    * $\\displaystyle \\lim_{A \\rightarrow \\infty} \\frac{v_o}{A} = v_2 - v_1 \\implies v_1 = v_2$.\n    * Common mode rejection is the amplifier's ability to rid of the common mode voltage. In other words, if an ideal op amp removed all of the common mode voltage, then say $v_1=v_2=1v$. \n    \n    $$v_o = A(v_2-v_1) = A(1 - 1) = 0$$\n\n### Differential & Common Mode Signals\n\n#### Differential Input\n- The difference between the input signals $v_1$ and $v_2$.\n\n$$v_{Id} = v_2 - v_1$$\n\n#### Common Mode Input\n- The average of the input signals $v_1$ and $v_2$.\n\n$$v_{Icm}=\\frac{1}{2} (v_1+v_2)$$\n\n### Closed Loop Gain\n\n$$G \\equiv \\frac{v_O}{v_I}$$\n\n# MOSFETs\nA MOSFET is a metal oxide semiconductor (MOS) field effect transistor (FET). It is a two terminal semiconductor device that is often utilized in integrated circuits. \n\n## N-channel MOS\nThe N-channel MOS is one that has a heavily doped n-channel regions in the source and drain regions. It has the drain, source, gate and body.\n\n### NMOS Dependencies\n\n#### Gate Voltage $V_G$\n\n#### Drain Voltage $V_D$\n\n#### Source Voltage $V_S$\n\n#### Gate to Source Voltage\n- The potential difference between the gate and source.\n\n$$V_{GS} = V_G - V_S$$\n\n#### Drain to Source Voltage\n- The potential difference between the drain and source.\n\n$$V_{DS} = V_D - V_S$$\n\n#### Threshold Voltage $V_t$\n\n- The minimum voltage required to form a conducting channel between the source and drain terminals.\n- A manufacturing dependent value.\n\n#### Effective / Overdrive Voltage\n\n- The excess $V_{GS}$ over $V_t$.\n- Determines charge of the conducting channel.\n\n$$V_{OV} \\equiv V_{GS} - V_t$$\n\n#### Channel Width $W$\n\n#### Channel Length $L$\n\n#### Permittivity of Silicon Dioxide\n\n$$\\epsilon_{ox} = 3.9 \\epsilon_0 = 3.9 \\times 8.854 \\times 10^{-12} = 3.45 \\times 10^{-11} \\frac{F}{M}$$\n\n#### Oxide Thickness $t_o\\phantom{}_x$\n\n- A manufacturing dependent value.\n\n#### Oxide Capacitance\n\n- The capacitance of the parallel plate capacitor per unit gate area.\n\n$$C_{ox} = \\frac{\\epsilon_{ox}}{t_{ox}}$$\n\n#### Electron Charge Magnitude\n\n$$|Q| = C_{ox}(WL)V_{OV}$$\n\n#### Gate to Channel Capacitance\n\n$$C=C_{ox}WL$$\n\n#### Electron Mobility $\\mu_n$\n\n#### Process Transconductance\n\n$$k^{'}_{n} = \\mu_n C_{ox}$$\n\n#### MOSFET Transconductance Parameter\n\n$$k_n = k^{'}_{n} (\\frac{W}{L}) = \\mu_n C_{ox} (\\frac{W}{L})$$\n\n#### MOSFET Resistance\n\n$$r_{DS}=\\frac{1}{g_{DS}} = \\frac{1}{\\mu_n C_{ox} (\\frac{W}{L})V_{OV}}$$\n\n### NMOS Regions of Operation\n\n#### Cutoff\n\n- As long as the following conditions are satisfied, the MOSFET is in cutoff:\n\n$$V_{GS} < V_{tn}$$\n\n- If the MOSFET is in cutoff, the drain current is zero ($i_D=0$).\n- In this state, the channel is pitched so no current can flow.\n\n#### Triode\n- As long as the following conditions are satisfied, the MOSFET is in triode:\n\n$$V_{GS} > V_{tn}$$\n$$V_{GD} > V_t \\text{ or } V_{DS} < V_{OV}$$\n\n- The drain current when in triode is:\n\n$$i_D= k'_n \\frac{W}{L} [(V_{GS}-V_{tn})V_{DS}-\\frac{1}{2} V_{DS}^2]$$\n\n#### Saturation\n\n- As long as the following conditions are satisfied, the MOSFET is in triode:\n\n$$V_{GS} > V_{tn}$$\n$$V_{GD} \\leq V_{tn} \\text{ or } V_{DS} \\geq V_{OV}$$\n\n- The drain current when in triode is:\n\n$$i_D= k'_n \\frac{W}{L} (V_{GS}-V_{tn})^2$$\n"},513:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Machine Learning\ndescription: Teaching machines.\n---\n\n\n"},514:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Orbital Mechanics\ndescription: Large bodies in motion.\n---\n\n# Two Body Orbital Mechanics\n\n## Motion in an Inertial Frame\n\n### Mechanics of the Center of Mass of Two Bodies\n\n#### Position\n\nThe position of the center of mass $G$ is:\n\n$$\\bold{R}_G = \\frac{m_1 \\bold{R_1} + m_2 \\bold{R_2}}{m_1 + m_2}$$\n\n### Velocity\n\nThe velocity of the center of mass $G$ is:\n\n$$\\bold{v}_G = \\dot{\\bold{R}}_G = \\frac{m_1 \\dot{\\bold{R_1}} + m_2 \\dot{\\bold{R_2}}}{m_1 + m_2}$$\n\n### Acceleration\n\nThe acceleration of the center of mass $G$ is:\n\n$$\\bold{a}_g = \\ddot{\\bold{R}}_G = \\frac{m_1 \\ddot{\\bold{R_1}} + m_2 \\ddot{\\bold{R_2}}}{m_1 + m_2}$$"},515:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Probability\ndescription: Probably relevant.\n---\n\n# Probabilistic Models\n\n### Experiment\n\nThe problem at hand.\n\n### Outcome\n\nThe results of the [experiment](#experiment).\n\n### Sample Space ($\\Omega$)\n\nThe set of all possible [outcomes](#outcome) of an [experiment](#experiment).\n\n### Event\n\nA subset of the [sample space](#sample-space-omega) containing the possible [outcomes](#outcome).\n\n### Mutual Exclusivity (Disjointness)\n\nElements of the [sample space](#sample-space-omega) should be distinct.\n\n### Collectively Exhaustive\n\nAll of the [events](#event) must be accounted for in the [sample space](#sample-space-omega).\n\n# Probability Axioms\n\n### Non-negativity\n\nFor each event $A$, \n\n$$\\bold{P}(A) \\geq 0$$\n\n### Additivity\n\nIf the sets $A$ and $B$ are [disjoint](/sets/#disjoint) events, then:\n\n$$\\bold{P}(A \\cup B) = \\bold{P}(A) + \\bold{P}(B)$$\n\n### Normalization\n\nThe probability of the [sample space](#sample-space-omega) is equal to $1$.\n\n$$\\bold{P}(\\Omega) = 1$$\n\n# Discrete Models\n\n### Discrete Probability\n\nThe probability of an event $\\left\\{s_1, s_2, ..., s_3\\right\\}$ is the sum of of the probabilities of its elements:\n\n$$\\bold{P}(\\left\\{s_1, s_2, ..., s_3\\right\\}) = \\bold{P}(s_1) + \\bold{P}(s_2) + ... + \\bold{P}(s_n) $$\n\n### Discrete Uniform Probability Law\n\nGiven an event $A$ with $n$ possible outcomes, the probability of any event $A$ is:\n\n$$\\bold{P}(A) = \\frac{\\text{number of elements of A}}{n}$$\n\n### Intersection\n\n$$\\bold P (AB) = \\bold P(A) + \\bold P(B) - \\bold P(A\\cup B)$$\n\n# Conditional Probability\n\nWhen only partial information is given and we want to know the outcome of an experiment, conditional probability is useful.\n\nLet $A$ and $B$ be two events. The conditional probability of $A$ given $B$ is denoted as:\n\n$$\\bold{P}(A|B) = \\frac{\\bold{P}(A \\cap B)}{\\bold{P}(B)} = \\frac{\\text{number of elements} \\in A \\cap B}{\\text{number of elements} \\in B}$$\n\nwhere $\\bold{P}(B) \\gt 0$.\n\nIn other words, the conditional probability is the ratio of $A$ and $B$ occurring to the probability of $B$ occurring. $B$ has already occurred, and so the only probability left of occurring is intersect between $A$ and $B$.\n\n## Multiplication Rule\n\nAssuming all of the conditioning events have positive probability,\n\n$$\\bold{P}\\bigg(\\bigcap^n_{i=1} A_i\\bigg) = \\bold{P}(A_1) \\bold{P}(A_2|A_1)\\bold{P}(A_3|A_1 \\cap A_2) ... \\bold{P}\\bigg(A_n | \\bigcap^{n-1}_{i=1}) A_i\\bigg)$$\n\nThis rule is often seen when multiplying independent events together in a tree diagram.\n\n## Total Probability Theorem\n\nLet $A_1 ... A_n$ be disjoint events that form a partition of the sample space and assume $\\bold{P}(A_i) > 0 \\forall i$. For any event $B$,\n\n$$\\bold{P}(B) = \\bold{P}(A_1 \\cap B) + ... + \\bold{P}(A_n \\cap B) \\\\\n= \\bold{P}(A_1)\\bold{P}(B|A_1) + ... + \\bold{P}(A_n)\\bold{P}(B|A_n)$$\n\nMore compactly,\n\n$$\\bold P(B) = \\sum_{i = 1}^n P(A_i)P(B | A_i)$$\n\nThis theorem is often seen when adding together various event outcomes to calculate the probability of an event occurring.\n\n## Bayes' Rule\n\nBayes' rule is used when finding the reversed conditional probability. \n\n$$\\bold{P}(A_i | B) = \\frac{\\bold{P}(A_i)\\bold{P}(B | A_i)}{\\bold{P}(B)} = \\frac{\\bold{P}(A_i) \\bold{P}(B|A_i)}{\\bold{P}(A_1) \\bold{P}(B|A_1) + ... + \\bold{P}(A_n) \\bold{P}(B|A_n)}$$\n\n# Independence\n\n$A$ is independent of $B$ if \n\n$$\\bold{P}(A|B) = \\bold{P}(A) = \\frac{\\bold{P}(A \\cap B)}{\\bold{P}(B)} = \\frac{\\bold{P}(A) \\bold P(B)}{\\bold{P}(B)}$$\n\nAlso, if $\\bold{P}(B) > 0$\n\n$$\\bold{P}(A|B) = \\bold{P}(A)$$\n\n# Counting Principle\n\nLet's say there is a process consisting of $r$ stages.\n- $n_1$ possible results in the first stage. \n- For every possible result of the first stage, there are $n_2$ possbile results at the second stage.\n\n### $k$-permutations\n\nLet's say we want to count the number of permutations with $n$ distinct objects, but we only have room to choose $k$ objects.\n\n- $n$ distinct objects\n- Pick only $k$ out of $n$ objects\n- Order matters (number of arrangements)\n- $n \\geq k$\n\n$$\\frac{n!}{(n-k)!}$$\n\n### Permutations\n\nLet's say we have $n$ distinct objects and have $n$ spots. In other words, $n = k$. We aim to find how many sequences there are when choosing $k$ balls.\n\n#### With Replacement\n\nFor the first spot, there are $n$ possibilities. Then, for the next spot, we replace the object, so the number of possibilities stays the same as the first, $n$.\n\nWe continue this until $k$.\n\n$$n \\cdot n \\cdot n \\cdots = n^k$$\n\n#### Without Replacement\n\n$$P^n_k = n(n-1)(n-2) \\cdots 2 \\cdot 1 = n!$$\n\nFor the first spot, there are $n$ possibilities. Then, for the next spot, we do not replace the object, so we lose one possibility; thus we are left with $n-1$ possibilities.\n\nWe continue this until $n - (k - 1)$.\n\n### Combinations\n\n#### Without Repetition\n\nLet's say we want to count the number of combinations with $n$ distinct objects, but we only want to choose $k$ objects.\n\n- $n$ distinct objects\n- Pick only $k$ out of $n$ objects\n- Order doesn't matter\n- $n \\geq k$\n\n$${ n \\choose k } = \\frac{n!}{k!(n-k)!}$$\n\n# Partitions\n\nLet's say we wanted to split $n$ distinct objects into $r$ groups, and those groups are of size $n_1, n_2, \\ldots, n_r$ where $n_1 + n_2 + \\ldots + n_r = n$. \n\nIn other words, $n_1, n_2, \\ldots, n_r$ is just the partition of $n$.\n\nWe use the counting principle and define it as the multinomial coefficient,\n\n$${n \\choose n_1,n_2,\\ldots, n_r} = {n \\choose n_1} {n - n_1 \\choose n_2} {n - n_1 - n_2 \\choose n_3} \\cdots {n - n_1 - \\ldots - n_{r-1} \\choose n_r}$$\n\n$$\\frac{n!}{n_1!(n-n_1)!} \\cdot \\frac{(n-n_1)!}{n_2!(n-n_1-n_2)!} \\cdots \\frac{n-n_1-\\ldots-n_{r-1}!}{n_r!(n-n_1-\\ldots-n_{r-1} - n_r)!}$$\n\nFirst, we choose $n_1$ objects to insert into the first group from the pool of $n$.\n\nNext, from the remaining pool of $n - n_1$, we choose however many objects we want to insert into group $n_2$.\n\nWe repeat this until the group $r$.\n\n# Discrete Random Variables\n\nA random variable $X$ is a function that assigns a real number $X(w)$ to each outcome $w \\in \\Omega$.\n\n## Probability Mass Function (PMF)\n\nIf $S_X$ is a discrete set, $X$ is a discrete random variable.\n\nWe can assign a probability to each element $x \\in S_X$, and this assignment is called the probability mass function (PMF). In essence, we are assigning some weight to how probable an element $x$ occurs. We denote the PMF by:\n\n$$\\bold{P}(X = x)$$\n\nGiven a PMF, it has the properties:\n- $\\bold{P}(X = x) \\geq 0$\n- $\\displaystyle \\sum_x \\bold{P}(X = x) = 1$\n\n## Cumulative Distribution Function (CDF)\n\nThe cumulative distribution function describes the situation in which the random value $X$ will take a value less than or equal to $x$.\n\n$$F_X(x) = \\bold{P}(X \\leq x) = \\sum_{k = - \\infty}^x P(X = k)$$\n\nIn other words, it's the cumulative probability that of a distribution up to a value $x$.\n\n## Probability Generating Function (PGF)\n\nThe probability generating function is a sequence encompasses both the probabilities and possible values associated with a random variable.\n\n$$G_X(z) = \\bold{E}[z^x] = \\sum_{k=0}^\\infty z^k \\bold{P} (X=k) = z^0 \\bold{P}(K = 0) + z^1 \\bold{P}(K = 1) + z^2 \\bold{P}(K = 2) + \\dots$$\n\n- Each exponent $k$ of $z^k$ denote the possible values a random variable can take on. \n- The coefficient of $z^k$ denotes the probability attached to the possible value.\n\nIf we take the first derivative:\n\n$$\\frac{\\text{d}G_X}{\\text{d}z} = \\sum_{k=0}^\\infty k z^{k-1} \\bold{P} (X=k)$$\n\nAnd second derivative:\n\n$$\\frac{\\text{d}^2G_X}{\\text{d}z^2} = \\sum_{k=0}^\\infty k(k-1) z^{k-2} \\bold{P} (X=k)$$\n\n### Expectation Value\n\nThe expectation value is the average of the probability mass function. In other words, it is the value we expect the most, as this is where the distribution of weights is located in the probability mass function.\n\nIt is the first derivative of the [PGF](#probability-generating-function-pgf) where $z=1$.\n\n$$\\frac{\\text{d}G_X}{\\text{d}z}\\Bigr|_{z=1} = \\bold{E}[X] = \\sum_{k \\in S_x} k \\bold{P} (X = k)$$\n\n### Second Moment\n\nThe second moment is the sum of the first and second derivatives of the [PGF](#probability-generating-function-pgf) where $z=1$.\n\n$$\\frac{\\text{d}^2G_X}{\\text{d}z^2}\\Bigr|_{z=1} + \\frac{\\text{d}G_X}{\\text{d}z}\\Bigr|_{z=1} = \\bold{E}[X^2]$$\n\n### Variance\n\nThe variance tells us how far numbers are spread out from the average or expected value.\n\n$$\\text{Var}[X] = \\bold{E}[X^2] - \\bold{E}[X]^2 = \\frac{\\text{d}^2G_X}{\\text{d}z^2}\\Bigr|_{z=1} + \\frac{\\text{d}G_X}{\\text{d}z}\\Bigr|_{z=1} - \\bigg(\\frac{\\text{d}G_X}{\\text{d}z}\\Bigr|_{z=1}\\bigg)^2$$\n\n### Standard Deviation\n\nThe standard deviation describes the spread of values.\n\n$\\text{stdiv} = \\sigma = \\sqrt{\\text{var}[X]}$\n\n## Discrete Random Variable Conditioning\n\n$$\\bold P(X = x \\mid Y = y) = \\frac{\\bold P(X = x, Y = y)}{\\bold P(Y = y)}$$\n\n## Bernoulli Distribution\n\nThe Bernoulli distribution describes the situation in which the probability mass function consists of a binary set of outcomes, $S_X = \\{ 0, 1 \\}$. It is a special case of the binomial distribution, where $n = 1$.\n\n### Probability Mass Function\n\n$$\\bold P (X = x) = \\begin{cases}p & x = 1\\\\1-p & x = 0\\end{cases}$$\n\n### Cumulative Density Function\n\n$$\\bold P (X \\leq x) = \\begin{cases}\n0 & x < 0 \\\\\n1 - p & 0 \\leq x < 1 \\\\\np & x \\geq 1\n\\end{cases}$$\n\n### Expected Value\n\n$$\\bold E[X] = p$$\n\n### Variance\n\n$$\\text{var}[X] = p(1-p)$$\n\n### Transform\n\n$$M_X(s) = 1 - p + pe^s \\quad k = 0, 1$$\n\n## Binomial Distribution\n\nThe binomial probability mass function describes the situation in which the $n$ independent trials occur with $k$ successes with probability $p$ and $n - k$ fails with probability $1-p$ with replacement.\n\n### Probability Mass Function\n\n$$\\bold P(X = k) = {n \\choose k} p^k (1-p)^k$$\n\n### Cumulative Density Function\n\n$$\\bold P(X \\leq x) = \\sum_{i = 1}^x {n \\choose i} p^i (1-p)^{n-i}$$\n\n### Expected Value\n\n$$\\bold E[X] = np$$\n\n### Variance\n\n$$\\text{var}[X] = np(1-p)$$\n\n### Transform\n\n$$M_X(s) = (1 - p + pe^s)^n \\quad k = 0, 1, \\dots, n$$\n\n## Geometric Distribution\n\nThe geometric probability mass function describes the situation in which $n$ independent trials are required before the event occurs with probability $p$.\n\n### Probability Mass Function\n\n$$P(X = n) = (1-p)^{n-1}p$$\n$$\\sum_{k=1}^\\infty \\bold{P}(X = k) = \\sum_{k=1}^\\infty (1-p)^{k-1} p$$\n\nIn other words, $k-1$ trials occur before the event occurs.\n\n### Cumulative Density Function\n\n$$\\bold P (X \\leq x) = 1-(1-p)^{n-1}$$\n\n### Expected Value\n\n$$\\bold E[X] = \\frac{1}{p}$$\n\n### Variance\n\n$$\\text{var}[X] = \\frac{1-p}{p^2}$$\n\n### Transform\n\n$$M_X(s) = \\frac{pe^s}{1 - (1 - p) e^s)} \\quad k = 1, 2, \\dots$$\n\n## Hypergeometric Distribution\n\nThe hypergeometric distribution is a discrete distribution that characterizes the probability of observed $k$ successes in $n$ draws without replacement. $N$ is the population size and $K$ is the number of successes in the population.\n\n### Probability Mass Function\n\n$$\\bold P (X = k) = \\frac{\\begin{pmatrix} K \\\\ k \\end{pmatrix} \\begin{pmatrix} N - K \\\\ n - k \\end{pmatrix}}{\\begin{pmatrix} N \\\\ n \\end{pmatrix}}$$\n\n### Multivariate Hypergeometric Distribution\n\n$$\\frac{\\displaystyle \\prod_{i = 1}^c \\begin{pmatrix} K_i \\\\ k_i \\end{pmatrix}}{\\begin{pmatrix} N \\\\ n \\end{pmatrix}}$$\n\nFor $c$ different populations $K_i$, we select $k_i$ people.\n\n$N = \\displaystyle \\sum_{i=1}^c K_i$ is the total number of populations.\n\n## Poisson Distribution\n\nThe Poisson distribution is a discrete distribution that is useful for calculating the likelihood of the average rate of occurrences $\\lambda$ of an event over some time $t$. $k$ is the number of occurrences.\n\n$$\\bold P(X = k) = \\frac{\\lambda t^k e^{-\\lambda t}}{k!} \\quad k = 0, 1, 2, \\dots$$\n\n### Expected Value\n\n$$\\bold E[X] = \\lambda t$$\n\n### Variance\n\n$$\\text{var}[X] = \\lambda t$$\n\n### Transform\n\n$$X(z) = e^{\\lambda t (z - 1)}$$\n\n## Functions of Random Variables\n\nGiven a random variable $X$, we can generate other random variables by applying various transformations on $X$. If $Y$ depends on $X$: $Y = g(X)$\n\n$$p_Y (y) = \\sum_{x \\mid g(x) = y } p_X(x) $$\n\n## Joint Probability Mass Function (PMF)\n\nThe joint PMF is denoted by\n\n$$\\bold P(\\{ X = x, Y = y \\})$$\n\nIt represents the probability of both $X$ and $Y$ occurring.\n\n### Marginal PMF\n\nThe marginal PMF allows us to obtain individual random variable values from a vector random variable.\n\n$$\\bold P(X = x) = \\sum_y \\bold P({X = x, Y = y}) $$\n$$\\bold P(Y = y) = \\sum_x \\bold P({X = x, Y = y}) $$\n\n# Continuous Random Variables\n\nA continuous random variable infinitely many values, as opposed to discrete random variables where only countable values are considered.\n\n## Probability Density Function (PDF)\n\nA PDF is analogous to a [PMF](#probability-mass-function-pmf). It defines the distribution of probabilities over a range of values.\n\n$$f_X(x) = \\frac{\\text dF_X(x)}{\\text dx}$$\n\nA function $f_X(x)$ is a PDF if it satisfies:\n\n- $f_X(x) \\geq 0 \\forall x$\n- $\\displaystyle \\int_{-\\infty}^\\infty f_X(x) \\text dx = 1$\n- $\\forall$ numbers $a$, $b$ then $\\displaystyle \\bold P [a \\leq X \\leq b] = \\int_a^b f_X(x) \\text dx$\n\n### Marginal PDF\n\nThe marginal PDF extracts the density function for a random variable from a joint PDF.\n\n$$f_X(x) = \\int_{-\\infty}^\\infty f_{X,Y}(X,Y) dy$$\n\n$$f_Y(y) = \\int_{-\\infty}^\\infty f_{X,Y}(X,Y) dx$$\n\n## Cumulative Distribution Function (CDF)\n\nThe CDF of a random variable describes the distribution and is defined as:\n\n$$F_X (x) = \\bold P(X \\leq x) = \\int_{-\\infty}^x f_X(t) dt$$\n\nIn continuous time, for an infinitesimally large step $\\delta$, the probability is found to be zero.\n\n$$\\bold P [x \\leq x \\leq x + \\delta] = \\int_x^{x+\\delta} f_X(x) \\text dx \\approx f_X(\\delta) \\delta X \\approx 0$$\n\nIn a semi-closed interval $(a, b]$ for $a< b$,\n\n$$P(a < X \\leq B) = F_X(b) - F_X(a)$$\n\n## Conditioning\n\n### On an event\n\n$$\\bold{P} (X \\in B \\mid X \\in A) = \\frac{\\bold P (X \\in B, X \\in A)}{\\bold P (X \\in A)} = \\frac{\\displaystyle \\int_{A \\cap B} f_X(x) dx)}{\\bold P (X \\in A)} = \\int_B f_{X \\mid A} (x) dx $$\n\n# General Random Variables\n\n## Uniform Distribution\n\nA uniformly distributed random variable has a constant probability density function.\n\n### Probability Density Function\n\n$$f_X(x) = \\begin{cases} \\frac{1}{b - a} & a \\leq x \\leq b \\\\ 0 & \\text{otherwise} \\end{cases}$$\n\n### Cumulative Distribution Function\n\n$$F_X(x) = \\int_{a}^x f_X(x) \\text dx = \\begin{cases} 0 & x < a \\\\ \\frac{x - a}{b - a} & a \\leq x \\leq b \\\\ 0 & \\text{otherwise} \\end{cases}$$\n\n### Expected Value\n\n$$\\bold E[X] = \\frac{1}{2} (a+b)$$\n\n### Variance\n\n$$\\text{var}[X] = \\frac{1}{12} (b-a)^2$$\n\n### Transform\n\n$$M_X(s) = \\frac{e^{as}}{b - a + 1} \\cdot \\frac{e^{(b - a + 1)s} - 1}{e^s - 1} \\quad k = a, a + 1, \\dots, b$$\n\n## Exponential Distribution\n\nAn exponentially distributed random variable has an exponentially growing probability density function with a rate $\\lambda$.\n\nIt is often used to calculate the inter-arrival time.\n\n### Probability Density Function\n\n$$f_X (x) = \\begin{cases} \\lambda e^{-\\lambda x} & x \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases} = \\lambda e^{-\\lambda x} u(x)$$\n\n### Cumulative Distribution Function\n\n$$F_X(x) = \\int_0^x \\lambda e^{-\\lambda \\beta} \\text d\\beta = 1-e^{-\\lambda x}$$\n\n### Expected Value\n\n$$\\bold E[X] = \\frac{1}{\\lambda}$$\n\n### Variance\n\n$$\\text{var}[X] = \\frac{1}{\\lambda^2}$$\n\n### Transform\n\n$$M_X(s) = \\displaystyle \\frac{\\lambda}{\\lambda - s} \\quad s < \\lambda$$\n\n## Gaussian / Normal Distribution\n\nA Gaussian or normally distributed distribution $X$ is only continuous.\n\n$$f_X (x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-(x-\\mu)^2 / 2\\sigma^2}$$\n\nIt has scalar parameters:\n\n$$\\sigma = \\sqrt{\\text{var}(X)} \\quad \\sigma > 0$$\n$$\\mu = \\bold E [X]$$ \n\n### Expected Value\n\n$$\\bold E [X] = \\mu$$\n\n### Variance\n\n$$\\text{var}(X) = \\sigma^2$$\n\n### Transform\n\n$$M_X(s) = \\displaystyle e^{(\\sigma^2 s^2 / 2) + \\mu s}$$\n\n## Standard Normal Distribution\n\nA normal random variable $X$ is standard normal if it has $\\bold E [X] = 0$ and $\\text{var}(X) = 1$. It is useful to find values for normal distributions since solving for the regular normal random variable is difficult. We define:\n\n$$\\phi(y) = \\bold P (Y \\leq y) = P(Y < y) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^y e^{-t^2 / 2} dt$$\n\nLet $Y$ be another random variable (that is also normal because of linearity) with $\\bold E [Y] = \\mu$ and $\\text{var}(Y) = \\sigma^2$. Then:\n\n$$Y = \\frac{X - \\mu}{\\sigma}$$\n\nA useful property is that $\\phi(-x) = 1 - \\phi(x) \\quad \\forall x$.\n\n### Calculating the Normal Distribution\n\nTo properly use the standard normal table, we need to normalize the normal distribution to have a mean $\\bold \\mu = E[X] = 0$ and variance $\\sigma^2 = 1$, also written as $X \\sim N(\\mu, \\sigma^2) = N(0, 1)$.\n\nWe calculate the standard normal variable $Y$, which is the normalized version of the normal random variable:\n\n$$\\bold P (X \\leq x) = \\bold P \\bigg(\\frac{X - \\mu}{\\sigma} \\leq \\frac{x - \\mu}{\\sigma}\\bigg) = \\bold P \\bigg(Y \\leq \\frac{x - \\mu}{\\sigma}\\bigg) = \\phi \\bigg(\\frac{x - \\mu}{\\sigma} \\bigg) $$\n\n## Erlang Distribution\n\nThe Erlang distribution is a **continuous distribution** that is useful for calculating the probability the of wait time given the average rate of occurrences $\\lambda$ and $n$ number of occurrences.\n\nIn other words, the $n$-Erlang distribution is defined by a sum of exponential random variables $X = X_1 + X_2 + \\dots + X_n$ where $X_i$ represents the $i$-th occurrence.\n\nGiven $n \\in \\mathbb{R}^+$ with a rate $\\lambda > 0$, the $n$-Erlang PDF is given by:\n\n$$f_X(x) = \\frac{\\lambda^n x^{n - 1} e^{-\\lambda x}}{(n - 1)!} \\quad x \\geq 0$$\n\n### Expected Value\n\n$$\\bold E[X] = \\frac{n}{\\lambda}$$\n\n### Variance\n\n$$\\text{var}[X] = \\frac{n}{\\lambda^2}$$\n\n### Transforms\n\n$$M_X(s) = \\bigg(\\frac{\\lambda}{\\lambda + s}\\bigg)^n$$\n\n## Rayleigh Distribution\n\nThe Rayleigh distribution is a continuous distribution used to often measure component lifetime.\n\n$$f_X(x) = x e^{-x^2/2}$$\n\nThis PDF originates from the magnitude of two independently, normally distributed, and non-negative random variables, $\\sqrt{X^2 + Y^2}$.\n\n# Derived Distributions\n\nA derived distribution is useful for finding a PMF or PDF of a function of one or more random variables.\n\n**Note**: When finding the expected value, we do not need to calculate derived distributions.\n\n## Discrete\n\nGiven a random variable $Y = g(X)$:\n\n$$p_Y = \\bold P (g(X) = y) = \\sum_{x: g(x) = y} p_X(x)$$\n\n## Continuous\n\nGiven a function of one random variable $Y = g(X)$ and its PDF $f_X(x)$:\n\nFirst, get the CDF of $Y$:\n\n$$F_Y(y) = \\bold P (g(X) \\leq y) = \\int_{x: g(x) \\leq y} f_X(x) dx$$\n\nThen differentiate the CDF:\n\n$$f_Y(y) = \\frac{dF_Y}{dy}(y)$$\n\n## Solving Cases\n\n### Multi-variable Derived Distribution\n\nGiven a function of random variable $W = g(X, Y)$ and its PDF $f_W(w)$:\n\nFirst, get the CDF of $W$:\n\n$$F_W(w) = \\bold P (g(X,Y) \\leq w) = \\iint_{(x, y) \\mid g(x,y) \\leq w} f_W(w) dydx$$\n\nThen differentiate the CDF:\n\n$$f_W(w) = \\frac{dF_W}{dw}(w)$$\n\n### Multi-variable Derived Distribution with Joint PDF\n\nGiven $V = g_1(X, Y)$ and $W = g_2(X, Y)$ with joint PDF $f_{X,Y}(x,y)$ and\n\n- $v = g_1(x,y), w = g_2(x,y)$ can be uniquely solved for $x,y$ in terms of $v,w$; for example $x=h_1(v,w), y=h_2(v,w)$\n- The derivatives of $x,y$ with respect to $v,w$ are continuous and exist.\n\nThen, solve for $x=h_1(v,w), y=h_2(v,w)$ and use the equation:\n\n$$f_{V,W}(v,w) = f_{X,Y}[h_1(v,w), h_2(v,w)] |J(v,w)|$$ \n\nWhere $J(v,w)$ is the Jacobian:\n\n$$J(v,w) = \\begin{vmatrix}\n\\displaystyle \\frac{\\partial x}{\\partial v} & \\displaystyle \\frac{\\partial x}{\\partial w} \\\\[10pt]\n\\displaystyle \\frac{\\partial y}{\\partial v} & \\displaystyle \\frac{\\partial y}{\\partial w}\n\\end{vmatrix}$$\n\n### Sum of Independent Random Variables (Convolution)\n\n# Expectation\n\nThe expected value predicts the value of a random variable. \n\n$$\\bold E [g(X,Y)] = \\begin{cases}\n\\displaystyle \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(X,Y) f_{X,Y}(x,y) dxdy & \\text{continuous} \\\\\n\\displaystyle \\sum_{-\\infty}^{\\infty} \\sum_{-\\infty}^{\\infty} g(X,Y) p_{X,Y}(x,y)& \\text{discrete}\n\\end{cases}$$\n\n## Conditional Expectation\n\nGiven a fixed value $Y = y$, we can find the expected value of $X$.\n\n$$\\bold E[X | Y = y] = \\begin{cases}\n\\displaystyle \\int_{-\\infty}^\\infty x f_{X \\mid Y}(x | y) dx  & \\text{continuous} \\\\\n\\displaystyle \\sum_x x \\bold P(X = x \\mid Y = y) & \\text{discrete}\n\\end{cases}$$\n\n## Law of Iterated (Total) Expectation\n\n$$\\bold E[X] = \\bold E[\\bold E[X | Y]] = \\begin{cases}\n\\displaystyle \\int_{-\\infty}^\\infty \\bold E[X|Y=y] f_Y(y) dy & \\text{continuous} \\\\\n\\displaystyle \\sum_y \\bold E [X|Y=y] \\bold P[Y = y] & \\text{discrete}\n\\end{cases}$$\n\n## Covariance\n\nThe covariance tells us how two random variables $X$ and $Y$ are related.\n\n$$\\text{cov}(X,Y) = \\bold E[(X - \\bold E[X])(Y - \\bold E[Y])]$$\n\n$$\\bold E [XY] - \\bold E[X] \\bold E[Y]$$\n\n## Correlation\n\nThe correlation tells us how linearly related two random variables $X$ and $Y$ are.\n\n$$\\rho_{XY}(x,y) = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} = \\frac{\\bold E [XY] - \\bold E[X] \\bold E[Y]}{\\sigma_x \\sigma_y}$$\n\n- If $X,Y$ are independent, then $\\rho_{X,Y} = 0$.\n- $\\rho_{X,Y}$ = 0 means $X$ and $Y$ are uncorrelated.\n- $-1 \\leq \\rho_{X,Y} \\leq 1$\n  - $\\rho_{X,Y}(x,y) \\rightarrow -1$ with high $X$ and low $Y$\n  - $\\rho_{X,Y}(x,y) \\rightarrow 1$ with low $X$ and high $Y$\n- $\\rho_{X,Y} = 1 \\iff Y = aX+b \\quad a > 0$\n- $\\rho_{X,Y} = -1 \\iff Y = aX+b \\quad a < 0$\n\n## Sum of Expectations\n\nGiven a sum of random variables $W=X+Y$, we observe:\n\n$$\\bold E[W] = \\bold E[X+Y] = \\bold E[X] + \\bold E[Y]$$\n\nfrom the linearity of integration. Note that this property is true even if $X,Y$ are dependent.\n\n## Sum of Variances\n\nUnlike expectation, variance is not linear. Thus,\n\n$$\\text{var}[aX+bY] = a^2 \\text{var}[X] + b^2 \\text{var}[Y] + 2ab \\text{ } \\text{cov}(X,Y)$$\n\n## Mean Squared Error\n\nIf we want to estimate the value of an unobserved random variable $X$, we can calculate the mean squared error.\n\n$$\\text{MSE} = [M_n - \\bold E[X]] = \\text{var}[M_n] = \\frac{1}{n^2} \\sum_{i=1}^n X_i = \\frac{1}{n^2} n \\sigma_x^2 = \\frac{\\sigma_x^2}{n}$$\n\n# Transforms & Moment Generating Function\n\nThe moment generating function represents the distribution of probabilities over the reals. It is similar to the probability generating function, except here we use $e^s$ instead of $z$.\n\n$$M_X(s) = \\bold E [e^{sX}] = \\begin{cases} \\displaystyle \\int_{-\\infty}^\\infty e^{sx} f_X(x) dx & X \\text{continuous} \\\\ \\displaystyle \\sum_x e^{sx} p_X(x) & X \\text{discrete} \\end{cases}$$\n\n## Expected Value\n\n$$\\bold E [X] = \\frac{d}{ds} M_X(s)\\Bigr|_{s=0}$$\n$$\\bold E [X^n] = \\frac{d^n}{ds^n} M_X(s)\\Bigr|_{s=0}$$\n\n## Linearity\n\n- If $Y = aX+b$ then $M_Y(s) = e^{sb}M_X(as)$\n\n## Independence\n\n### With transforms\n\nGiven $W = X + Y$, if $X$ and $Y$ are independent, then the sum is the multiplication of transforms. $W$'s distribution is characterized by:\n\n$$M_{W}(s) = \\bold E[e^{sW}] = \\bold E [e^{sX}] \\bold E [e^{sY}] = M_X(s) M_Y(s)$$\n\n### With convolution\n\nThe density function of multiple variables can also be obtained using convolution. Given $W = X + Y$, if $X$ and $Y$ are independent:\n\n#### Discrete Case\n\n$$p_W(w) = \\bold P (X + Y = w) = \\sum_x p_X(x) p_Y(w-x)$$\n\n#### Continuous Case\n\n$$\\bold P(W \\leq w | X = x) = \\bold P(Y \\leq w - x) = \\int_{-\\infty}^\\infty f_X(x) f_Y(w - x) dx$$\n\n## Multivariate Transform\n\nGiven $n$ random variables $X_1, \\dots, X_n$ and $s_1, \\dots, s_n$ scalars, the multivariate transform is defined as:\n\n$$M_{X_1, \\dots, X_n} (s_1, \\dots, s_n) = \\bold E [e^{s_1 X_1 + \\dots + s_n X_n}]$$\n\n# Limit Theorems\n\nLimit theorems answer questions to asymptotic behaviors to sequences of independently identically distributed random variables $S_n = X_1, X_2, \\dots, X_n$ as $n \\rightarrow \\infty$.\n\n## Sample Mean\n\nThe sample mean of a distribution is the average of picked values from the distribution.\n\n$$M_n = \\frac{1}{n} \\sum_{i = 1}^n X_i = \\frac{S_n}{n}$$\n\nFrom it, we can derive that\n\n$$\\bold E [M_n] = \\mu$$\n\n$$\\text{var}(M_n) = \\frac{\\sigma^2}{n}$$\n\n## Central Limit Theorem\n\nFor $X_1, X_2, \\dots$ independently and identically distributed random variables with common mean $\\bold E[X]$ and variance $\\sigma^2$.\n\n$$Z_n = \\frac{X_1 + \\dots + X_n - n\\mu}{\\sigma \\sqrt{n}}$$\n\nWe notice that as $n \\rightarrow \\infty$, $Z_n$ converges to the standard normal cumulative distribution function:\n\n$$\\lim_{n \\rightarrow \\infty} \\bold P(Z_n \\leq z) = \\Phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^z e^{-x^2 / 2} dx$$\n\n### De-Moivre - Laplace Approximation (1/2 approximation)\n\nFor a binomial random variable  with $n \\rightarrow \\infty$ and $p$ with $k,l \\in \\mathbb{Z}^+$\n\n$$\\bold P (k \\leq S_n \\leq l) = \\Phi(\\frac{l + \\frac{1}{2} - np}{\\sqrt{np(1-p)}}) - \\Phi(\\frac{k - \\frac{1}{2} - np}{\\sqrt{np(1-p)}}) $$\n\n## Markov Inequality\n\nFor a non-negative random variable $X$ and constant $a>0$,\n\n$$\\bold P (X \\geq a) \\leq \\frac{\\bold E [X]}{a}$$\n\nIn other words, the Markov inequality gives the confidence of the probability distribution for the upper bound of values $x \\geq a$.\n\n## Chebyshev Inequality\n\nFor any random variable $X$,\n\n$$\\bold P (|X - \\bold E[X]| \\geq a) \\leq \\frac{\\sigma_x^2}{a^2}$$\n\nIn other words, the Chebyshev inequality provides the confidence of the probability distribution for the range $\\bold E[X] \\pm a$.\n\n## The Weak Law of Large Numbers\n\nFor a finite mean $\\bold E[X]$ and $\\epsilon > 0$,\n\n$$\\lim_{n \\rightarrow \\infty} \\bold P [|M_n - \\mu| < \\epsilon] = 1$$\n\nIn other words, it tells us that if we had infinitely many data points, the sample mean converges in probability towards the expected value.\n"},516:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Quantum Mechanics\ndescription: Physics for small things.\n---\n\n# Energy Quantization\n\nIn quantum mechanics, a photon's energy is released in discrete packets of energy called quanta.\n\nThe energy of a photon is characterized by the equation:\n\n$$E = hv = \\frac{hc}{v}$$\n\n| Symbol | Meaning |\n| ------ | ------- |\n| $E$    | Energy of quanta |\n| $h$    | Planck's constant $6.626 \\cdot 10^{-34} J \\cdot s$ |\n| v      | Frequency of radiation |\n\n### Maximum Kinetic Energy of Photoelectron (Photoelectric Effect)\n\nThe **photoelectric effect** describes the event when photoelectrons are emitted and the photons fall incident to a material. It states that it takes a minimum amount of energy, $\\Phi$, the work function, to remove the electron from the material's surface.\n\n$$K = \\frac{1}{2}mv^2 = hv - \\Phi = hv - hv_0$$\n$$v \\geq v_0$$\n\n| Symbol | Meaning |\n| ------ | ------- |\n| $K$    | Max kinetic energy of photoelectron |\n| $\\Phi$ | Work Function |\n| $v$    | Frequency of radiation |\n\n# Wave-Particle Duality\n\nWaves have particle-like behaviors and vice versa.\n\nThe momentum of a photon is:\n\n$$p = \\frac{h}{\\lambda}$$\n\n| Symbol | Meaning |\n| ------ | ------- |\n| $p$    | Momentum of a photon |\n| $\\lambda$ | de Broglie Wavelength of the light wave |\n| $h$    | Planck's constant $6.626 \\cdot 10^{-34} J \\cdot s$ |\n\n# Heisenberg Uncertainty Principle\n\nThe Heisenberg uncertainty principle is especially useful for very small particles. It says that simultaneous measurements of two values has some error attached to it because of the scale.\n\n$$\\hbar = \\frac{h}{2 \\pi}$$\n\n$$\\Delta p \\Delta x \\geq \\hbar$$\n\n$$\\Delta E \\Delta t \\geq \\hbar$$\n\n# The Wave Function\n\nThe wave function, $\\Psi(x,t)$ describes the behavior of an electron in crystal.\n\nIt is written as:\n\n$$\\Psi(x, t) = \\psi(x)\\phi(t)$$\n\n$$\\phi(t) = e^{-j(E/\\hbar)t} = e^{j\\omega t}$$\n\nwhere $\\phi(t)$ is the time portion of the wave function.\n\nIt has the general solution:\n\n$$\\Psi(x,t) = A e^{j(kx-\\omega t)} + B e^{-j(kx-\\omega t)}$$\n\n### Wave Number\n\nThe wave number describes the number of wavelengths per unit distance.\n\n$$k = \\sqrt{\\frac{2mE}{\\hbar^2}} = \\sqrt{\\frac{p^2}{\\hbar^2}} = \\frac{p}{\\hbar} = \\frac{2\\pi}{\\lambda}$$\n\n$$p=\\hbar k$$\n\n### Probability Density Function\n\nAlone, the wave function $\\Psi(x, t)$ does not represent anything physcially. The probability density function,\n\n$$|\\Psi(x,t)|^2 = \\Psi(x, t) \\cdot \\Psi^*(x,t)$$\n\nrepresents the probability of finding the particle between $x$ and $x+dx$ at a certain time.\n\n### Boundary Conditions\n\nGiven the [probability density function](#probability-density-function), $|\\Psi(x)|^2$, we can say that, for a single particle:\n\n$$\\int_{- \\infty}^\\infty |\\Psi(x)|^2 dx = 1$$\n\nwhich states that over all of space, the probability of finding the particle is 100%.\n\n# Schrodinger's Wave Equation\n\nSchrodinger's wave equation describes the motion of electrons in a crystal and meshes the ideas of quantization and the wave-particle duality.\n\n$$- \\frac{\\hbar^2}{2m} \\cdot \\frac{\\partial^2 \\psi(x,t)}{\\partial x^2} + V(x) \\psi(x, t) = j \\hbar \\frac{\\partial \\psi (x,t)}{\\partial t} $$\n\nTime Independent Schrodinger's Wave Equations\n\n$$\\frac{\\partial^2 \\psi(x)}{\\partial x^2} + \\frac{2m}{\\hbar^2} (E-V(x)) \\psi(x) = 0$$\n\n### Particle in a Free Space\n\nIf a particle is in free space, it has no external forces acting on itself. It is represented as a traveling wave.\n\nTherefore, we assume:\n\n- $V(x) = 0$, the potential energy is zero.\n- $E > V(x)$\n\nThe time-independent wave equation is now:\n\n$$\\frac{\\partial^2 \\psi(x)}{\\partial x^2} + \\frac{2mE}{\\hbar^2} \\psi(x) = 0$$\n\nwith the solution\n\n$$\\psi(x) = A e^{\\frac{jx\\sqrt{2mE}}{\\hbar}} + B e^{\\frac{jx\\sqrt{2mE}}{\\hbar}} = A e^{jkx} + B e^{-jkx}$$\n\n$$k = \\sqrt{\\frac{2mE}{\\hbar^2}}$$\n\n### Infinite Potential Well\n\nWe define a particle to be confined within a width from $x=0$ to $=a$, surrounded by two infinitely high potential walls. Like with the particle in a free space, the time-independent Schrodinger's wave equation is defined as:\n\n$$\\frac{\\partial^2 \\psi(x)}{\\partial x^2} + \\frac{2mE}{\\hbar^2} \\psi(x) = 0$$\n\nwith a solution\n\n$$\\psi(x) = A_1 \\cos kx + A_2 \\sin kx$$\n$$k = \\frac{n \\pi}{a}, n = 1, 2, 3, ...$$\n\nThe boundary condition of the wave function is:\n\n$$\\psi(x = 0) = \\psi(x = a) = 0$$\n\nTherefore, \n\n$$A_1 = 0 \\text{ at } x = 0$$\n\n$$\\psi(x = a) = 0 = A_2 \\sin ka \\text{ at } x = a$$\n\nNow the probability is:\n\n$$\\int_0^a A_2^2 \\sin^2 kx dx = 1 \\implies A_2 = \\sqrt{\\frac{2}{a}}$$\n\nNow the wave function is:\n\n$$\\psi(x) = \\sqrt{\\frac{2}{a}} \\sin(\\frac{n \\pi x}{a}), n=1,2,3$$\n\n#### Energy Levels\n\nThe total energy of the particle in the infinite potential well must have discrete values. The energy of the particle is quantized.\n\n$$E = E_n = \\frac{\\hbar^2 n^2 \\pi^2}{2ma^2}, n=1,2,3,...$$\n\n$$\\psi(x) = \\sqrt{\\frac{2}{a}} \\sin k_n x$$\n\n### Step Potential Function\n\nWe assume that $E < V_0$.\n\nIn this case, the flux of particles is incident on a potential barrier with $E < V_0$, traveling from $-\\infty$ in the $+x$ direction. The step potential function follows Schrodinger's time independent equation:\n\n$$\\frac{\\partial^2 \\psi(x)}{\\partial x^2} + \\frac{2mE}{\\hbar^2} \\psi(x) = 0$$\n\nThe general solution is:\n\n$$\\psi(x) = A_1 e^{jkx} + B e^{-jkx}$$\n\nWhen the particle hits the potential barrier, it will be reflected completely and will travel in the $-z$ direction.\n\n### Potential Barrier\n\nIn this case, a potential barrier with is defined with a finite width from $x=0$ to $x=a$. The energy of an incident particle on the potential barrier is $E<V_0$. Now, the particle has the chance to tunnel through the potential barrier.\n\n### Tunneling\n\nThe probability that a particle penetrates through the barrier is:\n\n$$T \\approx 16 (\\frac{E}{V_0})(1-\\frac{E}{V_0}) e^{-2ka})$$\n"},517:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Semiconductors\ndescription: A little bit of conductors.\n---\n\n# Semiconductors\n\nA semiconductor is one of the categories of materials that describe its electrical properties. It is in between a **conductor** and **insulator**.\n\n### Elemental Semiconductor Materials\n\nElemental semiconductors are composed of an element, found in group $\\text{IV}$ of the periodic table.\n\n#### Examples\n\nSilicon ($\\text{Si}$), Germanium ($\\text{Ge}$)\n\n### Compound Semiconductor Materials (3-5 compound semiconductor material)\n\nCompound semiconductors are composed of compounds (multiple elements) and are composed of combinations of group $\\text{III}$ and group $\\text{V} \\text{}$.\n\n#### Binary Compounds\n\nTwo elements. For example, $\\text{AlP}$, $\\text{AlAs}$, $\\text{GaP}$, $\\text{GaAs}$, $\\text{InP}$.\n\n#### Ternary Compounds\n\nThree elements. For example $\\text{Al} \\text{G}_{a_{1-x}} \\text{As}$.\n\n# Types of Solids\n\n### Amorphous\n\n- No order\n- No patterns\n\n### Polycrystalline\n\n- Order in some regions\n- Patterns in some regions\n\n### Crystalline (Single)\n\nWell formed.\n\n- Order\n- Patterns\n\n# Space Lattices\n\nA lattice is the periodic arrangement of atoms.\n\n### Unit Cell\n\n- The smallest volume of crystal used to reproduce crystal.\n- Repetitive arrangement can build up the crystal without overlap.\n- Orthogonal\n- For example, zinc blende, diamond lattice\n\n### Primitive Cell\n\n- Contains one **lattice point**.\n- Smallest unit cell that can be repeated to form a lattice. Most basic cell of a lattice.\n- Can be non-orthogonal or orthogonal.\n\n# Crystal Structures\n\n3D structure of crystals.\n\n### Simple Cubic\n\n- Atoms are in each corner.\n\n### Body-Centered Cubic\n\n- One atom is added in the center.\n\n### Face-Centered Cubic\n\n- Atoms are added on the faces.\n- Each atom on the face contributes half of itself.\n\n### Atomic Density\n\n$$\\text{Density} = \\frac{\\text{atoms}}{\\text{volume of unit cell}}$$\n\n# Vector Characterization of Planes, Direction\n\n### Vector Characterization of Planes\n\n$$\\bar{r} = p \\bar{r} + q \\bar{b} + s \\bar{c}$$\n\nRepresentation of crystal planes.\n\n1. Find the intercepts of $\\bar{r} = p \\bar{r} + q \\bar{b} + s \\bar{c}$ on the $\\bar{a}$, $\\bar{b}$ and $\\bar{c}$ axes\n2. Put it in the form $\\displaystyle (\\frac{1}{p}, \\frac{1}{q}, \\frac{1}{s})$\n3. Find $\\displaystyle \\text{LCD}(\\frac{1}{p}, \\frac{1}{q}, \\frac{1}{s})$\n4. Multiply $\\displaystyle (\\frac{1}{p}, \\frac{1}{q}, \\frac{1}{s}) \\cdot \\text{LCD}(\\frac{1}{p}, \\frac{1}{q}, \\frac{1}{s})$\n\n### Vector Characterization of Direction\n\n$$|\\bar{r}| = [xyz]$$\n\nwhere $\\text{xyz}$ is the vector perpendicular to the surface of the crystal plane.\n\n### Characterization of Families\n\n$$\\{ r \\} = \\{ xyz \\}$$\n\n# Diamond Structure\n\n# Quantum Theory of Solids\n\n### K-Space Diagram\n\n$$k= \\frac{p}{\\hbar} = \\alpha$$\n\n#### Free Particle\n\n$$E=\\frac{p^2}{2m}=\\frac{k^2 \\hbar^2}{2m}$$\n\n#### Single Crystal Lattice\n\n$$f(\\alpha a) = P' \\frac{\\sin{\\alpha a}}{\\alpha a} + \\cos{\\alpha a}$$\n\n### Pauli Exclusion Principle\n\nNo two electrons in an atom or molecule can have the same set of quantum numbers.\n\n### Energy Band Model\n\nAn energy band is a collection of discrete energy levels.\n\nAs two atoms approach each other (interatomic distance $r_0$ decreases), the interaction increases due to the overlapping of wavefunctions.\n\n#### Discrete Energy Levels\n\nThe interaction results in a discrete quantized energy level splitting into two discrete energy levels to form an **energy band**. This is to allow the electrons to occupy a distinct quantum state, following the [Pauli Exclusion Principle](#pauli-exclusion-principle).\n\n#### Atomic Interaction\n\nNaturally, as atoms approach each other, the outermost shell of each atom tends to interact first, say at the $n=2$ energy level. As the atoms further approach each other, they eventually interact in lower energy levels until it reaches $n=1$.\n\n#### Forbidden Bands\n\nIn between the discrete energy bands, say between $n=1$ and $n=2$, are called the forbidden energy bands; no electrons are allowed in the area, only in the discrete energy bands.\n\n#### Absense of Thermal Energy\n\nWhen there is no thermal energy ($T=0$), electrons are in the lowest energy state. All states in the **valence band** are filled while all the states in the **conduction band** are empty.\n\n### Energy Bond Model\n\n#### Atomic Bonding\n\nElectrons are what keep two atoms bounded together. When there is an [absense of thermal energy](#absense-of-thermal-energy) ($T=0$), no bonds are broken. However, as thermal energy increases, the electrons may gain thermal energy to break the covalent bond and jump to the conduction band from the valence band. As a result, a hole (proton) is left in the valence band, and an electron is left in the conduction band.\n\n### Drift Current\n\nCurrent is the net flow of charge. If there is a net drift of electrons in the conduction band, current will exist since electrons are charged.\n\n#### Drift Current Density\n\n$$J = qNv_d \\qquad \\text{A/cm}^2$$\n\nwhere $q$ is the charge, $N$ is the volume density and $v_d$ is the drift current.\n\n#### Average Drift Current\n\n$$J = q \\sum_{i=1}^N v_i$$\n\nwhere $v_i$ is the velocity of the $i$th ion.\n\n### Electron Effective Mass\n\nThe force acting upon an electron is:\n\n$$F_{\\text{total}} = F_{\\text{external}} + F_{\\text{internal}} = ma$$\n\nIn order to model the internal forces that an electron experiences in the lattice, we use effective mass. Applying the simplification of the model, we have:\n\n$$F_{\\text{external}} = m^{*}a$$.\n\n$m^*$ is the effective mass.\n\nTaking the energy of an electron in free space, we take the second derivative of energy with respect to $k$ to isolate $m$:\n\n$$E=\\frac{p^2}{2m} = \\frac{\\hbar^2 k^2}{2m} \\implies \\frac{1}{\\hbar} \\frac{dE}{dk} = \\frac{p}{m} = v \\implies \\frac{1}{\\hbar^2} \\frac{d^2E}{dk^2} = \\frac{1}{m^*}$$\n\n#### Free Electron Mechanics\n\nApplying an electric field to a free electron causes an electron to accelerate in the opposite direction of the field due to its charge.\n\n$$F = ma = -eE \\implies a=\\frac{-eE}{m}$$\n\n### Density of States\n\nIt is important to find the density of allowed energy states (based on the energy band theory) to calculate electron and hole concentrations.\n\nWe can find the number of states per unit volume.\n\n#### Density of Allowed Electron Energy States in Conduction Band\n\n$$g_c(E) = \\frac{4 \\pi (2m^*_n)^{3/2}}{h^3} \\sqrt{E - E_c} \\qquad E \\geq E_c$$\n\nAs the nergy of the elctron in the conduction band decreases, the number of quantum states decreases.\n\n#### Density of Allowed Electron Energy States in Valence Band\n\n$$g_c(E) = \\frac{4 \\pi (2m^*_v)^{3/2}}{h^3} \\sqrt{E_v - E} \\qquad E \\leq E_v$$\n\n#### Finding Number of Quantum States Between Energy States\n\n$$N = \\int g(E) dE$$\n\n## Statistical Mechanics\n\n### Fermi-Dirac Probability Function\n\nThe Fermi-Dirac probability function considers particles to be indistinguishable. It includes all of the permutations of $N$ particles; also, it considers the $i$th energy level with $g_i$ quantum states.\n\n$$\\frac{N(E)}{g(E)} = f_F (E) = \\frac{1}{1+ e^{\\frac{E-E_F}{kT}}}$$\n\nwhere $N(E)$ is the number of particles per unit volume, while $g(E)$ is the number of quantum states per unit volume.\n\n#### Fermi-Dirac Function in Conduction Band\n\nTherefore, the Fermi-Dirac function $f_F (E)$ gives the probability that a quantum state at energy $E$ will be occupied by an electron.\n\n#### Fermi-Dirac Function in Valence Band\n\nAs a result, $f_F (E)$ will only be used in the conduction band since we are considering only electrons in the conduction band.\n\nOn the other hand, we will use $1 - f_F (E)$ in the valence band since we are considering holes in the valence band.\n\n### Maxwell-Boltzmann Approximation Function\n\nThis function considers particles to be numbered from $1 \\rightarrow N$ with no limit to the number of particles allowed in each energy state.\n\nWe extend the [Fermi-Dirac probability function](#fermi-dirac-probability-function) and approximate the case where $E - E_F >> kT$. In this case, \n\n$$f_F(E) \\approx e^{\\frac{-(E - E_F)}{kT}}$$\n\n# Semiconductor in Equilibrium\n\n## Charge Carriers in Semiconductors\n\nIn order to understand the current, we will examine the contribution of electrons and holes to current.\n\n### Distribution of Electrons\n\nWith respect to energy, the **distribution of electrons per unit volume** in the conduction band is:\n\n$$n(E) = g_c(E) f_F(E)$$\n\nAnd in the valence band:\n\n$$n(E) = g_c(E) (1 - f_F(E))$$\n\n### Concentration of Electrons\n\nBased on this equation, we can find the **concentration of electrons** in each band by integrating the [distribution of electrons](#distribution-of-electrons).\n\n#### Conduction Band Hole Concentration\n\n$$\\int n(E) = n_o = \\int_{E_c}^{E + E_c} g_c(E) f_F(E) dE = N_c e^{\\frac{-(E_c - E_F)}{kT}} $$\n\n#### Valence Band Electron Concentration\n\n$$\\int n(E) = p_o = \\int_{E_v - E}^{E_v} g_c(E) (1 - f_F(E)) dE = N_v e^{\\frac{-(E_F - E_v)}{kT}} $$\n\n### Intrinsic Semiconductor\n\nAn intrinsic semiconductor is one in which no impurities or imperfections are present.\n\n#### Electron Concentration in an Intrinsic Semiconductor\n\nIn other words, the concentration of electrons in the conduction band is equal to the holes in the valence band, $n_i = p_i$ ($i$ = intrinsic).\n\n#### Fermi Energy Level in an Intrinsic Semiconductor\n\nThe Fermi Energy level in an intrinsic semiconductor $E_{Fi}$ lies near the mid-gap between the conduction band and valence band.\n\nWe can calculate the exact location by equating the [conduction band electron concentration](#conduction-band-electron-concentration) and [valence band electron concentration](#valence-band-electron-concentration).\n\n$$E_{Fi} = E_{midgap} + \\frac{3}{4} kT \\ln{\\bigg(\\frac{m_p^*}{m_n^*}\\bigg)} $$\n\n- $m_p^* = m_n^* \\implies E_{Fi}$ is at the center. \n- $m_p^* > m_n^* \\implies E_{Fi}$ is above center. \n- $m_p^* < m_n^* \\implies E_{Fi}$ is below center. \n\n## Dopant Atoms and Energy Levels\n\n### Doping\n\nDoping a semiconductor means that we alter the semiconductor by adding substitutional impurity atoms to increase or decrease its conductivity. \n\n### Extrinsic Semiconductor\n\nIf an [intrinsic semiconductor](#intrinsic-semiconductor) is doped with an impurity atom, it becomes an **extrinsic semiconductor**.\n\n### Donor Impurity Atoms\n\nIf a semiconductor is doped with an atom that contains extra electrons and it donates those electrons to the conduction band (and does not create holes in the valence band), it is said to be a **donor impurity atom**.\n\nAn example of this would be if an Si semiconductor, a group IV element, was doped with a group V element (like Phosphorous).\n\n#### Donor Ionization Energy\n\nAs a result of an atom donating its extra electrons, it therefore becomes ionized positively.\n\n### Acceptor Impurity Atoms\n\nIf a semiconductor is doped with an atom that contains an empty spot in its electron shell and it is able to take electrons from the valence band (e.g. by creating holes in the valence band), it is said to be an **acceptor impurity atom**.\n\nAn example of this would be if an Si semiconductor, a group IV element, was doped with a group III element (like Boron).\n\n#### Acceptor Ionization Energy\n\nAs a result of an atom accepting extra electrons from the semiconductor, it therefore becomes ionized negatively.\n\n### Amphoteric Impurity Atoms\n\nAn **amphoteric impurity atom** is one that, depending on what atom it replaces, it can either be a donor or acceptor.\n\n### n-type Semiconductors\n\nIf the electron concentration is greater than the hole concentration ($n_0 > p_0$), the semiconductor is an **n-type**.\n\n### p-type Semiconductors\n\nIf the hole concentration is greater than the electron concentration ($n_0 < p_0$), the semiconductor is a **p-type**.\n\n### $n_0 p_0$ product\n\n$$n_0 p_0 = n_i^2\n\n### Degenerate Semiconductors\n\n\n"},518:function(n,e,t){"use strict";t.r(e),e.default='---\nheader: Set Theory\ndescription: The only theory that is set in stone.\n---\n\n# Set Notation\n\nA set is a collection of objects (or more formally, [elements](#elements)). It has the notation:\n\n$$\\left\\{ \\text{object}_1, \\text{object}_2, \\text{object}_3 \\right\\}$$\n\n### Elements\n\nElements are the objects within the set.\n\n### In\n\nThe $\\in$ symbol denotes whether an element is in the set.\n\n#### In Example\n\nLet $x$ be an element of the set $S$. This means:\n\n$$x \\in S$$\n\nIf $x$ was not in $S$, then it would be denoted as:\n\n$$x \\not\\in S$$\n\n### Universal Set ($\\Omega$)\n\nThe **universal set** is the set containing all elements or sets that are pertinent to your problem. It is denoted as:\n\n$$\\Omega$$\n\n### Empty Set ($\\emptyset$)\n\nIf the set $S$ contains no elements, it is denoted as:\n\n$$S = \\emptyset$$\n\n### Finite (and Countable) Set Notation\n\n$$S = \\left\\{x_1, x_2, x_3, ..., x_n\\right\\}$$\n\n### Infinite (and Countable) Set Notation\n\n$$S = \\left\\{x_1, x_2, x_3, ...\\right\\}$$\n\n### Set Builder (Uncountable) Notation\n\nSet builder notation can represent the above notations in addition to the uncountable set notation.\n\n$$\\left\\{ x \\mid x \\text{ satisfies } P \\right\\} $$\n\n"$\\mid$" means **such that**.\n\n### Subset ($\\subseteq$)\n\nLet $A$ and $B$ be sets.\n\nIf every element in $A$ is also an element of $B$, $A$ is a subset of $B$. It is denoted as:\n\n$$A \\subset B$$\n\n$A$ must equal $B$.\n\n### Proper Subset ($\\subsetneq$)\n\nLet $A$ and $B$ be sets.\n\nIf $A \\subset B$ but $A \\neq B$, $A$ is a proper subset of $B$. It is denoted as:\n\n$$A \\subsetneq B$$\n\nIn other words, there is at least one element in $B$ that is not in $A$.\n\n### Super Set ($\\supseteq$)\n\nLet $A$ and $B$ be sets.\n\nIf every element in $A$ is also an element of $B$, $B$ is a super set of $A$. It is denoted as:\n\n$$B \\supset A$$\n\n$B$ must equal $A$.\n\n### Proper Super Set ($\\supsetneq$)\n\nLet $A$ and $B$ be sets.\n\nIf $B \\supset A$ but $B \\neq A$, $B$ is a proper super set of $A$. It is denoted as:\n\n$$B \\supsetneq A$$\n\nIn other words, there is at least one element in $B$ that is not in $A$.\n\n### Disjoint Set\n\nIf the sets $A$ and $B$ are **disjoint**, $A$ does not share an element with $B$.\n\n$$A \\cap B = \\emptyset$$\n\n### Partition\n\nGiven a set $S$, a collection of sets is a **partition** of $S$ if the sets in the collection are [disjoint](#disjoint-set) and their [union](#union-cup) is $S$.\n\n$$A \\cup B \\cup ... = S$$\n\n# Set Operations\n\n### Complement ($S^c$)\n\nLet $S$ be a [proper subset](#proper-subset-subsetneq) of the [universal set](#universal-set-omega) $\\Omega$.\n\n$$S^c = \\left\\{ x \\in \\Omega \\mid x \\notin S \\right\\}$$\n\nIn other words, the complement is set of all elements in $\\Omega$ excluding the ones in $S$.\n\n### Union ($\\cup$)\n\nLet $A$ and $B$ be sets.\n\n$$A \\cup B = \\left\\{ x \\mid x \\in A \\lor x \\in B \\right\\}$$\n\nIn other words, the union is the set of all elements in $A$ or $B$.\n\n#### Union Sum\n\n$$\\displaystyle \\bigcup_{n=1}^{\\infty} S_n = S_1 \\cup S_2 \\cup S_3 \\cup ...$$\n\n### Intersection\n\nLet $A$ and $B$ be sets.\n\n$$A \\cap B = \\left\\{ x \\mid x \\in A \\land x \\in B \\right\\}$$\n\nIn other words, the intersection is the set of elements in both $A$ and $B$.\n\n#### Intersection Sum\n\n$$\\displaystyle \\bigcap_{n=1}^{\\infty} S_n = S_1 \\cap S_2 \\cap S_3 \\cap ...$$\n'},519:function(n,e,t){"use strict";t.r(e),e.default="---\nheader: Systems\ndescription: Big picture engineering.\n---\n\n# 1D Systems\n\n## Continuous Time Fourier Transform\n\nThe Fourier Transform assists with analyzing signals in the frequency domain. It is a transform from the time domain to the frequency domain and decomposes a function into its frequencies, hence why it is multiplied by sinusoids.\n\n$$F(f) = \\int_{-\\infty}^\\infty f(t) e^{-i2\\pi ft} dt$$\n\n$$f(t) = \\int_{-\\infty}^\\infty F(f) e^{i2\\pi ft} df$$\n\n## Continuous Time Signals\n\n### Rectangle Function\n\n$$\\text{rect}(t) \\triangleq \n\\begin{cases}\n  1 & |t| \\leq 1/2 \\\\\n  0 & \\text{otherwise}\n\\end{cases}\n$$\n\n### Step Function\n\n$$\\text{step}(t) \\triangleq \n\\begin{cases}\n  1 & t \\geq 0 \\\\\n  0 & t < 0\n\\end{cases}\n$$\n\n### Sign Function\n\n$$\\text{sign}(t) \\triangleq \n\\begin{cases}\n  1 & t > 0 \\\\\n  0 & t = 0 \\\\\n  -1 & t < 0\n\\end{cases}\n$$\n\n### sinc Function\n\n$$\\text{sinc}(t) \\triangleq \\frac{sin(\\pi t)}{\\pi t}$$\n$$\\text{sinc}(0) \\equiv \\lim_{t \\to 0} \\frac{sin(\\pi t)}{\\pi t} = 1 \\forall a \\neq 0$$\n\n### Lambda Function\n\n$$\\text{rect}(t) \\triangleq \n\\begin{cases}\n  1 - |t| & |t| \\leq 1 \\\\\n  0 & |t| > 1\n\\end{cases}\n$$\n\n### Dirac Impulse\n\n$$g(0) = \\int_{-\\infty}^\\infty \\delta(t) g(t) dt$$\n$$g(t_0) = \\int_{-\\infty}^\\infty \\delta(t-t_0) g(t) dt$$\n$$g(0) = \\lim_{\\epsilon \\to 0} \\int_{-\\infty}^\\infty \\frac{1}{\\epsilon} \\text{rect}\\Big(\\frac{t}{\\epsilon}\\Big) g(t) dt$$\n$$\\delta (t) = \\lim_{\\epsilon \\to 0} \\frac{1}{\\epsilon} \\text{rect}\\Big(\\frac{t}{\\epsilon}\\Big)$$\n\n## Continuous Time System Properties\n\n### Linear System\n\n$$x_i(t) \\to \n\\fbox{\n    {\n      L\n    }\n} \\to y_i(t) \\ \\to \\ \\displaystyle \\sum_{i = 1}^N c_i x_i(t) \\to\n\\fbox{\n    {\n      L\n    }\n} \\to \\displaystyle \\sum_{i = 1}^N c_i y_i(t)$$\n\n### Time Invariant\n\n$$x(t) \\to \n\\fbox{\n    {\n      TI\n    }\n} \\to y(t) \\ \\to \\ \\displaystyle x(t-\\tau) \\to\n\\fbox{\n    {\n      TI\n    }\n} \\to y(t-\\tau)$$\n\n### Linear & Time Invariant (LTI) Response\n\n$$x_i(t) \\to \n\\fbox{\n    {\n      LTI\n    }\n} \\to y_i(t) \\ \\to \\ \\displaystyle \\sum_{i = 1}^N c_i x_i(t-\\tau) \\to\n\\fbox{\n    {\n      LTI\n    }\n} \\to \\displaystyle \\sum_{i = 1}^N c_i y_i(t-\\tau)$$\n\nIn other words, $y(t) \\Big \\{ x(t-\\tau) \\Big \\} = y(t-\\tau) \\Big \\{ x(t-\\tau) \\Big \\} \\text{}$\n\n### Impulse Response\n\n$$\\delta(t-\\tau) \\to \n\\fbox{\n    {\n      LTI\n    }\n} \\to y(t) = h(t-\\tau)$$\n\n### Convolution\n\n$$y(t) = \\int_{-\\infty}^\\infty x(\\tau) h(t-\\tau) d\\tau = x(t) * h(t)$$\n\n# 2D Systems\n\n## Continuous Space Fourier Transform \n\n$$F(u,v) = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f(x,y) e^{-i2\\pi(ux+vy)} \\text{d}x\\text{d}y$$\n\n$$f(x,y) = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty F(u,v) e^{i2\\pi(ux+vy)} \\text{d}u\\text{d}v$$\n\n## Continuous Space Signals\n\n\n"},536:function(n,e,t){"use strict";var o=t(29),component=Object(o.a)({},(function(){var n=this.$createElement,e=this._self._c||n;return e("svg",{staticClass:"h-2 accent-green inline arrow",attrs:{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 20 20"}},[e("path",{attrs:{fill:"#a3f7bf",d:"M16.172 9l-6.071-6.071 1.414-1.414L20 10l-.707.707-7.778 7.778-1.414-1.414L16.172 11H0V9z"}})])}),[],!1,null,null,null);e.a=component.exports},537:function(n,e,t){var map={"./algorithms.md":[494,21],"./calculus.md":[495,22],"./cmake.md":[496,23],"./controls.md":[497,24],"./diffeq.md":[498,25],"./dip.md":[499,26],"./discrete.md":[500,27],"./docker.md":[501,28],"./ee.md":[502,29],"./electromagnetism.md":[503,30],"./finance.md":[504,31],"./fourier.md":[505,32],"./k8s.md":[506,33],"./kr.md":[507,34],"./legv8.md":[508,35],"./linear.md":[509,36],"./md.md":[510,37],"./mechanics.md":[511,38],"./microelectronics.md":[512,39],"./ml.md":[513,40],"./orbital.md":[514,41],"./probability.md":[515,42],"./quantum.md":[516,43],"./semiconductors.md":[517,44],"./sets.md":[518,45],"./systems.md":[519,46]};function o(n){if(!t.o(map,n))return Promise.resolve().then((function(){var e=new Error("Cannot find module '"+n+"'");throw e.code="MODULE_NOT_FOUND",e}));var e=map[n],o=e[0];return t.e(e[1]).then((function(){return t(o)}))}o.keys=function(){return Object.keys(map)},o.id=537,n.exports=o},538:function(n,e,t){var content=t(551);"string"==typeof content&&(content=[[n.i,content,""]]),content.locals&&(n.exports=content.locals);(0,t(44).default)("4c3d837a",content,!0,{sourceMap:!1})},549:function(n,e,t){var map={"./algorithms.md":494,"./calculus.md":495,"./cmake.md":496,"./controls.md":497,"./diffeq.md":498,"./dip.md":499,"./discrete.md":500,"./docker.md":501,"./ee.md":502,"./electromagnetism.md":503,"./finance.md":504,"./fourier.md":505,"./k8s.md":506,"./kr.md":507,"./legv8.md":508,"./linear.md":509,"./md.md":510,"./mechanics.md":511,"./microelectronics.md":512,"./ml.md":513,"./orbital.md":514,"./probability.md":515,"./quantum.md":516,"./semiconductors.md":517,"./sets.md":518,"./systems.md":519};function o(n){var e=r(n);return t(e)}function r(n){if(!t.o(map,n)){var e=new Error("Cannot find module '"+n+"'");throw e.code="MODULE_NOT_FOUND",e}return map[n]}o.keys=function(){return Object.keys(map)},o.resolve=r,n.exports=o,o.id=549},550:function(n,e,t){"use strict";var o=t(538);t.n(o).a},551:function(n,e,t){(e=t(43)(!1)).push([n.i,".clickable[data-v-bd1f3d00]{box-shadow:0 2px 5px 0 rgba(0,0,0,.16)}",""]),n.exports=e},628:function(n,e,t){"use strict";t.r(e);t(151),t(75),t(17),t(11),t(31),t(30);var o=t(3),r=t(0),l={components:{ArrowIcon:t(536).a},props:{title:{type:String,required:!0},tags:{type:Array,default:function(){return[]}},description:{type:String,default:""},link:{type:String,required:!0}}},$=t(29),c={components:{Note:Object($.a)(l,(function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"flex-1 rounded overflow-hidden shadow-lg card m-2"},[t("div",{staticClass:"px-6 py-4"},[t("div",{staticClass:"font-bold text-lg mb-1"},[n._v("\n      "+n._s(n.title)+"\n    ")]),n._v(" "),t("p",[n._v("\n      "+n._s(n.description)+"\n    ")]),n._v(" "),t("router-link",{staticClass:"read-more",attrs:{to:n.link}},[n._v("\n      Read More\n      "),t("ArrowIcon")],1)],1)])}),[],!1,null,"7aa55958",null).exports},asyncData:function(n){return Object(o.a)(regeneratorRuntime.mark((function e(){var l;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return n.params,l={},e.next=4,Promise.all(t(549).keys().map(function(){var n=Object(o.a)(regeneratorRuntime.mark((function n(e){var o,$;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return o=e.split("./")[1].split(".")[0],n.next=3,t(537)("./".concat(o,".md"));case 3:$=n.sent,l[o]=r.a.prototype.$markdown($.default);case 5:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}()));case 4:return e.abrupt("return",{topics:l});case 5:case"end":return e.stop()}}),e)})))()},data:function(){return{topics:[]}},methods:{importAll:function(n){var e=this;n.keys().forEach((function(n){e.topics.push(n.match(/\/(.*?)\./)[1])}))}}},d=(t(550),Object($.a)(c,(function(){var n=this.$createElement,e=this._self._c||n;return e("div",{staticClass:"flex p-4 flex-wrap"},this._l(this.topics,(function(n,t){return e("Note",{key:t,staticClass:"flex-initial",attrs:{title:n.frontmatter.header,description:n.frontmatter.description,link:"/notes/"+t}})})),1)}),[],!1,null,"bd1f3d00",null));e.default=d.exports}}]);